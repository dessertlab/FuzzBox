diff -ruN qemu/accel/tcg/afl-qemu-cpu-inl.h qemu_patched/accel/tcg/afl-qemu-cpu-inl.h
--- qemu/accel/tcg/afl-qemu-cpu-inl.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/accel/tcg/afl-qemu-cpu-inl.h	2023-11-30 22:15:30.604653361 +0100
@@ -0,0 +1,186 @@
+#include <sys/shm.h>
+#include <sys/mman.h>
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <fcntl.h>
+
+#include "libAFL/config.h" // CONTAINS MAP SIZE
+
+/* Map size for the traced binary (2^MAP_SIZE_POW2) */
+
+/* This snippet kicks in when the instruction pointer is positioned at
+   _start and does the usual forkserver stuff, not very different from
+   regular instrumentation injected via afl-as.h. */
+static int setup_flag1 = 0;
+
+#define AFL_QEMU_CPU_SNIPPET(pc) \ 
+   do                            \
+   {                             \
+      if (setup_flag1 == 0)      \
+      {                          \
+         afl_setup();            \
+         setup_flag1 = 1;        \
+      }                          \
+      afl_maybe_log(pc);         \
+   } while (0)
+
+typedef unsigned char u8;
+/* Bitmap definition */
+static u8 *afl_area_ptr;
+static u8 *block_coverage_ptr;
+static int shm_fd;
+static int shm_fd2;
+
+/* Function declarations */
+static inline void afl_setup(void);
+static inline void afl_maybe_log(target_ulong);
+
+/* Set up SHM region and initialize other stuff. */
+static inline void afl_setup(void)
+{
+
+   printf("AFL setup\n");
+   // edge coverage bitmap (usata)
+   shm_fd = shm_open(SHM_FILE, O_CREAT | O_RDWR, S_IRWXU | S_IRWXG | S_IRWXO); // 0666);
+   if (shm_fd == -1)
+   {
+      printf("Error in shm_open()\n");
+      exit(1);
+   }
+
+   if (ftruncate(shm_fd, sizeof(u8) * MAP_SIZE) == -1)
+   {
+      printf("Error in ftruncate()\n");
+      exit(1);
+   }
+
+   afl_area_ptr = mmap(NULL, sizeof(u8) * MAP_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0);
+   if (afl_area_ptr == MAP_FAILED)
+   {
+      printf("Error in mmap()\n");
+      exit(1);
+   }
+
+   memset(afl_area_ptr, 0x00, sizeof(u8) * MAP_SIZE);
+   afl_area_ptr[0] = 0x01;
+
+   // block coverage bitmap (per grafici)
+   shm_fd2 = shm_open(SHM_FILE_BLOCK_COV, O_CREAT | O_RDWR, S_IRWXU | S_IRWXG | S_IRWXO); // 0666);
+   if (shm_fd2 == -1)
+   {
+      printf("Error in shm_open()\n");
+      exit(1);
+   }
+
+   if (ftruncate(shm_fd2, sizeof(u8) * MAP_SIZE) == -1)
+   {
+      printf("Error in ftruncate()\n");
+      exit(1);
+   }
+
+   block_coverage_ptr = mmap(NULL, sizeof(u8) * MAP_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd2, 0);
+   if (afl_area_ptr == MAP_FAILED)
+   {
+      printf("Error in mmap()\n");
+      exit(1);
+   }
+
+   memset(block_coverage_ptr, 0x00, sizeof(u8) * MAP_SIZE);
+   block_coverage_ptr[0] = 0x01;
+
+   printf("SHM setup ok\n");
+}
+
+static inline target_ulong aflHash(target_ulong cur_loc)
+{
+
+   /* Looks like QEMU always maps to fixed locations, so ASAN is not a
+      concern. Phew. But instruction addresses may be aligned. Let's mangle
+      the value to get something quasi-uniform. */
+   target_ulong h = cur_loc;
+#if TARGET_LONG_BITS == 32
+   h ^= cur_loc >> 16;
+   h *= 0x85ebca6b;
+   h ^= h >> 13;
+   h *= 0xc2b2ae35;
+   h ^= h >> 16;
+#else
+   h ^= cur_loc >> 33;
+   h *= 0xff51afd7ed558ccd;
+   h ^= h >> 33;
+   h *= 0xc4ceb9fe1a85ec53;
+   h ^= h >> 33;
+#endif
+
+   h &= MAP_SIZE - 1;
+
+   /* Implement probabilistic instrumentation by looking at scrambled block
+      address. This keeps the instrumented locations stable across runs. */
+   if (h >= MAP_SIZE)
+   {
+      return 0;
+   }
+
+   return h;
+}
+
+static inline void afl_maybe_log(target_ulong cur_loc)
+{
+   //if(!FEEDBACK_MODE) return; // inutile calcolare la bitmap se non c'Ã¨ feedback
+   static __thread target_ulong prev_loc;
+
+   // cur_loc = (cur_loc >> 4) ^ (cur_loc << 8);
+   // cur_loc &= MAP_SIZE - 1;
+   cur_loc = aflHash(cur_loc); // alternative hash calculator
+
+   // prev_loc = 0; //if onTranslation update bitmap
+
+   if (!BLOCKCOV_MODE && !SAVE_METRICS) // blockcov mode=0, save_metrics=0 solo edge coverage
+   {
+      target_ulong index = cur_loc ^ prev_loc;
+      if (index < MAP_SIZE) // edge coverage update
+      {
+         if (afl_area_ptr[index] < 0xff)
+         {
+            // afl_area_ptr[index]=0xff; for onTranslation bitmap is better
+            afl_area_ptr[index]++;
+         }
+
+         prev_loc = cur_loc >> 1;
+      }
+   }
+   else if (BLOCKCOV_MODE && !SAVE_METRICS) // blockcov mode=1, save_metrics=0 solo blockcov salvata e usata
+   {
+      if (cur_loc < MAP_SIZE) // block coverage update
+      {
+         if (block_coverage_ptr[cur_loc] < 0xff)
+         {
+            block_coverage_ptr[cur_loc]++;
+         }
+      }
+   }
+   else
+   {
+
+      target_ulong index = cur_loc ^ prev_loc;
+      if (index < MAP_SIZE) // edge coverage update
+      {
+         if (afl_area_ptr[index] < 0xff)
+         {
+            // afl_area_ptr[index]=0xff; for onTranslation bitmap is better
+            afl_area_ptr[index]++;
+         }
+
+         prev_loc = cur_loc >> 1;
+      }
+      if (cur_loc < MAP_SIZE) // block coverage update
+      {
+         if (block_coverage_ptr[cur_loc] < 0xff)
+         {
+            block_coverage_ptr[cur_loc]++;
+         }
+      }
+   }
+}
diff -ruN qemu/accel/tcg/cpu-exec.c qemu_patched/accel/tcg/cpu-exec.c
--- qemu/accel/tcg/cpu-exec.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/accel/tcg/cpu-exec.c	2024-01-22 15:34:44.769476996 +0100
@@ -48,6 +48,7 @@
 #include "internal.h"
 #include "qemu-traces.h"
 #include "qemu-decision_map.h"
+#include "accel/tcg/afl-qemu-cpu-inl.h"
 
 static void trace_before_exec(TranslationBlock *);
 static void trace_after_exec(uintptr_t);
@@ -56,7 +57,8 @@
 
 /* -icount align implementation. */
 
-typedef struct SyncClocks {
+typedef struct SyncClocks
+{
     int64_t diff_clk;
     int64_t last_cpu_icount;
     int64_t realtime_clock;
@@ -79,7 +81,8 @@
 {
     int64_t cpu_icount;
 
-    if (!icount_align_option) {
+    if (!icount_align_option)
+    {
         return;
     }
 
@@ -87,14 +90,18 @@
     sc->diff_clk += icount_to_ns(sc->last_cpu_icount - cpu_icount);
     sc->last_cpu_icount = cpu_icount;
 
-    if (sc->diff_clk > VM_CLOCK_ADVANCE) {
+    if (sc->diff_clk > VM_CLOCK_ADVANCE)
+    {
 #ifndef _WIN32
         struct timespec sleep_delay, rem_delay;
         sleep_delay.tv_sec = sc->diff_clk / 1000000000LL;
         sleep_delay.tv_nsec = sc->diff_clk % 1000000000LL;
-        if (nanosleep(&sleep_delay, &rem_delay) < 0) {
+        if (nanosleep(&sleep_delay, &rem_delay) < 0)
+        {
             sc->diff_clk = rem_delay.tv_sec * 1000000000LL + rem_delay.tv_nsec;
-        } else {
+        }
+        else
+        {
             sc->diff_clk = 0;
         }
 #else
@@ -112,10 +119,12 @@
 
     if (icount_align_option &&
         sc->realtime_clock - last_realtime_clock >= MAX_DELAY_PRINT_RATE &&
-        nb_prints < MAX_NB_PRINTS) {
+        nb_prints < MAX_NB_PRINTS)
+    {
         if ((-sc->diff_clk / (float)1000000000LL > threshold_delay) ||
             (-sc->diff_clk / (float)1000000000LL <
-             (threshold_delay - THRESHOLD_REDUCE))) {
+             (threshold_delay - THRESHOLD_REDUCE)))
+        {
             threshold_delay = (-sc->diff_clk / 1000000000LL) + 1;
             qemu_printf("Warning: The guest is now late by %.1f to %.1f seconds\n",
                         threshold_delay - 1,
@@ -128,17 +137,19 @@
 
 static void init_delay_params(SyncClocks *sc, CPUState *cpu)
 {
-    if (!icount_align_option) {
+    if (!icount_align_option)
+    {
         return;
     }
     sc->realtime_clock = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL_RT);
     sc->diff_clk = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL) - sc->realtime_clock;
-    sc->last_cpu_icount
-        = cpu->icount_extra + cpu_neg(cpu)->icount_decr.u16.low;
-    if (sc->diff_clk < max_delay) {
+    sc->last_cpu_icount = cpu->icount_extra + cpu_neg(cpu)->icount_decr.u16.low;
+    if (sc->diff_clk < max_delay)
+    {
         max_delay = sc->diff_clk;
     }
-    if (sc->diff_clk > max_advance) {
+    if (sc->diff_clk > max_advance)
+    {
         max_advance = sc->diff_clk;
     }
 
@@ -167,11 +178,16 @@
      * For singlestep and -d nochain, suppress goto_tb so that
      * we can log -d cpu,exec after every TB.
      */
-    if (unlikely(cpu->singlestep_enabled)) {
+    if (unlikely(cpu->singlestep_enabled))
+    {
         cflags |= CF_NO_GOTO_TB | CF_NO_GOTO_PTR | CF_SINGLE_STEP | 1;
-    } else if (singlestep) {
+    }
+    else if (singlestep)
+    {
         cflags |= CF_NO_GOTO_TB | 1;
-    } else if (qemu_loglevel_mask(CPU_LOG_TB_NOCHAIN)) {
+    }
+    else if (qemu_loglevel_mask(CPU_LOG_TB_NOCHAIN))
+    {
         cflags |= CF_NO_GOTO_TB;
     }
 
@@ -197,11 +213,13 @@
                tb->cs_base == cs_base &&
                tb->flags == flags &&
                tb->trace_vcpu_dstate == *cpu->trace_dstate &&
-               tb_cflags(tb) == cflags)) {
+               tb_cflags(tb) == cflags))
+    {
         return tb;
     }
     tb = tb_htable_lookup(cpu, pc, cs_base, flags, cflags);
-    if (tb == NULL) {
+    if (tb == NULL)
+    {
         return NULL;
     }
     qatomic_set(&cpu->tb_jmp_cache[hash], tb);
@@ -211,8 +229,8 @@
 static inline void log_cpu_exec(target_ulong pc, CPUState *cpu,
                                 const TranslationBlock *tb)
 {
-    if (unlikely(qemu_loglevel_mask(CPU_LOG_TB_CPU | CPU_LOG_EXEC))
-        && qemu_log_in_addr_range(pc)) {
+    if (unlikely(qemu_loglevel_mask(CPU_LOG_TB_CPU | CPU_LOG_EXEC)) && qemu_log_in_addr_range(pc))
+    {
 
         qemu_log_mask(CPU_LOG_EXEC,
                       "Trace %d: %p [" TARGET_FMT_lx
@@ -221,11 +239,13 @@
                       tb->flags, tb->cflags, lookup_symbol(pc));
 
 #if defined(DEBUG_DISAS)
-        if (qemu_loglevel_mask(CPU_LOG_TB_CPU)) {
+        if (qemu_loglevel_mask(CPU_LOG_TB_CPU))
+        {
             FILE *logfile = qemu_log_lock();
             int flags = 0;
 
-            if (qemu_loglevel_mask(CPU_LOG_TB_FPU)) {
+            if (qemu_loglevel_mask(CPU_LOG_TB_FPU))
+            {
                 flags |= CPU_DUMP_FPU;
             }
 #if defined(TARGET_I386)
@@ -244,7 +264,8 @@
     CPUBreakpoint *bp;
     bool match_page = false;
 
-    if (likely(QTAILQ_EMPTY(&cpu->breakpoints))) {
+    if (likely(QTAILQ_EMPTY(&cpu->breakpoints)))
+    {
         return false;
     }
 
@@ -257,21 +278,27 @@
      * so that one could (gdb) singlestep into the guest kernel's
      * architectural breakpoint handler.
      */
-    if (cpu->singlestep_enabled) {
+    if (cpu->singlestep_enabled)
+    {
         return false;
     }
 
-    QTAILQ_FOREACH(bp, &cpu->breakpoints, entry) {
+    QTAILQ_FOREACH(bp, &cpu->breakpoints, entry)
+    {
         /*
          * If we have an exact pc match, trigger the breakpoint.
          * Otherwise, note matches within the page.
          */
-        if (pc == bp->pc) {
+        if (pc == bp->pc)
+        {
             bool match_bp = false;
 
-            if (bp->flags & BP_GDB) {
+            if (bp->flags & BP_GDB)
+            {
                 match_bp = true;
-            } else if (bp->flags & BP_CPU) {
+            }
+            else if (bp->flags & BP_CPU)
+            {
 #ifdef CONFIG_USER_ONLY
                 g_assert_not_reached();
 #else
@@ -281,11 +308,14 @@
 #endif
             }
 
-            if (match_bp) {
+            if (match_bp)
+            {
                 cpu->exception_index = EXCP_DEBUG;
                 return true;
             }
-        } else if (((pc ^ bp->pc) & TARGET_PAGE_MASK) == 0) {
+        }
+        else if (((pc ^ bp->pc) & TARGET_PAGE_MASK) == 0)
+        {
             match_page = true;
         }
     }
@@ -302,7 +332,8 @@
      * invalidated, nor would any TB need to be invalidated as
      * breakpoints are removed.
      */
-    if (match_page) {
+    if (match_page)
+    {
         *cflags = (*cflags & ~CF_COUNT_MASK) | CF_NO_GOTO_TB | 1;
     }
     return false;
@@ -326,12 +357,14 @@
     cpu_get_tb_cpu_state(env, &pc, &cs_base, &flags);
 
     cflags = curr_cflags(cpu);
-    if (check_for_breakpoints(cpu, pc, &cflags)) {
+    if (check_for_breakpoints(cpu, pc, &cflags))
+    {
         cpu_loop_exit(cpu);
     }
 
     tb = tb_lookup(cpu, pc, cs_base, flags, cflags);
-    if (tb == NULL) {
+    if (tb == NULL)
+    {
         return tcg_code_gen_epilogue;
     }
 
@@ -350,9 +383,64 @@
  * TCG is not considered a security-sensitive part of QEMU so this does not
  * affect the impact of CFI in environment with high security requirements
  */
-static inline TranslationBlock * QEMU_DISABLE_CFI
-cpu_tb_exec(CPUState *cpu, TranslationBlock *itb, int *tb_exit)
-{
+
+static int make_once = 1;
+int hashSet[HASH_SIZE];
+int shouldprint=0;
+
+static inline TranslationBlock *QEMU_DISABLE_CFI
+cpu_tb_exec(target_ulong pc, CPUState *cpu, TranslationBlock *itb, int *tb_exit)
+{
+    /*if(pc == 0x80201f04){
+        shouldprint=5000;
+        printf("SHOULD PRINT!!!!\n\n");
+    }
+    if(shouldprint>0){
+        shouldprint--;
+        printf("pc is %x\n",pc);
+    }*/
+    if (make_once && PC_PROFILING)
+    {
+        make_once = 0;
+        FILE *file;
+        char line[10];                      // Assuming each hex value is on a line of max length 9 (including newline)
+        uint32_t hexValues[MAX_HEX_VALUES]; // Array to store hex values
+        int count = 0;                      // Counter to keep track of the number of hex values read
+
+        // Create a hash set (initialize all values to 0, meaning the value is not present)
+
+        memset(hashSet, 0, sizeof(hashSet));
+
+        // Open the file containing the list of hex values
+        file = fopen(PC_FILE_READ, "r");
+        if (file == NULL)
+        {
+            perror("Error opening the file");
+            exit(0);
+            return EXIT_FAILURE;
+        }
+
+        // Read hex values from the file and store them in the hash set
+        while (fgets(line, sizeof(line), file))
+        {
+            uint32_t value = strtoul(line, NULL, 16);
+            hexValues[count] = value;
+
+            // Calculate hash and mark the corresponding index in the hash set
+            int hash = value % HASH_SIZE;
+            hashSet[hash] = 1;
+
+            count++;
+
+            if (count >= MAX_HEX_VALUES)
+            {
+                printf("Too many hex values in the file. Increase MAX_HEX_VALUES.\n");
+                break;
+            }
+        }
+
+        fclose(file); // Close the file
+    }
     CPUArchState *env = cpu->env_ptr;
     uintptr_t ret;
     TranslationBlock *last_tb;
@@ -360,7 +448,8 @@
 
     log_cpu_exec(itb->pc, cpu, itb);
 
-    if (tracefile_enabled) {
+    if (tracefile_enabled)
+    {
         trace_before_exec(itb);
     }
 
@@ -368,17 +457,22 @@
 
     ret = tcg_qemu_tb_exec(env, tb_ptr);
 
-    if (tracefile_enabled) {
+    if (tracefile_enabled)
+    {
         trace_after_exec(ret);
     }
-
-    if (ret & TB_EXIT_NOPATCH) {
+    // rintf("\npc is %ld\n",pc);
+    if (ret & TB_EXIT_NOPATCH)
+    {
         /* The flag TB_EXIT_NOPATCH means that next_tb should be 0 after
          * tracing.
          */
         ret = 0;
     }
-
+    if (!BITMAP_UPDATE_ONTRANSLATE) // on tlb block exec
+    {
+        upd_bitmap(pc);
+    }
     cpu->can_do_io = 1;
     /*
      * TODO: Delay swapping back to the read-write region of the TB
@@ -393,31 +487,39 @@
 
     trace_exec_tb_exit(last_tb, *tb_exit);
 
-    if (*tb_exit > TB_EXIT_IDX1) {
+    if (*tb_exit > TB_EXIT_IDX1)
+    {
         /* We didn't start executing this TB (eg because the instruction
          * counter hit zero); we must restore the guest PC to the address
          * of the start of the TB.
          */
         CPUClass *cc = CPU_GET_CLASS(cpu);
         qemu_log_mask_and_addr(CPU_LOG_EXEC, last_tb->pc,
-                               "Stopped execution of TB chain before %p ["
-                               TARGET_FMT_lx "] %s\n",
+                               "Stopped execution of TB chain before %p [" TARGET_FMT_lx "] %s\n",
                                last_tb->tc.ptr, last_tb->pc,
                                lookup_symbol(last_tb->pc));
-        if (cc->tcg_ops->synchronize_from_tb) {
+        if (cc->tcg_ops->synchronize_from_tb)
+        {
             cc->tcg_ops->synchronize_from_tb(cpu, last_tb);
-        } else {
+        }
+        else
+        {
             assert(cc->set_pc);
             cc->set_pc(cpu, last_tb->pc);
         }
     }
+    else
+    {
+    }
+    // printf("translation size is %d\n",itb->size);
 
     /*
      * If gdb single-step, and we haven't raised another exception,
      * raise a debug exception.  Single-step with another exception
      * is handled in cpu_handle_exception.
      */
-    if (unlikely(cpu->singlestep_enabled) && cpu->exception_index == -1) {
+    if (unlikely(cpu->singlestep_enabled) && cpu->exception_index == -1)
+    {
         cpu->exception_index = EXCP_DEBUG;
         cpu_loop_exit(cpu);
     }
@@ -425,12 +527,12 @@
     return last_tb;
 }
 
-
 static void cpu_exec_enter(CPUState *cpu)
 {
     CPUClass *cc = CPU_GET_CLASS(cpu);
 
-    if (cc->tcg_ops->cpu_exec_enter) {
+    if (cc->tcg_ops->cpu_exec_enter)
+    {
         cc->tcg_ops->cpu_exec_enter(cpu);
     }
 }
@@ -439,7 +541,8 @@
 {
     CPUClass *cc = CPU_GET_CLASS(cpu);
 
-    if (cc->tcg_ops->cpu_exec_exit) {
+    if (cc->tcg_ops->cpu_exec_exit)
+    {
         cc->tcg_ops->cpu_exec_exit(cpu);
     }
 }
@@ -452,7 +555,8 @@
     uint32_t flags, cflags;
     int tb_exit;
 
-    if (sigsetjmp(cpu->jmp_env, 0) == 0) {
+    if (sigsetjmp(cpu->jmp_env, 0) == 0)
+    {
         start_exclusive();
         g_assert(cpu == current_cpu);
         g_assert(!cpu->running);
@@ -473,8 +577,10 @@
          */
 
         tb = tb_lookup(cpu, pc, cs_base, flags, cflags);
-        if (tb == NULL) {
+        if (tb == NULL)
+        {
             mmap_lock();
+
             tb = tb_gen_code(cpu, pc, cs_base, flags, cflags);
             mmap_unlock();
         }
@@ -482,9 +588,11 @@
         cpu_exec_enter(cpu);
         /* execute the generated code */
         trace_exec_tb(tb, pc);
-        cpu_tb_exec(cpu, tb, &tb_exit);
+        cpu_tb_exec(pc, cpu, tb, &tb_exit);
         cpu_exec_exit(cpu);
-    } else {
+    }
+    else
+    {
         /*
          * The mmap_lock is dropped by tb_gen_code if it runs out of
          * memory.
@@ -493,7 +601,8 @@
         clear_helper_retaddr();
         tcg_debug_assert(!have_mmap_lock());
 #endif
-        if (qemu_mutex_iothread_locked()) {
+        if (qemu_mutex_iothread_locked())
+        {
             qemu_mutex_unlock_iothread();
         }
         assert_no_pages_locked();
@@ -510,7 +619,8 @@
     end_exclusive();
 }
 
-struct tb_desc {
+struct tb_desc
+{
     target_ulong pc;
     target_ulong cs_base;
     CPUArchState *env;
@@ -530,17 +640,22 @@
         tb->cs_base == desc->cs_base &&
         tb->flags == desc->flags &&
         tb->trace_vcpu_dstate == desc->trace_vcpu_dstate &&
-        tb_cflags(tb) == desc->cflags) {
+        tb_cflags(tb) == desc->cflags)
+    {
         /* check next page if needed */
-        if (tb->page_addr[1] == -1) {
+        if (tb->page_addr[1] == -1)
+        {
             return true;
-        } else {
+        }
+        else
+        {
             tb_page_addr_t phys_page2;
             target_ulong virt_page2;
 
             virt_page2 = (desc->pc & TARGET_PAGE_MASK) + TARGET_PAGE_SIZE;
             phys_page2 = get_page_addr_code(desc->env, virt_page2);
-            if (tb->page_addr[1] == phys_page2) {
+            if (tb->page_addr[1] == phys_page2)
+            {
                 return true;
             }
         }
@@ -563,7 +678,8 @@
     desc.trace_vcpu_dstate = *cpu->trace_dstate;
     desc.pc = pc;
     phys_pc = get_page_addr_code(desc.env, pc);
-    if (phys_pc == -1) {
+    if (phys_pc == -1)
+    {
         return NULL;
     }
     desc.phys_page1 = phys_pc & TARGET_PAGE_MASK;
@@ -573,13 +689,16 @@
 
 void tb_set_jmp_target(TranslationBlock *tb, int n, uintptr_t addr)
 {
-    if (TCG_TARGET_HAS_direct_jump) {
+    if (TCG_TARGET_HAS_direct_jump)
+    {
         uintptr_t offset = tb->jmp_target_arg[n];
         uintptr_t tc_ptr = (uintptr_t)tb->tc.ptr;
         uintptr_t jmp_rx = tc_ptr + offset;
         uintptr_t jmp_rw = jmp_rx - tcg_splitwx_diff;
         tb_target_set_jmp_target(tc_ptr, jmp_rx, jmp_rw, addr);
-    } else {
+    }
+    else
+    {
         tb->jmp_target_arg[n] = addr;
     }
 }
@@ -594,13 +713,15 @@
     qemu_spin_lock(&tb_next->jmp_lock);
 
     /* make sure the destination TB is valid */
-    if (tb_next->cflags & CF_INVALID) {
+    if (tb_next->cflags & CF_INVALID)
+    {
         goto out_unlock_next;
     }
     /* Atomically claim the jump destination slot only if it was NULL */
     old = qatomic_cmpxchg(&tb->jmp_dest[n], (uintptr_t)NULL,
                           (uintptr_t)tb_next);
-    if (old) {
+    if (old)
+    {
         goto out_unlock_next;
     }
 
@@ -620,7 +741,7 @@
                            tb_next->tc.ptr, tb_next->pc);
     return;
 
- out_unlock_next:
+out_unlock_next:
     qemu_spin_unlock(&tb_next->jmp_lock);
     return;
 }
@@ -628,9 +749,11 @@
 static inline bool cpu_handle_halt(CPUState *cpu)
 {
 #ifndef CONFIG_USER_ONLY
-    if (cpu->halted) {
+    if (cpu->halted)
+    {
 #if defined(TARGET_I386)
-        if (cpu->interrupt_request & CPU_INTERRUPT_POLL) {
+        if (cpu->interrupt_request & CPU_INTERRUPT_POLL)
+        {
             X86CPU *x86_cpu = X86_CPU(cpu);
             qemu_mutex_lock_iothread();
             apic_poll_irq(x86_cpu->apic_state);
@@ -638,7 +761,8 @@
             qemu_mutex_unlock_iothread();
         }
 #endif /* TARGET_I386 */
-        if (!cpu_has_work(cpu)) {
+        if (!cpu_has_work(cpu))
+        {
             return true;
         }
 
@@ -654,39 +778,46 @@
     CPUClass *cc = CPU_GET_CLASS(cpu);
     CPUWatchpoint *wp;
 
-    if (!cpu->watchpoint_hit) {
-        QTAILQ_FOREACH(wp, &cpu->watchpoints, entry) {
+    if (!cpu->watchpoint_hit)
+    {
+        QTAILQ_FOREACH(wp, &cpu->watchpoints, entry)
+        {
             wp->flags &= ~BP_WATCHPOINT_HIT;
         }
     }
 
-    if (cc->tcg_ops->debug_excp_handler) {
+    if (cc->tcg_ops->debug_excp_handler)
+    {
         cc->tcg_ops->debug_excp_handler(cpu);
     }
 }
 
 static inline bool cpu_handle_exception(CPUState *cpu, int *ret)
 {
-    if (cpu->exception_index < 0) {
+    if (cpu->exception_index < 0)
+    {
 #ifndef CONFIG_USER_ONLY
-        if (replay_has_exception()
-            && cpu_neg(cpu)->icount_decr.u16.low + cpu->icount_extra == 0) {
+        if (replay_has_exception() && cpu_neg(cpu)->icount_decr.u16.low + cpu->icount_extra == 0)
+        {
             /* Execute just one insn to trigger exception pending in the log */
-            cpu->cflags_next_tb = (curr_cflags(cpu) & ~CF_USE_ICOUNT)
-                | CF_NOIRQ | 1;
+            cpu->cflags_next_tb = (curr_cflags(cpu) & ~CF_USE_ICOUNT) | CF_NOIRQ | 1;
         }
 #endif
         return false;
     }
-    if (cpu->exception_index >= EXCP_INTERRUPT) {
+    if (cpu->exception_index >= EXCP_INTERRUPT)
+    {
         /* exit request from the cpu execution loop */
         *ret = cpu->exception_index;
-        if (*ret == EXCP_DEBUG) {
+        if (*ret == EXCP_DEBUG)
+        {
             cpu_handle_debug_exception(cpu);
         }
         cpu->exception_index = -1;
         return true;
-    } else {
+    }
+    else
+    {
 #if defined(CONFIG_USER_ONLY)
         /* if user mode only, we simulate a fake exception
            which will be handled outside the cpu execution
@@ -699,14 +830,16 @@
         cpu->exception_index = -1;
         return true;
 #else
-        if (replay_exception()) {
+        if (replay_exception())
+        {
             CPUClass *cc = CPU_GET_CLASS(cpu);
             qemu_mutex_lock_iothread();
             cc->tcg_ops->do_interrupt(cpu);
             qemu_mutex_unlock_iothread();
             cpu->exception_index = -1;
 
-            if (unlikely(cpu->singlestep_enabled)) {
+            if (unlikely(cpu->singlestep_enabled))
+            {
                 /*
                  * After processing the exception, ensure an EXCP_DEBUG is
                  * raised when single-stepping so that GDB doesn't miss the
@@ -716,7 +849,9 @@
                 cpu_handle_debug_exception(cpu);
                 return true;
             }
-        } else if (!replay_has_interrupt()) {
+        }
+        else if (!replay_has_interrupt())
+        {
             /* give a chance to iothread in replay mode */
             *ret = EXCP_INTERRUPT;
             return true;
@@ -743,6 +878,20 @@
 }
 #endif /* !CONFIG_USER_ONLY */
 
+void upd_bitmap(target_ulong pc)
+{
+    if (!hashSet[pc % HASH_SIZE] || PC_GATHERING_MODE) // PC FIltering
+    {
+        AFL_QEMU_CPU_SNIPPET(pc);
+        if (PC_GATHERING_MODE)
+        {
+            char comando[200];
+            sprintf(comando, "echo %x >> %s", pc, PC_FILE_WRITE);
+            system(comando);
+        }
+    }//else printf("filtered!\n");
+}
+
 static inline bool cpu_handle_interrupt(CPUState *cpu,
                                         TranslationBlock **last_tb)
 {
@@ -751,7 +900,8 @@
      * skip checking here. Any pending interrupts will get picked up
      * by the next TB we execute under normal cflags.
      */
-    if (cpu->cflags_next_tb != -1 && cpu->cflags_next_tb & CF_NOIRQ) {
+    if (cpu->cflags_next_tb != -1 && cpu->cflags_next_tb & CF_NOIRQ)
+    {
         return false;
     }
 
@@ -762,24 +912,30 @@
      */
     qatomic_mb_set(&cpu_neg(cpu)->icount_decr.u16.high, 0);
 
-    if (unlikely(qatomic_read(&cpu->interrupt_request))) {
+    if (unlikely(qatomic_read(&cpu->interrupt_request)))
+    {
         int interrupt_request;
         qemu_mutex_lock_iothread();
         interrupt_request = cpu->interrupt_request;
-        if (unlikely(cpu->singlestep_enabled & SSTEP_NOIRQ)) {
+        if (unlikely(cpu->singlestep_enabled & SSTEP_NOIRQ))
+        {
             /* Mask out external interrupts for this step. */
             interrupt_request &= ~CPU_INTERRUPT_SSTEP_MASK;
         }
-        if (interrupt_request & CPU_INTERRUPT_DEBUG) {
+        if (interrupt_request & CPU_INTERRUPT_DEBUG)
+        {
             cpu->interrupt_request &= ~CPU_INTERRUPT_DEBUG;
             cpu->exception_index = EXCP_DEBUG;
             qemu_mutex_unlock_iothread();
             return true;
         }
 #if !defined(CONFIG_USER_ONLY)
-        if (replay_mode == REPLAY_MODE_PLAY && !replay_has_interrupt()) {
+        if (replay_mode == REPLAY_MODE_PLAY && !replay_has_interrupt())
+        {
             /* Do nothing */
-        } else if (interrupt_request & CPU_INTERRUPT_HALT) {
+        }
+        else if (interrupt_request & CPU_INTERRUPT_HALT)
+        {
             replay_interrupt();
             cpu->interrupt_request &= ~CPU_INTERRUPT_HALT;
             cpu->halted = 1;
@@ -788,7 +944,8 @@
             return true;
         }
 #if defined(TARGET_I386)
-        else if (interrupt_request & CPU_INTERRUPT_INIT) {
+        else if (interrupt_request & CPU_INTERRUPT_INIT)
+        {
             X86CPU *x86_cpu = X86_CPU(cpu);
             CPUArchState *env = &x86_cpu->env;
             replay_interrupt();
@@ -799,7 +956,8 @@
             return true;
         }
 #else
-        else if (interrupt_request & CPU_INTERRUPT_RESET) {
+        else if (interrupt_request & CPU_INTERRUPT_RESET)
+        {
             replay_interrupt();
             cpu_reset(cpu);
             qemu_mutex_unlock_iothread();
@@ -810,12 +968,15 @@
            False when the interrupt isn't processed,
            True when it is, and we should restart on a new TB,
            and via longjmp via cpu_loop_exit.  */
-        else {
+        else
+        {
             CPUClass *cc = CPU_GET_CLASS(cpu);
 
             if (cc->tcg_ops->cpu_exec_interrupt &&
-                cc->tcg_ops->cpu_exec_interrupt(cpu, interrupt_request)) {
-                if (need_replay_interrupt(interrupt_request)) {
+                cc->tcg_ops->cpu_exec_interrupt(cpu, interrupt_request))
+            {
+                if (need_replay_interrupt(interrupt_request))
+                {
                     replay_interrupt();
                 }
                 /*
@@ -823,7 +984,8 @@
                  * raised when single-stepping so that GDB doesn't miss the
                  * next instruction.
                  */
-                if (unlikely(cpu->singlestep_enabled)) {
+                if (unlikely(cpu->singlestep_enabled))
+                {
                     cpu->exception_index = EXCP_DEBUG;
                     qemu_mutex_unlock_iothread();
                     return true;
@@ -836,7 +998,8 @@
             interrupt_request = cpu->interrupt_request;
         }
 #endif /* !CONFIG_USER_ONLY */
-        if (interrupt_request & CPU_INTERRUPT_EXITTB) {
+        if (interrupt_request & CPU_INTERRUPT_EXITTB)
+        {
             cpu->interrupt_request &= ~CPU_INTERRUPT_EXITTB;
             /* ensure that no TB jump will be modified as
                the program flow was changed */
@@ -848,12 +1011,11 @@
     }
 
     /* Finally, check if we need to exit to the main loop.  */
-    if (unlikely(qatomic_read(&cpu->exit_request))
-        || (icount_enabled()
-            && (cpu->cflags_next_tb == -1 || cpu->cflags_next_tb & CF_USE_ICOUNT)
-            && cpu_neg(cpu)->icount_decr.u16.low + cpu->icount_extra == 0)) {
+    if (unlikely(qatomic_read(&cpu->exit_request)) || (icount_enabled() && (cpu->cflags_next_tb == -1 || cpu->cflags_next_tb & CF_USE_ICOUNT) && cpu_neg(cpu)->icount_decr.u16.low + cpu->icount_extra == 0))
+    {
         qatomic_set(&cpu->exit_request, 0);
-        if (cpu->exception_index == -1) {
+        if (cpu->exception_index == -1)
+        {
             cpu->exception_index = EXCP_INTERRUPT;
         }
         return true;
@@ -868,15 +1030,17 @@
     int32_t insns_left;
 
     trace_exec_tb(tb, tb->pc);
-    tb = cpu_tb_exec(cpu, tb, tb_exit);
-    if (*tb_exit != TB_EXIT_REQUESTED) {
+    tb = cpu_tb_exec(tb->pc, cpu, tb, tb_exit);
+    if (*tb_exit != TB_EXIT_REQUESTED)
+    {
         *last_tb = tb;
         return;
     }
 
     *last_tb = NULL;
     insns_left = qatomic_read(&cpu_neg(cpu)->icount_decr.u32);
-    if (insns_left < 0) {
+    if (insns_left < 0)
+    {
         /* Something asked us to stop executing chained TBs; just
          * continue round the main loop. Whatever requested the exit
          * will also have set something else (eg exit_request or
@@ -902,7 +1066,8 @@
      * execute we need to ensure we find/generate a TB with exactly
      * insns_left instructions in it.
      */
-    if (insns_left > 0 && insns_left < tb->icount)  {
+    if (insns_left > 0 && insns_left < tb->icount)
+    {
         assert(insns_left <= CF_COUNT_MASK);
         assert(cpu->icount_extra == 0);
         cpu->cflags_next_tb = (tb->cflags & ~CF_COUNT_MASK) | insns_left;
@@ -915,12 +1080,13 @@
 int cpu_exec(CPUState *cpu)
 {
     int ret;
-    SyncClocks sc = { 0 };
+    SyncClocks sc = {0};
 
     /* replay_interrupt may need current_cpu */
     current_cpu = cpu;
 
-    if (cpu_handle_halt(cpu)) {
+    if (cpu_handle_halt(cpu))
+    {
         return EXCP_HALTED;
     }
 
@@ -936,7 +1102,8 @@
     init_delay_params(&sc, cpu);
 
     /* prepare setjmp context for exception handling */
-    if (sigsetjmp(cpu->jmp_env, 0) != 0) {
+    if (sigsetjmp(cpu->jmp_env, 0) != 0)
+    {
 #if defined(__clang__)
         /*
          * Some compilers wrongly smash all local variables after
@@ -962,11 +1129,13 @@
         tcg_debug_assert(!have_mmap_lock());
 #endif
 
-        if (tracefile_enabled) {
+        if (tracefile_enabled)
+        {
             trace_at_fault(cpu->env_ptr);
         }
 
-        if (qemu_mutex_iothread_locked()) {
+        if (qemu_mutex_iothread_locked())
+        {
             qemu_mutex_unlock_iothread();
         }
         qemu_plugin_disable_mem_helpers(cpu);
@@ -975,11 +1144,13 @@
     }
 
     /* if an exception is pending, we execute it here */
-    while (!cpu_handle_exception(cpu, &ret)) {
+    while (!cpu_handle_exception(cpu, &ret))
+    {
         TranslationBlock *last_tb = NULL;
         int tb_exit = 0;
 
-        while (!cpu_handle_interrupt(cpu, &last_tb)) {
+        while (!cpu_handle_interrupt(cpu, &last_tb))
+        {
             TranslationBlock *tb;
             target_ulong cs_base, pc;
             uint32_t flags, cflags;
@@ -994,20 +1165,33 @@
              * does not require tcg headers for cpu_common_reset.
              */
             cflags = cpu->cflags_next_tb;
-            if (cflags == -1) {
+            if (cflags == -1)
+            {
                 cflags = curr_cflags(cpu);
-            } else {
+            }
+            else
+            {
                 cpu->cflags_next_tb = -1;
             }
 
-            if (check_for_breakpoints(cpu, pc, &cflags)) {
+            if (check_for_breakpoints(cpu, pc, &cflags))
+            {
                 break;
             }
 
             tb = tb_lookup(cpu, pc, cs_base, flags, cflags);
-            if (tb == NULL) {
+            if (tb == NULL)
+            {
+
                 mmap_lock();
+
                 tb = tb_gen_code(cpu, pc, cs_base, flags, cflags);
+
+                if (BITMAP_UPDATE_ONTRANSLATE)
+                {
+                    upd_bitmap(pc);
+                }
+
                 mmap_unlock();
                 /*
                  * We add the TB in the virtual pc hash table
@@ -1023,18 +1207,22 @@
              * direct jump to a TB spanning two pages because the mapping
              * for the second page can change.
              */
-            if (tb->page_addr[1] != -1) {
+            if (tb->page_addr[1] != -1)
+            {
                 last_tb = NULL;
             }
 #endif
+
+#ifdef NOPE_NOT_NEVER
             /* See if we can patch the calling TB. */
-            if (last_tb) {
-                if (!(tb->cflags & CF_INVALID)
-                    && !tracefile_history_for_tb(last_tb)) {
+            if (last_tb)
+            {
+                if (!(tb->cflags & CF_INVALID) && !tracefile_history_for_tb(last_tb))
+                {
                     tb_add_jump(last_tb, tb_exit, tb);
                 }
             }
-
+#endif
             cpu_loop_exec_tb(cpu, tb, &last_tb, &tb_exit);
 
             /* Try to align the host and virtual clocks
@@ -1054,7 +1242,8 @@
     static bool tcg_target_initialized;
     CPUClass *cc = CPU_GET_CLASS(cpu);
 
-    if (!tcg_target_initialized) {
+    if (!tcg_target_initialized)
+    {
         cc->tcg_ops->initialize();
         tcg_target_initialized = true;
     }
@@ -1081,18 +1270,22 @@
 
 void dump_drift_info(GString *buf)
 {
-    if (!icount_enabled()) {
+    if (!icount_enabled())
+    {
         return;
     }
 
-    g_string_append_printf(buf, "Host - Guest clock  %"PRIi64" ms\n",
+    g_string_append_printf(buf, "Host - Guest clock  %" PRIi64 " ms\n",
                            (cpu_get_clock() - icount_get()) / SCALE_MS);
-    if (icount_align_option) {
-        g_string_append_printf(buf, "Max guest delay     %"PRIi64" ms\n",
+    if (icount_align_option)
+    {
+        g_string_append_printf(buf, "Max guest delay     %" PRIi64 " ms\n",
                                -max_delay / SCALE_MS);
-        g_string_append_printf(buf, "Max guest advance   %"PRIi64" ms\n",
+        g_string_append_printf(buf, "Max guest advance   %" PRIi64 " ms\n",
                                max_advance / SCALE_MS);
-    } else {
+    }
+    else
+    {
         g_string_append_printf(buf, "Max guest delay     NA\n");
         g_string_append_printf(buf, "Max guest advance   NA\n");
     }
@@ -1102,7 +1295,8 @@
 {
     g_autoptr(GString) buf = g_string_new("");
 
-    if (!tcg_enabled()) {
+    if (!tcg_enabled())
+    {
         error_setg(errp, "JIT information is only available with accel=tcg");
         return NULL;
     }
@@ -1117,7 +1311,8 @@
 {
     g_autoptr(GString) buf = g_string_new("");
 
-    if (!tcg_enabled()) {
+    if (!tcg_enabled())
+    {
         error_setg(errp, "Opcode count information is only available with accel=tcg");
         return NULL;
     }
@@ -1165,8 +1360,7 @@
 static void trace_before_exec(TranslationBlock *tb)
 {
 #ifdef DEBUG_TRACE
-    printf("From " TARGET_FMT_lx " - "
-           TARGET_FMT_lx "\n", tb->pc, tb->pc + tb->size - 1);
+    printf("From " TARGET_FMT_lx " - " TARGET_FMT_lx "\n", tb->pc, tb->pc + tb->size - 1);
 #endif
     trace_current_tb = tb;
 }
@@ -1179,18 +1373,19 @@
     int exit_val = next_tb & TB_EXIT_MASK;
     int br = exit_val & (TB_EXIT_IDX1 | TB_EXIT_IDX0);
 
-    if (exit_val == TB_EXIT_ICOUNT_EXPIRED || exit_val == TB_EXIT_REQUESTED) {
+    if (exit_val == TB_EXIT_ICOUNT_EXPIRED || exit_val == TB_EXIT_REQUESTED)
+    {
         /* Those two values mean that the TB was not executed, see tcg.h for
          * details.
          */
         return;
     }
 
-
 #ifdef DEBUG_TRACE
     printf("... to " TARGET_FMT_lx " (" TARGET_FMT_lx ")",
            trace_current_tb->pc + trace_current_tb->size - 1, env->nip);
-    if (last_tb) {
+    if (last_tb)
+    {
         printf(" (last_ip=" TARGET_FMT_lx ", tflags=%04x)",
                last_tb->pc + last_tb->size - 1, last_tb->tflags);
     }
@@ -1198,18 +1393,20 @@
            br, trace_current_tb->tflags, trace_current->op);
 #endif
 
-    if (last_tb) {
+    if (last_tb)
+    {
         /* Last instruction is a branch (because last_tb is set).  */
         /* If last_tb != tb, then this is a threaded execution and tb has
            already been executed.  */
         unsigned char op = (1 << br);
 
-        if (last_tb == trace_current_tb) {
+        if (last_tb == trace_current_tb)
+        {
             op |= TRACE_OP_BLOCK;
         }
 
-        if ((last_tb->tflags & op) == op
-            && !tracefile_history_for_tb(last_tb)) {
+        if ((last_tb->tflags & op) == op && !tracefile_history_for_tb(last_tb))
+        {
             return;
         }
 
@@ -1217,11 +1414,14 @@
         trace_current->size = last_tb->size;
         trace_current->op = op;
         last_tb->tflags |= op;
-    } else {
+    }
+    else
+    {
         /* Note: if last_tb is not set, we don't know if we exited from tb
            or not.  We just know that tb has been executed and the last
            instruction was not a branch.  */
-        if (trace_current_tb->tflags & TRACE_OP_BLOCK) {
+        if (trace_current_tb->tflags & TRACE_OP_BLOCK)
+        {
             return;
         }
         trace_current->pc = trace_current_tb->pc;
@@ -1242,22 +1442,25 @@
 #ifdef DEBUG_TRACE
     printf("... fault at " TARGET_FMT_lx "\n", pc);
 #endif
-    if (trace_current_tb
-        && pc >= trace_current_tb->pc
-        && pc < trace_current_tb->pc + trace_current_tb->size) {
-        if (!tracefile_history_for_tb(trace_current_tb)
-            && (trace_current_tb->tflags & TRACE_OP_BLOCK)) {
+    if (trace_current_tb && pc >= trace_current_tb->pc && pc < trace_current_tb->pc + trace_current_tb->size)
+    {
+        if (!tracefile_history_for_tb(trace_current_tb) && (trace_current_tb->tflags & TRACE_OP_BLOCK))
+        {
             return;
         }
         trace_current->pc = trace_current_tb->pc;
         trace_current->op = TRACE_OP_FAULT;
         trace_current->size = pc - trace_current->pc;
-        if (trace_current->size == trace_current_tb->size) {
+        if (trace_current->size == trace_current_tb->size)
+        {
             trace_current->op = TRACE_OP_FAULT | TRACE_OP_BLOCK;
         }
-    } else {
+    }
+    else
+    {
         if (trace_current_tb &&
-            !tracefile_history_for_tb(trace_current_tb)) {
+            !tracefile_history_for_tb(trace_current_tb))
+        {
             /* Discard single fault.  */
             return;
         }
diff -ruN qemu/accel/tcg/Makefile qemu_patched/accel/tcg/Makefile
--- qemu/accel/tcg/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/accel/tcg/Makefile	2023-10-30 19:01:26.105790767 +0100
@@ -0,0 +1,5 @@
+afl-showmap: util-afl-showmap.c
+	gcc -o util-afl-showmap util-afl-showmap.c
+
+clean:
+	rm util-afl-showmap
diff -ruN qemu/accel/tcg/translate-all.c qemu_patched/accel/tcg/translate-all.c
--- qemu/accel/tcg/translate-all.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/accel/tcg/translate-all.c	2023-11-09 16:09:48.921589451 +0100
@@ -1383,6 +1383,7 @@
                               target_ulong pc, target_ulong cs_base,
                               uint32_t flags, int cflags)
 {
+
     CPUArchState *env = cpu->env_ptr;
     TranslationBlock *tb, *existing_tb;
     tb_page_addr_t phys_pc, phys_page2;
diff -ruN qemu/accel/tcg/util-afl-showmap.c qemu_patched/accel/tcg/util-afl-showmap.c
--- qemu/accel/tcg/util-afl-showmap.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/accel/tcg/util-afl-showmap.c	2023-10-30 19:01:26.105790767 +0100
@@ -0,0 +1,72 @@
+#include <sys/shm.h>
+#include <string.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <stdbool.h>
+
+#define MAP_SIZE_POW2       21
+#define MAP_SIZE            (1 << MAP_SIZE_POW2)
+
+
+bool isInteresting(bool curr_bitmap[], bool prev_bitmap[]) {
+
+	for (int i=0; i<MAP_SIZE; i++) {
+		if (curr_bitmap[i] != prev_bitmap[i]) return true;
+	}
+
+	return false;
+}
+
+int main() {
+
+	key_t shm_key;
+	int shm_id;
+	bool *afl_area_ptr;
+	bool curr_bitmap[MAP_SIZE];
+	bool prev_bitmap[MAP_SIZE];
+
+        /* Setup SHM */
+	shm_key = ftok("./afl_shm_bitmap", 'x');
+        shm_id = shmget(shm_key, MAP_SIZE, IPC_CREAT|IPC_EXCL|0777);
+	if (shm_id == -1) {
+                shm_id = shmget(shm_key, MAP_SIZE, 0);
+		if(shm_id == -1) {
+                        perror("Error in shmget()");
+                        return EXIT_FAILURE;
+                }
+        }
+
+	afl_area_ptr = shmat(shm_id, 0, 0);
+        if (afl_area_ptr == (void *) -1) {
+                perror("Error in shmat()");
+                return EXIT_FAILURE;
+        }
+
+	// Clean shared memory
+//	for (int i=0; i<MAP_SIZE; i++) {
+//		afl_area_ptr[i] = false;
+//	}
+
+	// Check bitmap
+	while(true) {
+
+		// Copy the current bitmap
+		memcpy(curr_bitmap, afl_area_ptr, MAP_SIZE);
+
+		// Compare with the previous bitmap
+		if (isInteresting(curr_bitmap, prev_bitmap)) {
+			printf("Changed bitmap locations: \n");
+			for (int i=0; i<MAP_SIZE; i++) {
+				if (curr_bitmap[i] != prev_bitmap[i]) 
+					printf("Location %d: %d\n", i, curr_bitmap[i]);
+			}
+		}
+
+		// Update the previous bitmap for the next iteration
+		memcpy(prev_bitmap, curr_bitmap, MAP_SIZE);
+
+	}
+
+        return 0;
+}
+
diff -ruN qemu/afl.h qemu_patched/afl.h
--- qemu/afl.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/afl.h	2023-10-30 19:01:26.105790767 +0100
@@ -0,0 +1,2 @@
+
+extern const char *aflFile;
\ Manca newline alla fine del file
diff -ruN qemu/hw/ppc/meson.build qemu_patched/hw/ppc/meson.build
--- qemu/hw/ppc/meson.build	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/hw/ppc/meson.build	2023-10-30 19:01:26.105790767 +0100
@@ -77,6 +77,7 @@
 ))
 ppc_ss.add(when: 'CONFIG_E500', if_true: files(
   'mpc8544_guts.c',
+  'p2010rdb.c',
   'ppce500_spin.c'
 ))
 # PowerPC 440 Xilinx ML507 reference board.
diff -ruN qemu/hw/ppc/p2010rdb.c qemu_patched/hw/ppc/p2010rdb.c
--- qemu/hw/ppc/p2010rdb.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/hw/ppc/p2010rdb.c	2023-10-30 19:01:26.105790767 +0100
@@ -0,0 +1,1210 @@
+/*
+ * QEMU PowerPC p2010rdb board emulation
+ *
+ * Copyright (C) 2011-2020 AdaCore
+ *
+ * This file is derived from hw/ppce500_mpc8544ds.c,
+ * the copyright for that material belongs to the original owners.
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of  the GNU General  Public License as published by
+ * the Free Software Foundation;  either version 2 of the  License, or
+ * (at your option) any later version.
+ */
+
+#include <dirent.h>
+
+#include "qemu/osdep.h"
+#include "qapi/error.h"
+#include "qemu-common.h"
+#include "net/net.h"
+#include "hw/hw.h"
+#include "hw/pci/pci.h"
+#include "hw/ppc/ppc.h"
+#include "hw/boards.h"
+#include "sysemu/sysemu.h"
+#include "sysemu/kvm.h"
+#include "kvm_ppc.h"
+#include "hw/ppc/openpic.h"
+#include "hw/loader.h"
+#include "elf.h"
+#include "hw/sysbus.h"
+#include "sysemu/block-backend.h"
+#include "sysemu/reset.h"
+#include "sysemu/runstate.h"
+#include "hw/block/flash.h"
+#include "hw/net/fsl_etsec/etsec.h"
+#include "exec/memory.h"
+#include "hw/char/serial.h"
+#include "hw/qdev-properties.h"
+
+#include "hw/adacore/gnat-bus.h"
+#include "qemu-traces.h"
+#include "hw/adacore/hostfs.h"
+
+/* #define DEBUG_P2010 */
+
+#define MAX_ETSEC_CONTROLLERS 3
+
+#define UIMAGE_LOAD_BASE           0
+#define DTC_LOAD_PAD               0x500000
+#define DTC_PAD_MASK               0xFFFFF
+#define INITRD_LOAD_PAD            0x2000000
+#define INITRD_PAD_MASK            0xFFFFFF
+
+/* Offsets in CCSRBAR */
+#define PARAMS_ADDR                 0x80000
+#define PARAMS_SIZE                 0x01000
+#define QTRACE_START                0x81000
+#define QTRACE_SIZE                 0x01000
+#define HOSTFS_START                0x82000
+
+#define P2010RDB_CCSRBAR_BASE       0xff700000
+#if 0
+#define P2010RDB_CCSRBAR_BASE       0xf3000000 /* u-boot 0xffe00000 */
+                                               /* vxworks bootapp 0xF3000000 */
+                                               /* vxworks653: 0xe0000000 */
+#endif
+#define P2010RDB_LOCAL_CONF         0x00000
+#define P2010RDB_DDR_CONTROLLER     0x02000
+#define P2010RDB_SERIAL0_REGS_BASE  0x04500
+#define P2010RDB_SERIAL1_REGS_BASE  0x04600
+#define P2010RDB_ELBC               0x05000
+#define P2010RDB_ESPI_BASE          0x07000
+#define P2010RDB_PCI_REGS_BASE      0x08000
+#define P2010RDB_GPIO               0x0F000
+#define P2010RDB_L2CACHE            0x20000
+#define P2010RDB_ETSEC1_BASE        0x24000
+#define P2010RDB_MPIC_REGS_BASE     0x40000
+#define P2010RDB_GLOBAL_UTILITIES   0xE0000
+#define P2010RDB_VSC7385            0xF1000000
+#define P2010RDB_VSC7385_SIZE       0x00020000
+#define P2010RDB_PCI_REGS_SIZE      0x1000
+#define P2010RDB_PCI_IO             0xE1000000
+#define P2010RDB_PCI_IOLEN          0x10000
+
+#ifdef DEBUG_P2010
+#define PRINT_WRITE_UNSUPPORTED_REGISTER(name, addr, val, nip)          \
+do {                                                                    \
+ printf("%s\t: Unsupported addr:0x" TARGET_FMT_plx " val:0x%08lx nip:"  \
+        TARGET_FMT_lx " '" name "'\n",                                  \
+        __func__, (addr), (long unsigned int)(val), (nip));             \
+ } while (0)
+
+#define PRINT_READ_UNSUPPORTED_REGISTER(name, addr, nip)                \
+do {                                                                    \
+    printf("%s\t: Unsupported addr:0x" TARGET_FMT_plx "                nip:" \
+           TARGET_FMT_lx " '" name "'\n",                               \
+           __func__, (addr), (nip));                                    \
+ } while (0)
+
+#define PRINT_SUPPORTED_REGISTER(name, addr, val, nip)                  \
+do {                                                                    \
+    printf("%s\t: Supported addr:0x" TARGET_FMT_plx " val:0x%08lx nip:" \
+           TARGET_FMT_lx " '" name "'\n",                               \
+           __func__, (addr), (long unsigned int)(val), (nip));          \
+ } while (0)
+
+#else /* DEBUG_P2010 */
+#define PRINT_WRITE_UNSUPPORTED_REGISTER(name, addr, val, nip)  \
+do {                                                            \
+    (void)(name); (void)(addr); (void)(val); (void)(nip);       \
+ } while (0)
+#define PRINT_READ_UNSUPPORTED_REGISTER(name, addr, nip)        \
+do {                                                            \
+    (void)(name); (void)(addr); (void)(nip);                    \
+ } while (0)
+#define PRINT_SUPPORTED_REGISTER(name, addr, val, nip)          \
+do {                                                            \
+    (void)(name); (void)(addr); (void)(val); (void)(nip);       \
+ } while (0)
+
+#endif  /* DEBUG_P2010 */
+
+typedef struct ResetData {
+    PowerPCCPU  *cpu;
+    uint32_t     entry;         /* save kernel entry in case of reset */
+} ResetData;
+
+typedef struct fsl_e500_config {
+    uint32_t ccsr_init_addr;
+    const char *cpu_model;
+    uint32_t freq;
+
+    int serial_irq;
+
+    int cfi01_flash;
+
+    int etsec_irq_err[MAX_ETSEC_CONTROLLERS];
+    int etsec_irq_tx[MAX_ETSEC_CONTROLLERS];
+    int etsec_irq_rx[MAX_ETSEC_CONTROLLERS];
+} fsl_e500_config;
+
+static MemoryRegion *ccsr_space;
+static uint64_t      ccsr_addr = P2010RDB_CCSRBAR_BASE;
+
+static void sec_cpu_reset(void *opaque)
+{
+    PowerPCCPU *cpu = opaque;
+    CPUState *cs = CPU(cpu);
+
+    cpu_reset(cs);
+
+    /* Secondary CPU starts in halted state for now. Needs to change when
+       implementing non-kernel boot. */
+    cs->halted = 1;
+    cs->exception_index = EXCP_HLT;
+}
+
+static void main_cpu_reset(void *opaque)
+{
+    ResetData    *s    = (ResetData *)opaque;
+    PowerPCCPU   *cpu  = s->cpu;
+    CPUPPCState  *env  = &cpu->env;
+    ppcmas_tlb_t *tlb1 = booke206_get_tlbm(env, 1, 0, 0);
+    ppcmas_tlb_t *tlb2 = booke206_get_tlbm(env, 1, 0, 1);
+    hwaddr        size;
+
+    cpu_reset(CPU(cpu));
+
+    env->nip = s->entry;
+    env->tlb_dirty = true;
+
+    /* Init tlb1 entry 0 (e500 )*/
+    size = 0x1 << MAS1_TSIZE_SHIFT; /* 4 KBytes */
+    size <<= 1;                     /* Shift because MAS2's TSIZE field is
+                                     * implemented as PowerISA MAV=2.0
+                                     * compliant.
+                                     */
+    tlb1->mas1    = MAS1_VALID | MAS1_IPROT | size;
+    tlb1->mas2    = 0xfffff000 &   TARGET_PAGE_MASK;
+    tlb1->mas7_3  = 0xfffff000 & TARGET_PAGE_MASK;
+    tlb1->mas7_3 |= MAS3_UR | MAS3_UW | MAS3_UX | MAS3_SR | MAS3_SW | MAS3_SX;
+
+    /* Init tlb1 entry 1 (p2010rdb)*/
+    size = 0xb << MAS1_TSIZE_SHIFT; /* 4Gbytes */
+    size <<= 1;                     /* Shift because MAS2's TSIZE field is
+                                     * implemented as PowerISA MAV=2.0
+                                     * compliant.
+                                     */
+    tlb2->mas1    = MAS1_VALID | MAS1_IPROT | size;
+    tlb2->mas2    = 0x0 &   TARGET_PAGE_MASK;
+    tlb2->mas7_3  = 0x0 & TARGET_PAGE_MASK;
+    tlb2->mas7_3 |= MAS3_UR | MAS3_UW | MAS3_UX | MAS3_SR | MAS3_SW | MAS3_SX;
+
+    /*
+     * I'm still not sure to understand why, but the bootApp set PID to 1 before
+     * starting the kernel.
+     */
+    env->spr[SPR_BOOKE_PID] = 0x1;
+}
+
+uint32_t g_DEVDISR  = 0x0;
+uint32_t g_DDRDLLCR = 0x0;
+uint32_t g_LBDLLCR  = 0x0;
+
+static uint64_t p2010_gbu_read(void *opaque, hwaddr addr, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_GLOBAL_UTILITIES + ccsr_addr;
+
+    switch (addr) {
+    case 0x0:
+        /* e500_1_Ratio = 100
+         * e500_0_Ratio = 100
+         * DDR_Ratio = 1010
+         * plat_Ratio = 101
+         */
+        return 0x0404140a;
+        break;
+    case 0xa0:                  /* PVR */
+        return env->spr[SPR_PVR];
+        break;
+    case 0xa4:                  /* SVR */
+        return env->spr[SPR_E500_SVR];
+        break;
+    case 0x70:
+        return g_DEVDISR;
+        break;
+    case 0xe10:
+        PRINT_READ_UNSUPPORTED_REGISTER("DDRLLCR", full_addr, env->nip);
+        return g_DDRDLLCR | 0x100;
+        break;
+    case 0xe20:
+        PRINT_READ_UNSUPPORTED_REGISTER("LBDLLCR", full_addr, env->nip);
+        return g_LBDLLCR | 0x100;
+        break;
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_gbu_write(void *opaque, hwaddr addr,
+                            uint64_t val, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_GLOBAL_UTILITIES + ccsr_addr;
+
+    switch (addr) {
+    case 0xb0:
+        if (val & 2) {
+            qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);
+        }
+        break;
+    case 0x70:
+        g_DEVDISR = val;
+        break;
+    case 0xe10:
+        g_DDRDLLCR = val;
+        PRINT_WRITE_UNSUPPORTED_REGISTER("DDRLLCR", full_addr , val, env->nip);
+        break;
+    case 0xe20:
+        g_LBDLLCR = val;
+        PRINT_WRITE_UNSUPPORTED_REGISTER("LBDLLCR", full_addr , val, env->nip);
+        break;
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("? Unknown ?",
+                                         full_addr , val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_gbu_ops = {
+    .read = p2010_gbu_read,
+    .write = p2010_gbu_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 1,
+        .max_access_size = 4,
+    },
+};
+
+static uint64_t p2010_lca_read(void *opaque, hwaddr addr, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_LOCAL_CONF + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x0:
+        PRINT_SUPPORTED_REGISTER("CCSRBAR",
+                                 full_addr, ccsr_addr >> 12, env->nip);
+        return ccsr_addr >> 12;
+        break;
+    case 0xc08 ... 0xd70:
+        PRINT_READ_UNSUPPORTED_REGISTER("Local access window",
+                                        full_addr, env->nip);
+        break;
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_lca_write(void *opaque, hwaddr addr,
+                            uint64_t val, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_LOCAL_CONF + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x0:
+        ccsr_addr = val << 12;
+        memory_region_del_subregion(get_system_memory(), ccsr_space);
+        memory_region_add_subregion(get_system_memory(), ccsr_addr, ccsr_space);
+        PRINT_SUPPORTED_REGISTER("CCSRBAR", full_addr, val << 12, env->nip);
+        break;
+
+    case 0xc08 ... 0xd70:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Local access window",
+                                         full_addr, val, env->nip);
+        break;
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("? Unknown ?",
+                                         full_addr, val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_lca_ops = {
+    .read = p2010_lca_read,
+    .write = p2010_lca_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static uint64_t p2010_elbc_read(void *opaque, hwaddr addr, unsigned size)
+{
+    CPUPPCState *env       = opaque;
+    hwaddr       full_addr = (addr & 0xfff) + P2010RDB_ELBC + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x00:    case 0x08:    case 0x10:    case 0x18:
+    case 0x20:    case 0x28:    case 0x30:    case 0x38:
+        PRINT_READ_UNSUPPORTED_REGISTER("Base registers", full_addr, env->nip);
+        break;
+
+    case 0x04:    case 0x0c:    case 0x14:    case 0x1c:
+    case 0x24:    case 0x2c:    case 0x34:    case 0x3c:
+        PRINT_READ_UNSUPPORTED_REGISTER("Base registers", full_addr, env->nip);
+        break;
+
+    case 0xd4:
+        PRINT_READ_UNSUPPORTED_REGISTER("Clock Ratio Register",
+                                        full_addr, env->nip);
+        break;
+
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_elbc_write(void *opaque, hwaddr addr,
+                             uint64_t val, unsigned size)
+{
+    CPUPPCState *env       = opaque;
+    hwaddr       full_addr = (addr & 0xfff) + P2010RDB_ELBC + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x00:    case 0x08:    case 0x10:    case 0x18:
+    case 0x20:    case 0x28:    case 0x30:    case 0x38:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Base registers",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x04:    case 0x0c:    case 0x14:    case 0x1c:
+    case 0x24:    case 0x2c:    case 0x34:    case 0x3c:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Base registers",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0xd4:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Clock Ratio Register",
+                                         full_addr, val, env->nip);
+        break;
+
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("? Unknown ?",
+                                         full_addr, val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_elbc_ops = {
+    .read = p2010_elbc_read,
+    .write = p2010_elbc_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static uint64_t p2010_ddr_read(void *opaque, hwaddr addr,
+                               unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_DDR_CONTROLLER + ccsr_addr;
+
+    switch (addr & 0xfff) {
+
+    case 0x00:        case 0x08:        case 0x10:         case 0x18:
+        PRINT_READ_UNSUPPORTED_REGISTER("Chip Select Memory Bounds",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x80 ... 0x8c:
+    case 0xc0 ... 0xcc:
+        PRINT_READ_UNSUPPORTED_REGISTER("Chip Select Config",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x110 ... 0x11c:
+        PRINT_READ_UNSUPPORTED_REGISTER("Control Config",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x120:
+        PRINT_READ_UNSUPPORTED_REGISTER("Mode Control",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x124:
+        PRINT_READ_UNSUPPORTED_REGISTER("Interval Configuration",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x128:
+        PRINT_READ_UNSUPPORTED_REGISTER("Data Initialization",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x130:
+        PRINT_READ_UNSUPPORTED_REGISTER("Clock Control",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x100 ... 0x10c:
+    case 0x160 ... 0x164:
+        PRINT_READ_UNSUPPORTED_REGISTER("Timing config",
+                                        full_addr, env->nip);
+        break;
+
+    case 0x170:
+        PRINT_READ_UNSUPPORTED_REGISTER("Calibration Control",
+                                        full_addr, env->nip);
+
+    case 0x174:
+        PRINT_READ_UNSUPPORTED_REGISTER("Write Leveling Control",
+                                        full_addr, env->nip);
+        break;
+
+    case 0xe48:
+        PRINT_READ_UNSUPPORTED_REGISTER("Memory error interrupt enable",
+                                        full_addr, env->nip);
+        break;
+
+    case 0xe58:
+        PRINT_READ_UNSUPPORTED_REGISTER("Single-Bit ECC memory error mgmt",
+                                        full_addr, env->nip);
+        break;
+
+    case 0xb28 ... 0xb2c:
+        PRINT_READ_UNSUPPORTED_REGISTER("Control Driver Register",
+                                        full_addr, env->nip);
+        break;
+
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_ddr_write(void *opaque, hwaddr addr,
+                            uint64_t val, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_DDR_CONTROLLER + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x00:        case 0x08:        case 0x10:         case 0x18:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Chip Select Memory Bounds",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x80 ... 0x8c:
+    case 0xc0 ... 0xcc:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Chip Select Config",
+                                        full_addr, val, env->nip);
+        break;
+
+    case 0x110 ... 0x11c:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Control Config",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x120:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Mode Control",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x124:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Interval Configuration",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x128:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Data Initialization",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x130:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Clock Control",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x100 ... 0x10c:
+    case 0x160 ... 0x164:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Timing config",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x170:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Calibration Control",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0x174:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Write Leveling Control",
+                                         full_addr, val, env->nip);
+        break;
+
+    case 0xe48:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Memory error interrupt enable",
+                                        full_addr, val, env->nip);
+        break;
+
+    case 0xe58:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Single-Bit ECC memory error mgmt",
+                                        full_addr, val, env->nip);
+        break;
+
+    case 0xb28 ... 0xb2c:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("Control Driver Register",
+                                        full_addr, val, env->nip);
+        break;
+
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER("? Unknown ?",
+                                         full_addr, val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_ddr_ops = {
+    .read = p2010_ddr_read,
+    .write = p2010_ddr_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static uint64_t p2010_gpio_read(void *opaque, hwaddr addr,
+                                unsigned size)
+{
+    CPUPPCState *env       = opaque;
+    hwaddr       full_addr = (addr & 0xfff) + P2010RDB_GPIO + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_gpio_write(void *opaque, hwaddr addr,
+                             uint64_t val, unsigned size)
+{
+    CPUPPCState *env       = opaque;
+    hwaddr       full_addr = (addr & 0xfff) + P2010RDB_GPIO + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "? Unknown ?", full_addr, val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_gpio_ops = {
+    .read = p2010_gpio_read,
+    .write = p2010_gpio_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static uint64_t p2010_l2cache_read(void *opaque, hwaddr addr,
+                                   unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_L2CACHE + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x00:
+        PRINT_READ_UNSUPPORTED_REGISTER(
+            "L2 Control Register (return 0x20000000)", full_addr, env->nip);
+        return 0x20000000;
+        break;
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_l2cache_write(void *opaque, hwaddr addr,
+                                uint64_t val, unsigned size)
+{
+    CPUPPCState *env       = opaque;
+    hwaddr       full_addr = (addr & 0xfff) + P2010RDB_L2CACHE + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    case 0x00:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 Control Register",
+            full_addr, val, env->nip);
+        break;
+    case 0x10:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register 0",
+            full_addr, val, env->nip);
+        break;
+    case 0x14:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register extended address 0",
+            full_addr, val, env->nip);
+        break;
+    case 0x18:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write control register 0",
+            full_addr, val, env->nip);
+        break;
+    case 0x20:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register 1",
+            full_addr, val, env->nip);
+        break;
+    case 0x24:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register extended address 1",
+            full_addr, val, env->nip);
+        break;
+    case 0x28:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write control register 1",
+            full_addr, val, env->nip);
+        break;
+    case 0x30:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register 2",
+            full_addr, val, env->nip);
+        break;
+    case 0x34:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register extended address 2",
+            full_addr, val, env->nip);
+        break;
+    case 0x38:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write control register 2",
+            full_addr, val, env->nip);
+        break;
+    case 0x40:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register 3",
+            full_addr, val, env->nip);
+        break;
+    case 0x44:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write address register extended address 3",
+            full_addr, val, env->nip);
+        break;
+    case 0x48:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 cache external write control register 3",
+            full_addr, val, env->nip);
+        break;
+    case 0x100:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 memory-mapped SRAM base address register 0",
+            full_addr, val, env->nip);
+        break;
+    case 0x104:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 memory-mapped SRAM base address register extended address 0",
+            full_addr, val, env->nip);
+        break;
+    case 0x108:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 memory-mapped SRAM base address register 1",
+            full_addr, val, env->nip);
+        break;
+    case 0x10c:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "L2 memory-mapped SRAM base address register extended address 1",
+            full_addr, val, env->nip);
+        break;
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "? Unknown ?", full_addr, val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_l2cache_ops = {
+    .read = p2010_l2cache_read,
+    .write = p2010_l2cache_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static uint64_t p2010_vsc7385_read(void *opaque, hwaddr addr, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_VSC7385 + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    default:
+        PRINT_READ_UNSUPPORTED_REGISTER("? Unknown ?", full_addr, env->nip);
+    }
+    return 0;
+}
+
+static void p2010_vsc7385_write(void *opaque, hwaddr addr,
+                                uint64_t val, unsigned size)
+{
+    CPUPPCState *env  = opaque;
+    hwaddr  full_addr = (addr & 0xfff) + P2010RDB_VSC7385 + ccsr_addr;
+
+    switch (addr & 0xfff) {
+    default:
+        PRINT_WRITE_UNSUPPORTED_REGISTER(
+            "? Unknown ?", full_addr, val, env->nip);
+    }
+}
+
+static const MemoryRegionOps p2010_vsc7385_ops = {
+    .read = p2010_vsc7385_read,
+    .write = p2010_vsc7385_write,
+    .endianness = DEVICE_NATIVE_ENDIAN,
+    .impl = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static void write_qtrace(void *opaque, hwaddr addr, uint64_t value,
+                         unsigned size)
+{
+    switch (addr & 0xfff) {
+    case 0x00:
+        exec_trace_special(TRACE_SPECIAL_LOADADDR, value);
+        break;
+    default:
+#ifdef DEBUG_P2010
+        printf("%s: writing non implemented register 0x" TARGET_FMT_plx "\n",
+               __func__, QTRACE_START + addr);
+#endif
+        break;
+    }
+}
+
+static const MemoryRegionOps qtrace_ops = {
+    .write = write_qtrace,
+    .endianness = DEVICE_BIG_ENDIAN,
+    .valid = {
+        .min_access_size = 4,
+        .max_access_size = 4,
+    },
+};
+
+static qemu_irq *init_mpic(MemoryRegion *ccsr, qemu_irq **irqs,
+                           unsigned int smp_cpus)
+{
+    qemu_irq *mpic;
+    DeviceState *dev = NULL;
+    SysBusDevice *s;
+    int i, j, k;
+
+    mpic = g_new(qemu_irq, 256);
+
+    dev = qdev_new(TYPE_OPENPIC);
+    qdev_prop_set_uint32(dev, "model", OPENPIC_MODEL_FSL_MPIC_20);
+    qdev_prop_set_uint32(dev, "nb_cpus", 1);
+
+    //qdev_init_nofail(dev);
+    s = SYS_BUS_DEVICE(dev);
+    sysbus_realize_and_unref(s, &error_fatal);
+
+    k = 0;
+    for (i = 0; i < smp_cpus; i++) {
+        for (j = 0; j < OPENPIC_OUTPUT_NB; j++) {
+            sysbus_connect_irq(s, k++, irqs[i][j]);
+        }
+    }
+
+    for (i = 0; i < 256; i++) {
+        mpic[i] = qdev_get_gpio_in(dev, i);
+    }
+
+    memory_region_add_subregion(ccsr, P2010RDB_MPIC_REGS_BASE,
+                                s->mmio[0].memory);
+
+    return mpic;
+}
+
+static void fsl_e500_init(fsl_e500_config *config, MachineState *machine)
+{
+    CPUPPCState *env = NULL;
+    CPUPPCState *firstenv = NULL;
+    uint64_t elf_entry;
+    uint64_t elf_lowaddr;
+    const char *kernel_filename = machine->kernel_filename;
+    target_long kernel_size = 0;
+    target_ulong dt_base = 0;
+    int bios_size = 0;
+    MemoryRegion *ram, *misc_io;
+    MemoryRegion *params = g_new(MemoryRegion, 1);
+    qemu_irq *mpic, **irqs;
+    ResetData *reset_info;
+    DriveInfo *dinfo;
+    int i;
+
+    /* Setup CPU */
+    if (machine->cpu_type == NULL) {
+        if (config->cpu_model != NULL) {
+            machine->cpu_type = config->cpu_model;
+        } else {
+            machine->cpu_type = "e500v2_v30";
+        }
+    }
+
+    reset_info = g_malloc0(sizeof(ResetData));
+
+    irqs = g_malloc0(machine->smp.cpus * sizeof(qemu_irq *));
+    irqs[0] = g_malloc0(machine->smp.cpus * sizeof(qemu_irq)
+                                          * OPENPIC_OUTPUT_NB);
+    for (i = 0; i < machine->smp.cpus; i++) {
+        PowerPCCPU *cpu;
+        CPUState *cs;
+        qemu_irq *input;
+
+        cpu = POWERPC_CPU(cpu_create(machine->cpu_type));
+
+        if (cpu == NULL) {
+            fprintf(stderr, "Unable to initialize CPU!\n");
+            exit(1);
+        }
+        env = &cpu->env;
+        cs = CPU(cpu);
+
+        if (!firstenv) {
+            firstenv = env;
+        }
+
+        irqs[i] = irqs[0] + (i * OPENPIC_OUTPUT_NB);
+        input = (qemu_irq *)env->irq_inputs;
+        irqs[i][OPENPIC_OUTPUT_INT] = input[PPCE500_INPUT_INT];
+        irqs[i][OPENPIC_OUTPUT_CINT] = input[PPCE500_INPUT_CINT];
+        env->spr[SPR_BOOKE_PIR] = cs->cpu_index = i;
+        env->mpic_iack = config->ccsr_init_addr +
+            P2010RDB_MPIC_REGS_BASE + 0xa0;
+
+        ppc_booke_timers_init(cpu, config->freq, PPC_TIMER_E500);
+
+        /* Register reset handler */
+        if (!i) {
+            /* Primary CPU */
+            reset_info->cpu = cpu;
+            qemu_register_reset(main_cpu_reset, reset_info);
+
+            reset_info->entry = 0xfffffffc;
+
+        } else {
+            /* Secondary CPUs */
+            qemu_register_reset(sec_cpu_reset, cpu);
+        }
+    }
+
+    /* Register Memory */
+    ram = g_malloc0(sizeof(*ram));
+    memory_region_init_ram(ram, NULL, "p2010.ram", machine->ram_size, &error_abort);
+    memory_region_add_subregion(get_system_memory(), 0x0, ram);
+
+    ram = g_malloc0(sizeof(*ram));
+    memory_region_init_ram(ram, NULL, "p2010.ram_L2",
+                           0x1000000 /* 512 * 1024 */, &error_abort);
+    memory_region_add_subregion(get_system_memory(),
+                                0xf8000000 /* 0xf8b00000 */, ram);
+
+    ccsr_addr = config->ccsr_init_addr;
+
+    /* Configuration, Control, and Status Registers */
+    ccsr_space = g_malloc0(sizeof(*ccsr_space));
+    memory_region_init(ccsr_space, NULL, "CCSR_space", 0x100000);
+    memory_region_add_subregion_overlap(get_system_memory(), ccsr_addr,
+                                        ccsr_space, 2);
+
+    /* Global Utilities */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_gbu_ops, env,
+                          "Global Utilities", 0x1000);
+    memory_region_add_subregion(ccsr_space, P2010RDB_GLOBAL_UTILITIES, misc_io);
+
+    /* Local configuration/access */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_lca_ops, env,
+                          "Local configuration/access", 0x1000);
+    memory_region_add_subregion(ccsr_space, P2010RDB_LOCAL_CONF, misc_io);
+
+    /* eLBC */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_elbc_ops, env, "eLBC", 0x1000);
+    memory_region_add_subregion(ccsr_space, P2010RDB_ELBC, misc_io);
+
+    /* DDR Memory Controller */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_ddr_ops, env,
+                          "DDR memory controller", 0x1000);
+    memory_region_add_subregion(ccsr_space, P2010RDB_DDR_CONTROLLER, misc_io);
+
+    /* GPIO */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_gpio_ops, env, "GPIO", 0x1000);
+    memory_region_add_subregion(ccsr_space, P2010RDB_GPIO, misc_io);
+
+    /* L2CACHE */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_l2cache_ops, env, "L2CACHE",
+                          0x1000);
+    memory_region_add_subregion(ccsr_space, P2010RDB_L2CACHE, misc_io);
+
+    /* VSC7385 */
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &p2010_vsc7385_ops, env,
+                          "VSC7385", P2010RDB_VSC7385_SIZE);
+    memory_region_add_subregion(get_system_memory(), P2010RDB_VSC7385, misc_io);
+
+    /* MPIC */
+    mpic = init_mpic(ccsr_space, irqs, machine->smp.cpus);
+
+    if (!mpic) {
+        cpu_abort(CPU(env), "MPIC failed to initialize\n");
+    }
+
+    /* Serial */
+    if (serial_hd(0)) {
+        serial_mm_init(ccsr_space, P2010RDB_SERIAL0_REGS_BASE,
+                       0, mpic[config->serial_irq], 399193,
+                       serial_hd(0), DEVICE_BIG_ENDIAN);
+    }
+
+    if (serial_hd(1)) {
+        serial_mm_init(ccsr_space, P2010RDB_SERIAL1_REGS_BASE,
+                       0, mpic[config->serial_irq], 399193,
+                       serial_hd(1), DEVICE_BIG_ENDIAN);
+    }
+
+    for (i = 0; i < nb_nics && i < MAX_ETSEC_CONTROLLERS; i++) {
+        etsec_create(P2010RDB_ETSEC1_BASE + 0x1000 * i, ccsr_space,
+                     &nd_table[i],
+                     mpic[config->etsec_irq_tx[i]],
+                     mpic[config->etsec_irq_rx[i]],
+                     mpic[config->etsec_irq_err[i]]);
+    }
+
+    /* Load pflash after serial initialization because pflash_cfi02_register
+     * causes a race condition in monitor initialization which makes the monitor
+     * to print in stdout. This is because the monitor is not in mux mode until
+     * the serial port is started.
+     */
+
+    /* allocate and load BIOS */
+    dinfo           = drive_get(IF_PFLASH, 0, 0);
+    if (dinfo) {
+        BlockBackend *blk = blk_by_legacy_dinfo(dinfo);
+        bios_size   = blk_getlength(blk);
+
+        if (config->cfi01_flash) {
+            if (pflash_cfi01_register(0xff800000, "8548.flash.1",
+                                      bios_size, blk, 65536,
+                                      2, 0x0001, 0x22DA, 0x0000, 0x0000,
+                                      1) == NULL) {
+                fprintf(stderr, "%s: Failed to load flash image\n", __func__);
+                abort();
+            }
+        } else {
+            if (pflash_cfi02_register((uint32_t)(-bios_size),
+                                      "p2010.bios", bios_size, blk,
+                                      65536, 1, 2, 0x0001, 0x22DA,
+                                      0x0, 0x0, 0x555, 0x2AA, 1) == NULL) {
+                fprintf(stderr, "%s: Failed to load flash image\n", __func__);
+                abort();
+            }
+        }
+
+    }
+
+    /* Load kernel. */
+    if (kernel_filename) {
+        kernel_size = load_elf(kernel_filename, NULL, NULL, NULL, &elf_entry,
+                               &elf_lowaddr, NULL, NULL, 1, PPC_ELF_MACHINE,
+                               0, 0);
+
+        /* XXX try again as binary */
+        if (kernel_size < 0) {
+            fprintf(stderr, "qemu: could not load kernel '%s'\n",
+                    kernel_filename);
+            exit(1);
+        }
+    }
+
+    /* If we're loading a kernel directly, we must load the device tree too. */
+    if (kernel_filename) {
+        dt_base = (kernel_size + DTC_LOAD_PAD) & ~DTC_PAD_MASK;
+
+        /* Set initial guest state. */
+        env->gpr[1] = elf_lowaddr + 4 * 1024 * 1024; /* FIXME: sp? */
+        env->gpr[3] = dt_base;
+        env->nip = elf_entry;    /* FIXME: entry? */
+        reset_info->entry = elf_entry;
+    }
+
+    /* Set params.  */
+    memory_region_init_ram(params, NULL, "p2010rdb.params", PARAMS_SIZE,
+                           &error_abort);
+    memory_region_add_subregion(ccsr_space, PARAMS_ADDR, params);
+
+    if (machine->kernel_cmdline) {
+        cpu_physical_memory_write(ccsr_addr + PARAMS_ADDR,
+                                  machine->kernel_cmdline,
+                                  strlen(machine->kernel_cmdline) + 1);
+    } else {
+        stb_phys(CPU(env)->as, ccsr_addr + PARAMS_ADDR, 0);
+    }
+
+    /* Set read-only after writing command line */
+    memory_region_set_readonly(params, true);
+
+    /* Qtrace*/
+    misc_io = g_malloc0(sizeof(*misc_io));
+    memory_region_init_io(misc_io, NULL, &qtrace_ops, env,
+                          "Exec-traces", QTRACE_SIZE);
+    memory_region_add_subregion(ccsr_space, QTRACE_START, misc_io);
+
+    /* HostFS */
+    hostfs_create(HOSTFS_START, ccsr_space);
+
+    /* Initialize the GnatBus Master */
+    gnatbus_master_init(mpic, 16);
+    gnatbus_device_init();
+}
+
+/******************************************************************************/
+
+static fsl_e500_config p2010rdb_vxworks_config = {
+    .ccsr_init_addr = 0xf3000000,
+    .cpu_model      = POWERPC_CPU_TYPE_NAME("p2010"),
+    .freq           = 700000000UL >> 3,
+    .serial_irq     = 16 + 26,
+    .etsec_irq_err  = {16 + 18, 16 + 24, 16 + 17},
+    .etsec_irq_tx   = {16 + 13, 16 + 19, 16 + 15},
+    .etsec_irq_rx   = {16 + 14, 16 + 20, 16 + 16},
+    .cfi01_flash    = FALSE,
+};
+
+static void p2010rdb_vxworks_generic_hw_init(MachineState *machine)
+{
+    fsl_e500_init(&p2010rdb_vxworks_config, machine);
+}
+
+static void p2010rdb_vxworks_generic_machine_init(MachineClass *mc)
+{
+    mc->desc = "p2010rdb initialized for VxWorks6";
+    mc->init = p2010rdb_vxworks_generic_hw_init;
+}
+
+DEFINE_MACHINE("p2010rdb_vxworks", p2010rdb_vxworks_generic_machine_init)
+
+/******************************************************************************/
+
+
+static fsl_e500_config p2010rdb_config = {
+    .ccsr_init_addr = 0xff700000,
+    .cpu_model      = POWERPC_CPU_TYPE_NAME("p2010"),
+    .freq           = 700000000UL >> 3,
+    .serial_irq     = 16 + 26,
+    .etsec_irq_err  = {16 + 18, 16 + 24, 16 + 17},
+    .etsec_irq_tx   = {16 + 13, 16 + 19, 16 + 15},
+    .etsec_irq_rx   = {16 + 14, 16 + 20, 16 + 16},
+    .cfi01_flash    = FALSE,
+};
+
+
+static void p2010rdb_generic_hw_init(MachineState *machine)
+{
+    fsl_e500_init(&p2010rdb_config, machine);
+}
+
+static void p2010rdb_generic_machine_init(MachineClass *mc)
+{
+    mc->desc = "p2010rdb reset state";
+    mc->init = p2010rdb_generic_hw_init;
+}
+
+DEFINE_MACHINE("p2010rdb", p2010rdb_generic_machine_init)
+
+/******************************************************************************/
+
+static fsl_e500_config wrsbc8548_vxworks_config = {
+    .ccsr_init_addr = 0xE0000000,
+    .cpu_model      = POWERPC_CPU_TYPE_NAME("p2010"),
+    .freq           = 700000000UL >> 3,
+    .serial_irq     = 16 + 26,
+    .etsec_irq_err  = {16 + 18, 16 + 24, 16 + 17},
+    .etsec_irq_tx   = {16 + 13, 16 + 19, 16 + 15},
+    .etsec_irq_rx   = {16 + 14, 16 + 20, 16 + 16},
+    .cfi01_flash    = TRUE,
+};
+
+static void wrsbc8548_vxworks_generic_hw_init(MachineState *machine)
+{
+    fsl_e500_init(&wrsbc8548_vxworks_config, machine);
+}
+
+static void wrsbc8548_vxworks_generic_machine_init(MachineClass *mc)
+{
+    mc->desc = "wrsbc8548 initialized for VxWorks6";
+    mc->init = wrsbc8548_vxworks_generic_hw_init;
+}
+
+DEFINE_MACHINE("wrsbc8548_vxworks", wrsbc8548_vxworks_generic_machine_init)
+
+/******************************************************************************/
+
+static fsl_e500_config wrsbc8548_vx653_config = {
+    .ccsr_init_addr = 0xE0000000,
+    .cpu_model      = POWERPC_CPU_TYPE_NAME("p2010"),
+    .freq           = 700000000UL >> 3,
+    .serial_irq     = 16 + 26,
+    .etsec_irq_err  = {16 + 18, 16 + 24, 16 + 17},
+    .etsec_irq_tx   = {16 + 13, 16 + 19, 16 + 15},
+    .etsec_irq_rx   = {16 + 14, 16 + 20, 16 + 16},
+    .cfi01_flash    = TRUE,
+};
+
+static void wrsbc8548_vx653_generic_hw_init(MachineState *machine)
+{
+    fsl_e500_init(&wrsbc8548_vx653_config, machine);
+}
+
+static void wrsbc8548_vx653_generic_machine_init(MachineClass *mc)
+{
+    mc->desc = "wrsbc8548 initialized for VxWorks653";
+    mc->init = wrsbc8548_vx653_generic_hw_init;
+}
+
+DEFINE_MACHINE("wrsbc8548_vx653", wrsbc8548_vx653_generic_machine_init)
+
+/******************************************************************************/
+
+static fsl_e500_config wrsbc8548_config = {
+    .ccsr_init_addr = 0xff700000,
+    .cpu_model      = POWERPC_CPU_TYPE_NAME("p2010"),
+    .freq           = 700000000UL >> 3,
+    .serial_irq     = 16 + 26,
+    .etsec_irq_err  = {16 + 18, 16 + 24, 16 + 17},
+    .etsec_irq_tx   = {16 + 13, 16 + 19, 16 + 15},
+    .etsec_irq_rx   = {16 + 14, 16 + 20, 16 + 16},
+    .cfi01_flash    = TRUE,
+};
+
+static void wrsbc8548_generic_hw_init(MachineState *machine)
+{
+    fsl_e500_init(&wrsbc8548_config, machine);
+}
+
+static void wrsbc8548_generic_machine_init(MachineClass *mc)
+{
+    mc->desc = "wrsbc8548";
+    mc->init = wrsbc8548_generic_hw_init;
+}
+
+DEFINE_MACHINE("wrsbc8548", wrsbc8548_generic_machine_init)
+
+/******************************************************************************/
diff -ruN qemu/include/libAFL/aflpp.h qemu_patched/include/libAFL/aflpp.h
--- qemu/include/libAFL/aflpp.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/aflpp.h	2023-10-31 01:24:23.617884543 +0100
@@ -0,0 +1,181 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#ifndef AFLPP_H
+#define AFLPP_H
+
+#include "libAFL/types.h"
+
+#include "libAFL/common.h"
+#include "libAFL/config.h"
+#include "libAFL/observer.h"
+#include "libAFL/input.h"
+#include "libAFL/mutator.h"
+#include "libAFL/queue.h"
+#include "libAFL/engine.h"
+#include "libAFL/fuzzone.h"
+#include "libAFL/feedback.h"
+#include "libAFL/stage.h"
+#include "libAFL/os.h"
+#include "libAFL/afl-returns.h"
+
+/*
+This is the generic forkserver interface that we have, in order to use the
+library to build something, agin "inherit" from this struct (yes, we'll be
+trying OO design principles here :D) and then extend adding your own fields to
+it. See the example forksever executor that we have in examples/
+*/
+
+struct afl_executor_funcs {
+
+  afl_ret_t (*init_cb)(afl_executor_t *);  // can be NULL
+  u8 (*destroy_cb)(afl_executor_t *);      // can be NULL
+
+  afl_exit_t (*run_target_cb)(afl_executor_t *);          // Similar to afl_fsrv_run_target we have in afl
+  //u8 (*place_input_cb)(afl_executor_t *, afl_input_t *);  // similar to the write_to_testcase function in afl.
+  void (*place_input_cb)(afl_executor_t *, afl_input_t *); 
+  afl_ret_t (*observer_add)(afl_executor_t *, afl_observer_t *);  // Add an observtion channel to the list
+
+  afl_input_t *(*input_get)(afl_executor_t *);  // Getter function for the current input
+
+  void (*observers_reset)(afl_executor_t *);  // Reset the observation channels
+
+};
+
+// This is like the generic vtable for the executor.
+
+struct afl_executor {
+
+  afl_observer_t **observors;  // This will be swapped for the observation channel once its ready
+  char * info; 		/* Additional information*/
+  u32 observors_count;
+
+  afl_input_t *current_input;  // Holds current input for the executor
+
+  struct afl_executor_funcs funcs;  // afl executor_ops;
+
+};
+
+afl_ret_t    afl_executor_init(afl_executor_t *);
+void         afl_executor_deinit(afl_executor_t *);
+afl_ret_t    afl_executor_add_observer(afl_executor_t *, afl_observer_t *);
+afl_input_t *afl_executor_get_current_input(afl_executor_t *);
+void         afl_observers_reset(afl_executor_t *);
+
+// Function used to create an executor, we alloc the memory ourselves and
+// initialize the executor
+
+AFL_NEW_AND_DELETE_FOR(afl_executor)
+
+/* Forkserver executor */
+typedef struct afl_forkserver {
+
+  afl_executor_t base;                                                           /* executer struct to inherit from */
+
+  u8 *trace_bits;                                                               /* SHM with instrumentation bitmap  */
+  u8  use_stdin;                                                                /* use stdin for sending data       */
+
+  s32 fsrv_pid,                                                                 /* PID of the fork server           */
+      child_pid,                                                                /* PID of the fuzzed program        */
+      child_status,                                                             /* waitpid result for the child     */
+      out_dir_fd,                                                               /* FD of the lock file              */
+      dev_null_fd;
+
+  s32 out_fd,                                                                   /* Persistent fd for fsrv->out_file */
+
+      fsrv_ctl_fd,                                                              /* Fork server control pipe (write) */
+      fsrv_st_fd;                                                               /* Fork server status pipe (read)   */
+
+  u32 exec_tmout;                                                               /* Configurable exec timeout (ms)   */
+  u32 map_size;                                                                 /* map size used by the target      */
+
+  u64 total_execs;                                                              /* How often run_target was called  */
+
+  char *out_file,                                                               /* File to fuzz, if any             */
+      *target_path;                                                             /* Path of the target               */
+
+  char **target_args;
+
+  u32 last_run_timed_out;                                                       /* Traced process timed out?        */
+  u32 last_run_time;                                                            /* Time this exec took to execute   */
+
+  u8 last_kill_signal;                                                          /* Signal that killed the child     */
+
+} afl_forkserver_t;
+
+/* Functions related to the forkserver defined above */
+afl_forkserver_t *fsrv_init(char *target_path, char **extra_target_args);
+afl_exit_t        fsrv_run_target(afl_executor_t *fsrv_executor);
+u8                fsrv_place_input(afl_executor_t *fsrv_executor, afl_input_t *input);
+afl_ret_t         fsrv_start(afl_executor_t *fsrv_executor);
+
+/* In-memory executor */
+
+/* Function ptr for the harness */
+typedef afl_exit_t (*harness_function_type)(afl_executor_t *executor, u8 *, size_t);
+
+typedef struct in_memory_executor {
+
+  afl_executor_t        base;
+  harness_function_type harness;
+  char **               argv;  // These are to support the libfuzzer harnesses
+  int                   argc;  // To support libfuzzer harnesses
+  afl_stage_t *         stage;
+  afl_queue_global_t *  global_queue;
+
+} in_memory_executor_t;
+
+afl_exit_t in_memory_run_target(afl_executor_t *executor);
+u8         in_mem_executor_place_input(afl_executor_t *executor, afl_input_t *input);
+void       in_memory_executor_init(in_memory_executor_t *in_memory_executor, harness_function_type harness);
+
+
+
+/* Mils executor */
+typedef struct mils_executor 
+{
+  afl_executor_t        base;
+  harness_function_type harness;
+  afl_stage_t *         stage;
+  afl_queue_global_t *  global_queue;
+} mils_executor_t;
+
+
+/* Functions related to the mils_executor defined above */
+
+void mils_executor_init(mils_executor_t *mils_executor, harness_function_type harness);
+
+void mils_executor_deinit(mils_executor_t *mils_executor);
+
+afl_exit_t mils_run_target(afl_executor_t *executor);
+
+void mils_executor_place_input(afl_executor_t *executor, afl_input_t *input);
+
+
+
+
+#endif
+
diff -ruN qemu/include/libAFL/afl-returns.h qemu_patched/include/libAFL/afl-returns.h
--- qemu/include/libAFL/afl-returns.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/afl-returns.h	2023-10-31 00:26:50.112623643 +0100
@@ -0,0 +1,95 @@
+/* This file includes return codes for libafl. */
+
+#ifndef AFL_RETURNS_H
+#define AFL_RETURNS_H
+
+#include <errno.h>
+#include <string.h>
+
+#include "libAFL/debug.h"
+
+/* Shorthand to check for RET_SUCCESS */
+#define AFL_OK(expr) ((expr) == AFL_RET_SUCCESS)
+
+/* If expr != AFL_RET_SUCCESS, run block, error is in err. Return from here will return the parent func */
+#define AFL_TRY(expr, block)                                      \
+  do {                                                            \
+                                                                  \
+    afl_ret_t err = (expr);                                       \
+    if (err != AFL_RET_SUCCESS) {                                 \
+                                                                  \
+      DBG("AFL_TRY returning error: %s", afl_ret_stringify(err)); \
+      block                                                       \
+                                                                  \
+    }                                                             \
+                                                                  \
+  } while (0);
+
+/* Shorthand to check for RET_SUCCESS and assign to ret */
+#define AFL_OK_RET(expr, ret) ((ret = (expr)) == AFL_RET_SUCCESS)
+
+typedef enum afl_ret {
+
+  AFL_RET_SUCCESS = 0,
+  AFL_RET_UNKNOWN_ERROR,
+  AFL_RET_FILE_DUPLICATE,
+  AFL_RET_ALLOC,
+  AFL_RET_FILE_OPEN_ERROR,
+  AFL_RET_FILE_SIZE,
+  AFL_RET_SHORT_READ,
+  AFL_RET_SHORT_WRITE,
+  AFL_RET_ARRAY_END,
+  AFL_RET_EXEC_ERROR,
+  AFL_RET_BROKEN_TARGET,
+  AFL_RET_NULL_PTR,
+  AFL_RET_ERRNO,
+  AFL_RET_NULL_QUEUE_ENTRY,
+  AFL_RET_WRITE_TO_CRASH,
+  AFL_RET_QUEUE_ENDS,
+  AFL_RET_ERROR_INITIALIZE,
+  AFL_RET_NO_FUZZ_WORKERS,
+  AFL_RET_TRIM_FAIL,
+  AFL_RET_ERROR_INPUT_COPY,
+  AFL_RET_EMPTY,
+
+} afl_ret_t;
+
+/* Returns a string representation of afl_ret_t or of the errno if applicable */
+static inline char *afl_ret_stringify(afl_ret_t afl_ret) {
+
+  switch (afl_ret) {
+
+    case AFL_RET_SUCCESS:
+      return "Success";
+    case AFL_RET_ARRAY_END:
+      return "No more elements in array";
+    case AFL_RET_EXEC_ERROR:
+      return "Could not execute target";
+    case AFL_RET_BROKEN_TARGET:
+      return "Target did not behave as expected";
+    case AFL_RET_ERROR_INPUT_COPY:
+      return "Error creating input copy";
+    case AFL_RET_EMPTY:
+      return "Empty data";
+    case AFL_RET_FILE_DUPLICATE:
+      return "File exists";
+    case AFL_RET_ALLOC:
+      if (!errno) { return "Allocation failed"; }
+      /* fall-through */
+    case AFL_RET_FILE_OPEN_ERROR:
+      if (!errno) { return "Error opening file"; }
+      /* fall-through */
+    case AFL_RET_SHORT_READ:
+      if (!errno) { return "Got less bytes than expected"; }
+      /* fall-through */
+    case AFL_RET_ERRNO:
+      return strerror(errno);
+    default:
+      return "Unknown error. Please report this bug!";
+
+  }
+
+}
+
+#endif
+
diff -ruN qemu/include/libAFL/alloc-inl.h qemu_patched/include/libAFL/alloc-inl.h
--- qemu/include/libAFL/alloc-inl.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/alloc-inl.h	2023-11-12 02:39:26.234924986 +0100
@@ -0,0 +1,724 @@
+/*
+   american fuzzy lop++ - error-checking, memory-zeroing alloc routines
+   --------------------------------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This allocator is not designed to resist malicious attackers (the canaries
+   are small and predictable), but provides a robust and portable way to detect
+   use-after-free, off-by-one writes, stale pointers, and so on.
+
+ */
+
+/* this file contains anything allocator-realted libafl */
+
+#ifndef _HAVE_ALLOC_INL_H
+#define _HAVE_ALLOC_INL_H
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <stddef.h>
+#include <string.h>
+#include <stddef.h>
+
+#include "config.h"
+#include "types.h"
+#include "debug.h"
+
+/* Initial size used for afl_realloc */
+#define INITIAL_GROWTH_SIZE (64)
+
+// Be careful! _WANT_ORIGINAL_AFL_ALLOC is not compatible with custom mutators
+
+#ifndef _WANT_ORIGINAL_AFL_ALLOC
+  // afl++ stuff without memory corruption checks - for speed
+
+  /* User-facing macro to sprintf() to a dynamically allocated buffer. */
+
+  #define alloc_printf(_str...)                        \
+    ({                                                 \
+                                                       \
+      u8 *_tmp;                                        \
+      s32 _len = snprintf(NULL, 0, _str);              \
+      if (_len < 0) FATAL("Whoa, snprintf() fails?!"); \
+      _tmp = ck_alloc(_len + 1);                       \
+      snprintf((char *)_tmp, _len + 1, _str);          \
+      _tmp;                                            \
+                                                       \
+    })
+
+  /* Macro to enforce allocation limits as a last-resort defense against
+     integer overflows. */
+
+  #define ALLOC_CHECK_SIZE(_s)                                          \
+    do {                                                                \
+                                                                        \
+      if ((_s) > MAX_ALLOC) ABORT("Bad alloc request: %u bytes", (_s)); \
+                                                                        \
+    } while (0)
+
+  /* Macro to check malloc() failures and the like. */
+
+  #define ALLOC_CHECK_RESULT(_r, _s)                                    \
+    do {                                                                \
+                                                                        \
+      if (!(_r)) ABORT("Out of memory: can't allocate %u bytes", (_s)); \
+                                                                        \
+    } while (0)
+
+/* Allocate a buffer, explicitly not zeroing it. Returns NULL for zero-sized
+   requests. */
+
+static inline void *DFL_ck_alloc_nozero(u32 size) {
+
+  void *ret;
+
+  if (!size) { return NULL; }
+
+  ALLOC_CHECK_SIZE(size);
+  ret = malloc(size);
+  ALLOC_CHECK_RESULT(ret, size);
+
+  return (void *)ret;
+
+}
+
+/* Allocate a buffer, returning zeroed memory. */
+
+static inline void *DFL_ck_alloc(u32 size) {
+
+  void *mem;
+
+  if (!size) { return NULL; }
+  mem = DFL_ck_alloc_nozero(size);
+
+  return memset(mem, 0, size);
+
+}
+
+/* Free memory, checking for double free and corrupted heap. When DEBUG_BUILD
+   is set, the old memory will be also clobbered with 0xFF. */
+
+static inline void DFL_ck_free(void *mem) {
+
+  if (!mem) { return; }
+
+  free(mem);
+
+}
+
+/* Re-allocate a buffer, checking for issues and zeroing any newly-added tail.
+   With DEBUG_BUILD, the buffer is always reallocated to a new addresses and the
+   old memory is clobbered with 0xFF. */
+
+static inline void *DFL_ck_realloc(void *orig, u32 size) {
+
+  void *ret;
+
+  if (!size) {
+
+    DFL_ck_free(orig);
+    return NULL;
+
+  }
+
+  ALLOC_CHECK_SIZE(size);
+
+  /* Catch pointer issues sooner: force relocation and make sure that the
+     original buffer is wiped. */
+
+  ret = realloc(orig, size);
+
+  ALLOC_CHECK_RESULT(ret, size);
+
+  return (void *)ret;
+
+}
+
+/* Create a buffer with a copy of a string. Returns NULL for NULL inputs. */
+
+static inline u8 *DFL_ck_strdup(u8 *str) {
+
+  u8 *ret;
+  u32 size;
+
+  if (!str) { return NULL; }
+
+  size = strlen((char *)str) + 1;
+
+  ALLOC_CHECK_SIZE(size);
+  ret = (u8 *)malloc(size);
+  ALLOC_CHECK_RESULT(ret, size);
+
+  return (u8 *)memcpy(ret, str, size);
+
+}
+
+  /* In non-debug mode, we just do straightforward aliasing of the above
+     functions to user-visible names such as ck_alloc(). */
+
+  #define ck_alloc DFL_ck_alloc
+  #define ck_alloc_nozero DFL_ck_alloc_nozero
+  #define ck_realloc DFL_ck_realloc
+  #define ck_strdup DFL_ck_strdup
+  #define ck_free DFL_ck_free
+
+  #define alloc_report()
+
+#else
+  // This is the original alloc-inl of stock afl
+
+  /* User-facing macro to sprintf() to a dynamically allocated buffer. */
+
+  #define alloc_printf(_str...)                        \
+    ({                                                 \
+                                                       \
+      u8 *_tmp;                                        \
+      s32 _len = snprintf(NULL, 0, _str);              \
+      if (_len < 0) FATAL("Whoa, snprintf() fails?!"); \
+      _tmp = ck_alloc(_len + 1);                       \
+      snprintf((char *)_tmp, _len + 1, _str);          \
+      _tmp;                                            \
+                                                       \
+    })
+
+  /* Macro to enforce allocation limits as a last-resort defense against
+     integer overflows. */
+  #define ALLOC_CHECK_SIZE(_s)                                          \
+    do {                                                                \
+                                                                        \
+      if ((_s) > MAX_ALLOC) ABORT("Bad alloc request: %u bytes", (_s)); \
+                                                                        \
+    } while (0)
+
+  /* Macro to check malloc() failures and the like. */
+
+  #define ALLOC_CHECK_RESULT(_r, _s)                                    \
+    do {                                                                \
+                                                                        \
+      if (!(_r)) ABORT("Out of memory: can't allocate %u bytes", (_s)); \
+                                                                        \
+    } while (0)
+
+  /* Magic tokens used to mark used / freed chunks. */
+
+  #define ALLOC_MAGIC_C1 0xFF00FF00                                                           /* Used head (dword)  */
+  #define ALLOC_MAGIC_F 0xFE00FE00                                                            /* Freed head (dword) */
+  #define ALLOC_MAGIC_C2 0xF0                                                                 /* Used tail (byte)   */
+
+  /* Positions of guard tokens in relation to the user-visible pointer. */
+
+  #define ALLOC_C1(_ptr) (((u32 *)(_ptr))[-2])
+  #define ALLOC_S(_ptr) (((u32 *)(_ptr))[-1])
+  #define ALLOC_C2(_ptr) (((u8 *)(_ptr))[ALLOC_S(_ptr)])
+
+  #define ALLOC_OFF_HEAD 8
+  #define ALLOC_OFF_TOTAL (ALLOC_OFF_HEAD + 1)
+
+  /* Sanity-checking macros for pointers. */
+
+  #define CHECK_PTR(_p)                                                           \
+    do {                                                                          \
+                                                                                  \
+      if (_p) {                                                                   \
+                                                                                  \
+        if (ALLOC_C1(_p) ^ ALLOC_MAGIC_C1) {                                      \
+                                                                                  \
+          if (ALLOC_C1(_p) == ALLOC_MAGIC_F)                                      \
+            ABORT("Use after free.");                                             \
+          else                                                                    \
+            ABORT("Corrupted head alloc canary.");                                \
+                                                                                  \
+        }                                                                         \
+        if (ALLOC_C2(_p) ^ ALLOC_MAGIC_C2) ABORT("Corrupted tail alloc canary."); \
+                                                                                  \
+      }                                                                           \
+                                                                                  \
+    } while (0)
+
+  #define CHECK_PTR_EXPR(_p)  \
+    ({                        \
+                              \
+      typeof(_p) _tmp = (_p); \
+      CHECK_PTR(_tmp);        \
+      _tmp;                   \
+                              \
+    })
+
+/* Allocate a buffer, explicitly not zeroing it. Returns NULL for zero-sized
+   requests. */
+
+static inline void *DFL_ck_alloc_nozero(u32 size) {
+
+  void *ret;
+
+  if (!size) return NULL;
+
+  ALLOC_CHECK_SIZE(size);
+  ret = malloc(size + ALLOC_OFF_TOTAL);
+  ALLOC_CHECK_RESULT(ret, size);
+
+  ret += ALLOC_OFF_HEAD;
+
+  ALLOC_C1(ret) = ALLOC_MAGIC_C1;
+  ALLOC_S(ret) = size;
+  ALLOC_C2(ret) = ALLOC_MAGIC_C2;
+
+  return ret;
+
+}
+
+/* Allocate a buffer, returning zeroed memory. */
+
+static inline void *DFL_ck_alloc(u32 size) {
+
+  void *mem;
+
+  if (!size) return NULL;
+  mem = DFL_ck_alloc_nozero(size);
+
+  return memset(mem, 0, size);
+
+}
+
+/* Free memory, checking for double free and corrupted heap. When DEBUG_BUILD
+   is set, the old memory will be also clobbered with 0xFF. */
+
+static inline void DFL_ck_free(void *mem) {
+
+  if (!mem) return;
+
+  CHECK_PTR(mem);
+  #ifdef DEBUG_BUILD
+
+  /* Catch pointer issues sooner. */
+  memset(mem, 0xFF, ALLOC_S(mem));
+
+  #endif                                                                                             /* DEBUG_BUILD */
+
+  ALLOC_C1(mem) = ALLOC_MAGIC_F;
+
+  free(mem - ALLOC_OFF_HEAD);
+
+}
+
+/* Re-allocate a buffer, checking for issues and zeroing any newly-added tail.
+   With DEBUG_BUILD, the buffer is always reallocated to a new addresses and the
+   old memory is clobbered with 0xFF. */
+
+static inline void *DFL_ck_realloc(void *orig, u32 size) {
+
+  void *ret;
+  u32 old_size = 0;
+
+  if (!size) {
+
+    DFL_ck_free(orig);
+    return NULL;
+
+  }
+
+  if (orig) {
+
+    CHECK_PTR(orig);
+
+  #ifndef DEBUG_BUILD
+    ALLOC_C1(orig) = ALLOC_MAGIC_F;
+  #endif                                                                                            /* !DEBUG_BUILD */
+
+    old_size = ALLOC_S(orig);
+    orig -= ALLOC_OFF_HEAD;
+
+    ALLOC_CHECK_SIZE(old_size);
+
+  }
+
+  ALLOC_CHECK_SIZE(size);
+
+  #ifndef DEBUG_BUILD
+
+  ret = realloc(orig, size + ALLOC_OFF_TOTAL);
+  ALLOC_CHECK_RESULT(ret, size);
+
+  #else
+
+  /* Catch pointer issues sooner: force relocation and make sure that the
+     original buffer is wiped. */
+
+  ret = malloc(size + ALLOC_OFF_TOTAL);
+  ALLOC_CHECK_RESULT(ret, size);
+
+  if (orig) {
+
+    memcpy(ret + ALLOC_OFF_HEAD, orig + ALLOC_OFF_HEAD, MIN(size, old_size));
+    memset(orig + ALLOC_OFF_HEAD, 0xFF, old_size);
+
+    ALLOC_C1(orig + ALLOC_OFF_HEAD) = ALLOC_MAGIC_F;
+
+    free(orig);
+
+  }
+
+  #endif                                                                                           /* ^!DEBUG_BUILD */
+
+  ret += ALLOC_OFF_HEAD;
+
+  ALLOC_C1(ret) = ALLOC_MAGIC_C1;
+  ALLOC_S(ret) = size;
+  ALLOC_C2(ret) = ALLOC_MAGIC_C2;
+
+  if (size > old_size) memset(ret + old_size, 0, size - old_size);
+
+  return ret;
+
+}
+
+/* Create a buffer with a copy of a string. Returns NULL for NULL inputs. */
+
+static inline u8 *DFL_ck_strdup(u8 *str) {
+
+  void *ret;
+  u32 size;
+
+  if (!str) return NULL;
+
+  size = strlen((char *)str) + 1;
+
+  ALLOC_CHECK_SIZE(size);
+  ret = malloc(size + ALLOC_OFF_TOTAL);
+  ALLOC_CHECK_RESULT(ret, size);
+
+  ret += ALLOC_OFF_HEAD;
+
+  ALLOC_C1(ret) = ALLOC_MAGIC_C1;
+  ALLOC_S(ret) = size;
+  ALLOC_C2(ret) = ALLOC_MAGIC_C2;
+
+  return memcpy(ret, str, size);
+
+}
+
+  #ifndef DEBUG_BUILD
+
+    /* In non-debug mode, we just do straightforward aliasing of the above
+       functions to user-visible names such as ck_alloc(). */
+
+    #define ck_alloc DFL_ck_alloc
+    #define ck_alloc_nozero DFL_ck_alloc_nozero
+    #define ck_realloc DFL_ck_realloc
+    #define ck_strdup DFL_ck_strdup
+    #define ck_free DFL_ck_free
+
+    #define alloc_report()
+
+  #else
+
+    /* In debugging mode, we also track allocations to detect memory leaks, and
+       the flow goes through one more layer of indirection. */
+
+    /* Alloc tracking data structures: */
+
+    #define ALLOC_BUCKETS 4096
+
+struct TRK_obj {
+
+  void *ptr;
+  char *file, *func;
+  u32 line;
+
+};
+
+    #ifdef AFL_MAIN
+
+struct TRK_obj *TRK[ALLOC_BUCKETS];
+u32 TRK_cnt[ALLOC_BUCKETS];
+
+      #define alloc_report() TRK_report()
+
+    #else
+
+extern struct TRK_obj *TRK[ALLOC_BUCKETS];
+extern u32             TRK_cnt[ALLOC_BUCKETS];
+
+      #define alloc_report()
+
+    #endif                                                                                             /* ^AFL_MAIN */
+
+    /* Bucket-assigning function for a given pointer: */
+
+    #define TRKH(_ptr) (((((u32)(_ptr)) >> 16) ^ ((u32)(_ptr))) % ALLOC_BUCKETS)
+
+/* Add a new entry to the list of allocated objects. */
+
+static inline void TRK_alloc_buf(void *ptr, const char *file, const char *func, u32 line) {
+
+  u32 i, bucket;
+
+  if (!ptr) return;
+
+  bucket = TRKH(ptr);
+
+  /* Find a free slot in the list of entries for that bucket. */
+
+  for (i = 0; i < TRK_cnt[bucket]; i++)
+
+    if (!TRK[bucket][i].ptr) {
+
+      TRK[bucket][i].ptr = ptr;
+      TRK[bucket][i].file = (char *)file;
+      TRK[bucket][i].func = (char *)func;
+      TRK[bucket][i].line = line;
+      return;
+
+    }
+
+  /* No space available - allocate more. */
+
+  TRK[bucket] = DFL_ck_realloc(TRK[bucket], (TRK_cnt[bucket] + 1) * sizeof(struct TRK_obj));
+
+  TRK[bucket][i].ptr = ptr;
+  TRK[bucket][i].file = (char *)file;
+  TRK[bucket][i].func = (char *)func;
+  TRK[bucket][i].line = line;
+
+  TRK_cnt[bucket]++;
+
+}
+
+/* Remove entry from the list of allocated objects. */
+
+static inline void TRK_free_buf(void *ptr, const char *file, const char *func, u32 line) {
+
+  u32 i, bucket;
+
+  if (!ptr) return;
+
+  bucket = TRKH(ptr);
+
+  /* Find the element on the list... */
+
+  for (i = 0; i < TRK_cnt[bucket]; i++)
+
+    if (TRK[bucket][i].ptr == ptr) {
+
+      TRK[bucket][i].ptr = 0;
+      return;
+
+    }
+
+  WARNF("ALLOC: Attempt to free non-allocated memory in %s (%s:%u)", func, file, line);
+
+}
+
+/* Do a final report on all non-deallocated objects. */
+
+static inline void TRK_report(void) {
+
+  u32 i, bucket;
+
+  fflush(0);
+
+  for (bucket = 0; bucket < ALLOC_BUCKETS; bucket++)
+    for (i = 0; i < TRK_cnt[bucket]; i++)
+      if (TRK[bucket][i].ptr)
+        WARNF("ALLOC: Memory never freed, created in %s (%s:%u)", TRK[bucket][i].func, TRK[bucket][i].file,
+              TRK[bucket][i].line);
+
+}
+
+/* Simple wrappers for non-debugging functions: */
+
+static inline void *TRK_ck_alloc(u32 size, const char *file, const char *func, u32 line) {
+
+  void *ret = DFL_ck_alloc(size);
+  TRK_alloc_buf(ret, file, func, line);
+  return ret;
+
+}
+
+static inline void *TRK_ck_realloc(void *orig, u32 size, const char *file, const char *func, u32 line) {
+
+  void *ret = DFL_ck_realloc(orig, size);
+  TRK_free_buf(orig, file, func, line);
+  TRK_alloc_buf(ret, file, func, line);
+  return ret;
+
+}
+
+static inline void *TRK_ck_strdup(u8 *str, const char *file, const char *func, u32 line) {
+
+  void *ret = DFL_ck_strdup(str);
+  TRK_alloc_buf(ret, file, func, line);
+  return ret;
+
+}
+
+static inline void TRK_ck_free(void *ptr, const char *file, const char *func, u32 line) {
+
+  TRK_free_buf(ptr, file, func, line);
+  DFL_ck_free(ptr);
+
+}
+
+    /* Aliasing user-facing names to tracking functions: */
+
+    #define ck_alloc(_p1) TRK_ck_alloc(_p1, __FILE__, __FUNCTION__, __LINE__)
+
+    #define ck_alloc_nozero(_p1) TRK_ck_alloc(_p1, __FILE__, __FUNCTION__, __LINE__)
+
+    #define ck_realloc(_p1, _p2) TRK_ck_realloc(_p1, _p2, __FILE__, __FUNCTION__, __LINE__)
+
+    #define ck_strdup(_p1) TRK_ck_strdup(_p1, __FILE__, __FUNCTION__, __LINE__)
+
+    #define ck_free(_p1) TRK_ck_free(_p1, __FILE__, __FUNCTION__, __LINE__)
+
+  #endif                                                                                           /* ^!DEBUG_BUILD */
+
+#endif                                                                                  /* _WANT_ORIGINAL_AFL_ALLOC */
+
+/* This function calculates the next power of 2 greater or equal its argument.
+ @return The rounded up power of 2 (if no overflow) or 0 on overflow.
+*/
+static inline size_t next_pow2(size_t in) {
+
+  // Commented this out as this behavior doesn't change, according to unittests
+  // if (in == 0 || in > (size_t)-1) {
+
+  //
+  //   return 0;                  /* avoid undefined behaviour under-/overflow
+  //   */
+  //
+  // }
+
+  size_t out = in - 1;
+  out |= out >> 1;
+  out |= out >> 2;
+  out |= out >> 4;
+  out |= out >> 8;
+  out |= out >> 16;
+  return out + 1;
+
+}
+
+#define AFL_REALLOC_MAGIC (0xAF1A110C)
+
+/* AFL alloc buffer, the struct is here so we don't need to do fancy ptr
+ * arithmetics */
+struct afl_alloc_buf {
+
+  /* The complete allocated size, including the header of len
+   * AFL_ALLOC_SIZE_OFFSET */
+  size_t complete_size;
+  /* Make sure this is an alloc_buf */
+  size_t magic;
+  /* ptr to the first element of the actual buffer */
+  u8 __attribute__((aligned(8))) buf[0];
+
+};
+
+#define AFL_ALLOC_SIZE_OFFSET (offsetof(struct afl_alloc_buf, buf))
+
+/* Returs the container element to this ptr */
+static inline struct afl_alloc_buf *afl_alloc_bufptr(void *buf) {
+
+  return (struct afl_alloc_buf *)((u8 *)buf - AFL_ALLOC_SIZE_OFFSET);
+
+}
+
+/* Gets the maximum size of the buf contents (ptr->complete_size -
+ * AFL_ALLOC_SIZE_OFFSET) */
+static inline size_t afl_alloc_bufsize(void *buf) {
+
+  return afl_alloc_bufptr(buf)->complete_size - AFL_ALLOC_SIZE_OFFSET;
+
+}
+
+/* This function makes sure *size is > size_needed after call.
+ It will realloc *buf otherwise.
+ *size will grow exponentially as per:
+ https://blog.mozilla.org/nnethercote/2014/11/04/please-grow-your-buffers-exponentially/
+ Will return NULL and free *buf if size_needed is <1 or realloc failed.
+ @return For convenience, this function returns *buf.
+ */
+static inline void *afl_realloc(void *buf, size_t size_needed) {
+
+  struct afl_alloc_buf *new_buf = NULL;
+  //printf("size needed is %d\n",size_needed);
+
+  size_t current_size = 0;
+  size_t next_size = 0;
+
+  if (likely(buf)) {
+
+    /* the size is always stored at buf - 1*size_t */
+    new_buf = afl_alloc_bufptr(buf);
+    if (unlikely(new_buf->magic != AFL_REALLOC_MAGIC)) {
+
+      FATAL(
+          "Illegal, non-null pointer passed to afl_realloc (buf 0x%p, magic "
+          "0x%x)",
+          new_buf, (unsigned)new_buf->magic);
+
+    }
+
+    current_size = new_buf->complete_size;
+
+  }
+
+  size_needed += AFL_ALLOC_SIZE_OFFSET;
+
+  /* No need to realloc */
+  if (likely(current_size >= size_needed)) { return buf; }
+
+  /* No initial size was set */
+  if (size_needed < INITIAL_GROWTH_SIZE) {
+
+    next_size = INITIAL_GROWTH_SIZE;
+
+  } else {
+
+    /* grow exponentially */
+    next_size = next_pow2(size_needed);
+
+    /* handle overflow: fall back to the original size_needed */
+    if (unlikely(!next_size)) { next_size = size_needed; }
+
+  }
+
+  /* alloc */
+  //printf("Old size is %d\n",current_size);
+  //printf("next size is %d\n",next_size);
+  //printf("new buf addr is %x\n",new_buf);
+  new_buf = realloc(new_buf, next_size);
+  //printf("after realloc\n");
+  if (unlikely(!new_buf)) { return NULL; }
+
+  new_buf->complete_size = next_size;
+  new_buf->magic = AFL_REALLOC_MAGIC;
+  return new_buf->buf;
+
+}
+
+static inline void afl_free(void *buf) {
+
+  if (buf) { free(afl_alloc_bufptr(buf)); }
+
+}
+
+#undef INITIAL_GROWTH_SIZE
+
+#endif                                                                                       /* ! _HAVE_ALLOC_INL_H */
diff -ruN qemu/include/libAFL/android-ashmem.h qemu_patched/include/libAFL/android-ashmem.h
--- qemu/include/libAFL/android-ashmem.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/android-ashmem.h	2023-10-27 20:26:54.000000000 +0200
@@ -0,0 +1,114 @@
+/*
+   american fuzzy lop++ - android shared memory compatibility layer
+   ----------------------------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This header re-defines the shared memory routines used by AFL++
+   using the Andoid API.
+
+ */
+
+/* Thif file contains an android-specific shared mem wrapper */
+
+#ifndef _ANDROID_ASHMEM_H
+#define _ANDROID_ASHMEM_H
+
+#ifdef __ANDROID__
+
+  #include <fcntl.h>
+  #include <linux/shm.h>
+  #include <linux/ashmem.h>
+  #include <sys/ioctl.h>
+  #include <sys/mman.h>
+
+  #if __ANDROID_API__ >= 26
+    #define shmat bionic_shmat
+    #define shmctl bionic_shmctl
+    #define shmdt bionic_shmdt
+    #define shmget bionic_shmget
+  #endif
+
+  #include <sys/shm.h>
+  #undef shmat
+  #undef shmctl
+  #undef shmdt
+  #undef shmget
+  #include <stdio.h>
+
+  #define ASHMEM_DEVICE "/dev/ashmem"
+
+static inline int shmctl(int __shmid, int __cmd, struct shmid_ds *__buf) {
+
+  int ret = 0;
+  if (__cmd == IPC_RMID) {
+
+    int               length = ioctl(__shmid, ASHMEM_GET_SIZE, NULL);
+    struct ashmem_pin pin = {0, (unsigned int)length};
+    ret = ioctl(__shmid, ASHMEM_UNPIN, &pin);
+    close(__shmid);
+
+  }
+
+  return ret;
+
+}
+
+static inline int shmget(key_t __key, size_t __size, int __shmflg) {
+
+  (void)__shmflg;
+  int  fd, ret;
+  char ourkey[11];
+
+  fd = open(ASHMEM_DEVICE, O_RDWR);
+  if (fd < 0) return fd;
+
+  sprintf(ourkey, "%d", __key);
+  ret = ioctl(fd, ASHMEM_SET_NAME, ourkey);
+  if (ret < 0) goto error;
+
+  ret = ioctl(fd, ASHMEM_SET_SIZE, __size);
+  if (ret < 0) goto error;
+
+  return fd;
+
+error:
+  close(fd);
+  return ret;
+
+}
+
+static inline void *shmat(int __shmid, const void *__shmaddr, int __shmflg) {
+
+  (void)__shmflg;
+  int   size;
+  void *ptr;
+
+  size = ioctl(__shmid, ASHMEM_GET_SIZE, NULL);
+  if (size < 0) { return NULL; }
+
+  ptr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, __shmid, 0);
+  if (ptr == MAP_FAILED) { return NULL; }
+
+  return ptr;
+
+}
+
+#endif                                                                                               /* __ANDROID__ */
+
+#endif
+
diff -ruN qemu/include/libAFL/cmplog.h qemu_patched/include/libAFL/cmplog.h
--- qemu/include/libAFL/cmplog.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/cmplog.h	2023-10-27 20:26:54.000000000 +0200
@@ -0,0 +1,83 @@
+/*
+   american fuzzy lop++ - cmplog header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Forkserver design by Jann Horn <jannhorn@googlemail.com>
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   Shared code to handle the shared memory. This is used by the fuzzer
+   as well the other components like afl-tmin, afl-showmap, etc...
+
+ */
+
+#ifndef _AFL_CMPLOG_H
+#define _AFL_CMPLOG_H
+
+#include "config.h"
+//#include "forkserver.h"
+
+#define CMP_MAP_W 65536
+#define CMP_MAP_H 256
+#define CMP_MAP_RTN_H (CMP_MAP_H / 4)
+
+#define SHAPE_BYTES(x) (x + 1)
+
+#define CMP_TYPE_INS 0
+#define CMP_TYPE_RTN 1
+
+struct cmp_header {
+
+  unsigned hits : 20;
+
+  unsigned cnt : 20;
+  unsigned id : 16;
+
+  unsigned shape : 5;  // from 0 to 31
+  unsigned type : 1;
+
+} __attribute__((packed));
+
+struct cmp_operands {
+
+  u64 v0;
+  u64 v1;
+
+};
+
+struct cmpfn_operands {
+
+  u8 v0[32];
+  u8 v1[32];
+
+};
+
+typedef struct cmp_operands cmp_map_list[CMP_MAP_H];
+
+struct cmp_map {
+
+  struct cmp_header   headers[CMP_MAP_W];
+  struct cmp_operands log[CMP_MAP_W][CMP_MAP_H];
+
+};
+
+/* Execs the child */
+
+// void cmplog_exec_child(afl_forkserver_t *fsrv, char **argv);
+
+#endif
+
diff -ruN qemu/include/libAFL/common.h qemu_patched/include/libAFL/common.h
--- qemu/include/libAFL/common.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/common.h	2023-10-31 00:29:20.597107568 +0100
@@ -0,0 +1,173 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+/* This file contains commonly used functionality for libafl */
+
+#ifndef COMMON_H
+#define COMMON_H
+
+#include <pthread.h>
+#include <unistd.h>
+#include <sys/time.h>
+#include <stdbool.h>
+
+#include "libAFL/types.h"
+#include "libAFL/alloc-inl.h"
+#include "libAFL/afl-returns.h"
+
+#define AFL_DEINITIALIZED (0xAF1DA10C)
+
+// We're declaring a few structs here which have an interdependency between them
+
+typedef struct afl_fuzz_one afl_fuzz_one_t;
+
+typedef struct afl_engine afl_engine_t;
+
+typedef struct afl_stage afl_stage_t;
+
+typedef struct afl_executor afl_executor_t;
+
+typedef struct afl_mutator afl_mutator_t;
+
+// Returns new buf containing the substring token
+void *afl_insert_substring(u8 *src_buf, u8 *dest_buf, size_t len, void *token, size_t token_len, size_t offset);
+// Erases remove_len number of bytes from offset
+size_t afl_erase_bytes(u8 *buf, size_t len, size_t offset, size_t remove_len);
+
+// Inserts a certain length of a byte value (byte) at offset in buf
+u8 *afl_insert_bytes(u8 *src_buf, u8 *dest_buf, size_t len, u8 byte, size_t insert_len, size_t offset);
+
+static inline char **afl_argv_cpy_dup(int argc, char **argv) {
+
+  int i = 0;
+
+  char **ret = calloc(1, (argc + 1) * sizeof(char *));
+  if (!ret) { return NULL; }
+
+  for (i = 0; i < argc; i++) {
+
+    ret[i] = strdup(argv[i]);
+    if (!ret[i]) {
+
+      int k;
+      for (k = 0; k < i; k++) {
+
+        free(ret[k]);
+
+      }
+
+      free(ret);
+      return NULL;
+
+    }
+
+  }
+
+  ret[i] = NULL;
+
+  return ret;
+
+}
+
+/* Get unix time in microseconds */
+u64 afl_get_cur_time_us(void);
+
+/* Get unix time in microseconds */
+u64 afl_get_cur_time(void);
+
+/* Get unix time in seconds */
+u64 afl_get_cur_time_s(void);
+
+/* returns true, if the given dir exists, else false */
+bool afl_dir_exists(char *dirpath);
+
+/* This function uses select calls to wait on a child process for given
+ * timeout_ms milliseconds and kills it if it doesn't terminate by that time */
+static inline u32 afl_read_s32_timed(s32 fd, s32 *buf, u32 timeout_ms) {
+
+  fd_set readfds;
+  FD_ZERO(&readfds);
+  FD_SET(fd, &readfds);
+  struct timeval timeout;
+  int            sret;
+  ssize_t        len_read;
+
+  timeout.tv_sec = (timeout_ms / 1000);
+  timeout.tv_usec = (timeout_ms % 1000) * 1000;
+#if !defined(__linux__)
+  u64 read_start = afl_get_cur_time_us();
+#endif
+
+  /* set exceptfds as well to return when a child exited/closed the pipe. */
+restart_select:
+  sret = select(fd + 1, &readfds, NULL, NULL, &timeout);
+
+  if (likely(sret > 0)) {
+
+  restart_read:
+    len_read = read(fd, (u8 *)buf, 4);
+
+    if (likely(len_read == 4)) {  // for speed we put this first
+
+#if defined(__linux__)
+      u32 exec_ms = MIN(timeout_ms, ((u64)timeout_ms - (timeout.tv_sec * 1000 + timeout.tv_usec / 1000)));
+#else
+      u32 exec_ms = MIN(timeout_ms, afl_get_cur_time_us() - read_start);
+#endif
+
+      // ensure to report 1 ms has passed (0 is an error)
+      return exec_ms > 0 ? exec_ms : 1;
+
+    } else if (unlikely(len_read == -1 && errno == EINTR)) {
+
+      goto restart_read;
+
+    } else if (unlikely(len_read < 4)) {
+
+      return 0;
+
+    }
+
+  } else if (unlikely(!sret)) {
+
+    *buf = -1;
+    return timeout_ms + 1;
+
+  } else if (unlikely(sret < 0)) {
+
+    if (likely(errno == EINTR)) goto restart_select;
+
+    *buf = -1;
+    return 0;
+
+  }
+
+  return 0;  // not reached
+
+}
+
+#endif
+
diff -ruN qemu/include/libAFL/config.h qemu_patched/include/libAFL/config.h
--- qemu/include/libAFL/config.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/config.h	2024-01-22 15:39:41.032467646 +0100
@@ -0,0 +1,464 @@
+/*
+   american fuzzy lop++ - vaguely configurable bits
+   ------------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+/* This file, taken from the original afl, contains compile-time configurations. */
+
+#ifndef CONFIG_H
+#define CONFIG_H
+
+#include "libAFL/types.h"
+
+/* Version string: */
+
+// c = release, d = volatile github dev, e = experimental branch
+#define VERSION "libafl++0.1"
+
+/* user defined (me) */
+#define SEED_COUNT 1
+#define STATS_FILE "./stats.csv" // metrics save
+#define RESUME_CAMPAIGN 0 //todo
+#define SAVE_METRICS 1 //1: enabled, 0 disabled
+#define VERBOSE_LOG 0 // should be in config of entire qemu but whatever man its 2 am i just hope it works
+
+#define BASE64_MODE 0 //future imp. sendMessageSPIC and Receive implementation truncate on 00 even if message continues
+// per alcune librerie Ã¨ importante quindi codificare prima della trasmissione (oppure nel fuzzer) es. con base64 e decodificare con base64
+// b64 nasce proprio per risolvere problemi di questo tipo
+
+// coverage settings
+#define SHM_FILE_BLOCK_COV "/afl_shm_block_bitmap"
+#define MAP_SIZE_POW2 16 //bitmap size: 2^MAP_SIZE_POW2
+#define COV_FILE "./coverage/feedback.txt"
+#define COV_FILE_READ "./coverage/feedback_1.txt"
+#define BLOCKCOV_MODE 1 // =: edge coverage. 1: block coverage
+#define BITMAP_UPDATE_ONTRANSLATE 1 // 1: updates bitmap on new block translation. 0: on block execution
+   // snapshot may need ontranslate=1 to work efficiently
+#define FEEDBACK_MODE 1 // libafl feedback based on qemu bitmap
+
+//throughput monitoring
+#define THROUGHPUT_METRIC 0
+#define THROUGHPUT_LBOUND 50 // non a 0 cosÃ¬ si supera transitorio e si vede a regime
+#define THROUGHPUT_HBOUND 100 
+
+// pc thresholding (not used anymore)
+#define PC_LBOUND 0
+#define PC_HBOUND 10000000 
+
+// seed gathering
+#define SEED_GATHERING 0 //uses first input as seed for fuzzing campaign
+
+// pc profiling
+#define PC_PROFILING 1
+#define PC_GATHERING_MODE 0 // this should also not override fuzzing input in tcg or debug_harness function(fuzzing non deve funzionare se Ã¨ a 1)
+   // attenzione: per avere unique PCs bisogna usare bitmap_update_ontranslate 1 
+#define MAX_HEX_VALUES 7600 // Define the maximum number of hex values in the file
+#define HASH_SIZE 65536     // Define the hash table size
+#define PC_FILE_WRITE "../pc_filtering/firmware_pc3.txt"
+#define PC_FILE_READ "../pc_filtering/firmware_pc2.txt"
+
+//
+#define INIT_WAIT_TIME 10 // initial wait
+#define INIT_TIMES 2 // wait = INIT_WAIT_TIME * INIT_TIMES
+
+// snapshot settings
+#define SNAPSHOT_ENABLED 0 // set this parameter also in the TCG plugin
+#define SNAPSHOT_LABEL "snapxd" // qemu file label of snapshot
+
+
+/******************************************************
+ *                                                    *
+ *  Settings that may be of interest to power users:  *
+ *                                                    *
+ ******************************************************/
+
+/* Comment out to disable terminal colors (note that this makes afl-analyze
+   a lot less nice): */
+
+#define USE_COLOR
+
+/* Constants that we use throughtout the library */
+
+#define FILENAME_LEN_MAX 4120
+
+/* If you want to have the original afl internal memory corruption checks.
+   Disabled by default for speed. it is better to use "make ASAN_BUILD=1". */
+
+//#define _WANT_ORIGINAL_AFL_ALLOC
+
+/* Comment out to disable fancy ANSI boxes and use poor man's 7-bit UI: */
+
+#ifndef ANDROID_DISABLE_FANCY  // Fancy boxes are ugly from adb
+  #define FANCY_BOXES
+#endif
+
+/* Default timeout for fuzzed code (milliseconds). This is the upper bound,
+   also used for detecting hangs; the actual value is auto-scaled: */
+
+#define EXEC_TIMEOUT 1000
+
+/* Timeout rounding factor when auto-scaling (milliseconds): */
+
+#define EXEC_TM_ROUND 20
+
+/* 64bit arch MACRO */
+#if (defined(__x86_64__) || defined(__arm64__) || defined(__aarch64__))
+  #define WORD_SIZE_64 1
+#endif
+
+/* Default memory limit for child process (MB): */
+
+#ifndef __NetBSD__
+  #ifndef WORD_SIZE_64
+    #define MEM_LIMIT 25
+  #else
+    #define MEM_LIMIT 50
+  #endif                                                                                          /* ^!WORD_SIZE_64 */
+#else    /* NetBSD's kernel needs more space for stack, see discussion for issue \
+            #165 */
+  #define MEM_LIMIT 200
+#endif
+/* Default memory limit when running in QEMU mode (MB): */
+
+#define MEM_LIMIT_QEMU 200
+
+/* Default memory limit when running in Unicorn mode (MB): */
+
+#define MEM_LIMIT_UNICORN 200
+
+/* Number of calibration cycles per every new test case (and for test
+   cases that show variable behavior): */
+
+#define CAL_CYCLES 8
+#define CAL_CYCLES_LONG 40
+
+/* Number of subsequent timeouts before abandoning an input file: */
+
+#define TMOUT_LIMIT 250
+
+/* Maximum number of unique hangs or crashes to record: */
+
+#define KEEP_UNIQUE_HANG 500
+#define KEEP_UNIQUE_CRASH 5000
+
+/* Baseline number of random tweaks during a single 'havoc' stage: */
+
+#define HAVOC_CYCLES 256
+#define HAVOC_CYCLES_INIT 1024
+
+/* Maximum multiplier for the above (should be a power of two, beware
+   of 32-bit int overflows): */
+
+#define HAVOC_MAX_MULT 16
+#define HAVOC_MAX_MULT_MOPT 32
+
+/* Absolute minimum number of havoc cycles (after all adjustments): */
+
+#define HAVOC_MIN 16
+
+/* Power Schedule Divisor */
+#define POWER_BETA 1
+#define MAX_FACTOR (POWER_BETA * 32)
+
+/* Maximum stacking for havoc-stage tweaks. The actual value is calculated
+   like this:
+
+   n = random between 1 and HAVOC_STACK_POW2
+   stacking = 2^n
+
+   In other words, the default (n = 7) produces 2, 4, 8, 16, 32, 64, or
+   128 stacked tweaks: */
+
+#define HAVOC_STACK_POW2 7
+
+/* Caps on block sizes for cloning and deletion operations. Each of these
+   ranges has a 33% probability of getting picked, except for the first
+   two cycles where smaller blocks are favored: */
+
+#define HAVOC_BLK_SMALL 32
+#define HAVOC_BLK_MEDIUM 128
+#define HAVOC_BLK_LARGE 1500
+
+/* Extra-large blocks, selected very rarely (<5% of the time): */
+
+#define HAVOC_BLK_XL 32768
+
+/* Probabilities of skipping non-favored entries in the queue, expressed as
+   percentages: */
+
+#define SKIP_TO_NEW_PROB 99                                             /* ...when there are new, pending favorites */
+#define SKIP_NFAV_OLD_PROB 95                                           /* ...no new favs, cur entry already fuzzed */
+#define SKIP_NFAV_NEW_PROB 75                                           /* ...no new favs, cur entry not fuzzed yet */
+
+/* Splicing cycle count: */
+
+#define SPLICE_CYCLES 15
+
+/* Nominal per-splice havoc cycle length: */
+
+#define SPLICE_HAVOC 32
+
+/* Maximum offset for integer addition / subtraction stages: */
+
+#define ARITH_MAX 35
+
+/* Limits for the test case trimmer. The absolute minimum chunk size; and
+   the starting and ending divisors for chopping up the input file: */
+
+#define TRIM_MIN_BYTES 4
+#define TRIM_START_STEPS 16
+#define TRIM_END_STEPS 1024
+
+/* Maximum size of input file, in bytes (keep under 100MB): */
+
+#define MAX_FILE (1 * 1024 * 1024)
+
+/* The same, for the test case minimizer: */
+
+#define TMIN_MAX_FILE (10 * 1024 * 1024)
+
+/* Block normalization steps for afl-tmin: */
+
+#define TMIN_SET_MIN_SIZE 4
+#define TMIN_SET_STEPS 128
+
+/* Maximum dictionary token size (-x), in bytes: */
+
+#define MAX_DICT_FILE 128
+
+/* Length limits for auto-detected dictionary tokens: */
+
+#define MIN_AUTO_EXTRA 3
+#define MAX_AUTO_EXTRA 32
+
+/* Maximum number of user-specified dictionary tokens to use in deterministic
+   steps; past this point, the "extras/user" step will be still carried out,
+   but with proportionally lower odds: */
+
+#define MAX_DET_EXTRAS 200
+
+/* Maximum number of auto-extracted dictionary tokens to actually use in fuzzing
+   (first value), and to keep in memory as candidates. The latter should be much
+   higher than the former. */
+
+#define USE_AUTO_EXTRAS 128
+#define MAX_AUTO_EXTRAS (USE_AUTO_EXTRAS * 64)
+
+/* Scaling factor for the effector map used to skip some of the more
+   expensive deterministic steps. The actual divisor is set to
+   2^EFF_MAP_SCALE2 bytes: */
+
+#define EFF_MAP_SCALE2 3
+
+/* Minimum input file length at which the effector logic kicks in: */
+
+#define EFF_MIN_LEN 128
+
+/* Maximum effector density past which everything is just fuzzed
+   unconditionally (%): */
+
+#define EFF_MAX_PERC 90
+
+/* UI refresh frequency (Hz): */
+
+#define UI_TARGET_HZ 5
+
+/* Fuzzer stats file and plot update intervals (sec): */
+
+#define STATS_UPDATE_SEC 60
+#define PLOT_UPDATE_SEC 5
+
+/* Smoothing divisor for CPU load and exec speed stats (1 - no smoothing). */
+
+#define AVG_SMOOTHING 16
+
+/* Sync interval (every n havoc cycles): */
+
+#define SYNC_INTERVAL 8
+
+/* Output directory reuse grace period (minutes): */
+
+#define OUTPUT_GRACE 25
+
+/* Uncomment to use simple file names (id_NNNNNN): */
+
+// #define SIMPLE_FILES
+
+/* List of interesting values to use in fuzzing. */
+
+#define INTERESTING_8                                    \
+  -128,    /* Overflow signed 8-bit when decremented  */ \
+      -1,  /*                                         */ \
+      0,   /*                                         */ \
+      1,   /*                                         */ \
+      16,  /* One-off with common buffer size         */ \
+      32,  /* One-off with common buffer size         */ \
+      64,  /* One-off with common buffer size         */ \
+      100, /* One-off with common buffer size         */ \
+      127                                                                /* Overflow signed 8-bit when incremented  */
+
+#define INTERESTING_8_LEN 9
+
+#define INTERESTING_16                                    \
+  -32768,   /* Overflow signed 16-bit when decremented */ \
+      -129, /* Overflow signed 8-bit                   */ \
+      128,  /* Overflow signed 8-bit                   */ \
+      255,  /* Overflow unsig 8-bit when incremented   */ \
+      256,  /* Overflow unsig 8-bit                    */ \
+      512,  /* One-off with common buffer size         */ \
+      1000, /* One-off with common buffer size         */ \
+      1024, /* One-off with common buffer size         */ \
+      4096, /* One-off with common buffer size         */ \
+      32767                                                              /* Overflow signed 16-bit when incremented */
+
+#define INTERESTING_16_LEN 10
+
+#define INTERESTING_32                                          \
+  -2147483648LL,  /* Overflow signed 32-bit when decremented */ \
+      -100663046, /* Large negative number (endian-agnostic) */ \
+      -32769,     /* Overflow signed 16-bit                  */ \
+      32768,      /* Overflow signed 16-bit                  */ \
+      65535,      /* Overflow unsig 16-bit when incremented  */ \
+      65536,      /* Overflow unsig 16 bit                   */ \
+      100663045,  /* Large positive number (endian-agnostic) */ \
+      2147483647                                                         /* Overflow signed 32-bit when incremented */
+
+#define INTERESTING_32_LEN 8
+
+/***********************************************************
+ *                                                         *
+ *  Really exotic stuff you probably don't want to touch:  *
+ *                                                         *
+ ***********************************************************/
+
+/* Call count interval between reseeding the libc PRNG from /dev/urandom: */
+
+#define RESEED_RNG 100000
+
+/* Maximum line length passed from GCC to 'as' and used for parsing
+   configuration files: */
+
+#define MAX_LINE 8192
+
+/* Environment variable used to pass SHM ID to the called program. */
+
+#define SHM_ENV_VAR "__AFL_SHM_ID"
+
+/* Environment variable used to pass SHM FUZZ ID to the called program. */
+
+#define SHM_FUZZ_ENV_VAR "__AFL_SHM_FUZZ_ID"
+
+/* Other less interesting, internal-only variables. */
+
+#define CLANG_ENV_VAR "__AFL_CLANG_MODE"
+#define AS_LOOP_ENV_VAR "__AFL_AS_LOOPCHECK"
+#define PERSIST_ENV_VAR "__AFL_PERSISTENT"
+#define DEFER_ENV_VAR "__AFL_DEFER_FORKSRV"
+
+/* In-code signatures for deferred and persistent mode. */
+
+#define PERSIST_SIG "##SIG_AFL_PERSISTENT##"
+#define DEFER_SIG "##SIG_AFL_DEFER_FORKSRV##"
+
+/* Distinctive bitmap signature used to indicate failed execution: */
+
+#define EXEC_FAIL_SIG 0xfee1dead
+
+/* Distinctive exit code used to indicate MSAN trip condition: */
+
+#define MSAN_ERROR 86
+
+/* Designated file descriptors for forkserver commands (the application will
+   use FORKSRV_FD and FORKSRV_FD + 1): */
+
+#define FORKSRV_FD 198
+
+/* Fork server init timeout multiplier: we'll wait the user-selected
+   timeout plus this much for the fork server to spin up. */
+
+#define FORK_WAIT_MULT 10
+
+/* Calibration timeout adjustments, to be a bit more generous when resuming
+   fuzzing sessions or trying to calibrate already-added internal finds.
+   The first value is a percentage, the other is in milliseconds: */
+
+#define CAL_TMOUT_PERC 125
+#define CAL_TMOUT_ADD 50
+
+/* Number of chances to calibrate a case before giving up: */
+
+#define CAL_CHANCES 3
+
+/* Map size for the traced binary (2^MAP_SIZE_POW2). Must be greater than
+   2; you probably want to keep it under 18 or so for performance reasons
+   (adjusting AFL_INST_RATIO when compiling is probably a better way to solve
+   problems with complex programs). You need to recompile the target binary
+   after changing this - otherwise, SEGVs may ensue. */
+
+#define MAP_SIZE (1 << MAP_SIZE_POW2)
+
+/* Maximum allocator request size (keep well under INT_MAX): */
+
+#define MAX_ALLOC 0x40000000
+
+/* A made-up hashing seed: */
+
+#define HASH_CONST 0xa5b35705
+
+/* Constants for afl-gotcpu to control busy loop timing: */
+
+#define CTEST_TARGET_MS 5000
+#define CTEST_CORE_TRG_MS 1000
+#define CTEST_BUSY_CYCLES (10 * 1000 * 1000)
+
+/* Enable NeverZero counters in QEMU mode */
+
+/* afl bitmap file */
+#define SHM_FILE "/afl_shm_bitmap"
+
+#define AFL_QEMU_NOT_ZERO
+
+/* AFL RedQueen */
+
+#define CMPLOG_SHM_ENV_VAR "__AFL_CMPLOG_SHM_ID"
+
+/* CPU Affinity lockfile env var */
+
+#define CPU_AFFINITY_ENV_VAR "__AFL_LOCKFILE"
+
+/* Uncomment this to use inferior block-coverage-based instrumentation. Note
+   that you need to recompile the target binary for this to have any effect: */
+
+// #define COVERAGE_ONLY
+
+/* Uncomment this to ignore hit counts and output just one bit per tuple.
+   As with the previous setting, you will need to recompile the target
+   binary: */
+
+// #define SKIP_COUNTS
+
+/* Uncomment this to use instrumentation data to record newly discovered paths,
+   but do not use them as seeds for fuzzing. This is useful for conveniently
+   measuring coverage that could be attained by a "dumb" fuzzing algorithm: */
+
+// #define IGNORE_FINDS
+
+#endif                                                                                          /* ! _HAVE_CONFIG_H */
+
diff -ruN qemu/include/libAFL/debug.h qemu_patched/include/libAFL/debug.h
--- qemu/include/libAFL/debug.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/debug.h	2023-10-31 00:29:25.865122062 +0100
@@ -0,0 +1,293 @@
+/*
+   american fuzzy lop++ - debug / error handling macros
+   ----------------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+/* This file contains helpers for debugging and fancy printing */
+
+#ifndef DEBUG_H
+#define DEBUG_H
+
+#include <errno.h>
+
+#include "libAFL/types.h"
+#include "libAFL/config.h"
+
+/* wrap __LINE__ (int) as string using preprocessor magic, see
+http://decompile.com/cpp/faq/file_and_line_error_string.htm
+*/
+#define _STR(x) #x
+#define TOSTRING(x) _STR(x)
+
+/*******************
+ * Terminal colors *
+ *******************/
+#ifndef MESSAGES_TO_STDOUT
+  #define MESSAGES_TO_STDOUT
+#endif
+
+#ifdef USE_COLOR
+
+  #define cBLK "\x1b[0;30m"
+  #define cRED "\x1b[0;31m"
+  #define cGRN "\x1b[0;32m"
+  #define cBRN "\x1b[0;33m"
+  #define cBLU "\x1b[0;34m"
+  #define cMGN "\x1b[0;35m"
+  #define cCYA "\x1b[0;36m"
+  #define cLGR "\x1b[0;37m"
+  #define cGRA "\x1b[1;90m"
+  #define cLRD "\x1b[1;91m"
+  #define cLGN "\x1b[1;92m"
+  #define cYEL "\x1b[1;93m"
+  #define cLBL "\x1b[1;94m"
+  #define cPIN "\x1b[1;95m"
+  #define cLCY "\x1b[1;96m"
+  #define cBRI "\x1b[1;97m"
+  #define cRST "\x1b[0m"
+
+  #define bgBLK "\x1b[40m"
+  #define bgRED "\x1b[41m"
+  #define bgGRN "\x1b[42m"
+  #define bgBRN "\x1b[43m"
+  #define bgBLU "\x1b[44m"
+  #define bgMGN "\x1b[45m"
+  #define bgCYA "\x1b[46m"
+  #define bgLGR "\x1b[47m"
+  #define bgGRA "\x1b[100m"
+  #define bgLRD "\x1b[101m"
+  #define bgLGN "\x1b[102m"
+  #define bgYEL "\x1b[103m"
+  #define bgLBL "\x1b[104m"
+  #define bgPIN "\x1b[105m"
+  #define bgLCY "\x1b[106m"
+  #define bgBRI "\x1b[107m"
+
+#else
+
+  #define cBLK ""
+  #define cRED ""
+  #define cGRN ""
+  #define cBRN ""
+  #define cBLU ""
+  #define cMGN ""
+  #define cCYA ""
+  #define cLGR ""
+  #define cGRA ""
+  #define cLRD ""
+  #define cLGN ""
+  #define cYEL ""
+  #define cLBL ""
+  #define cPIN ""
+  #define cLCY ""
+  #define cBRI ""
+  #define cRST ""
+
+  #define bgBLK ""
+  #define bgRED ""
+  #define bgGRN ""
+  #define bgBRN ""
+  #define bgBLU ""
+  #define bgMGN ""
+  #define bgCYA ""
+  #define bgLGR ""
+  #define bgGRA ""
+  #define bgLRD ""
+  #define bgLGN ""
+  #define bgYEL ""
+  #define bgLBL ""
+  #define bgPIN ""
+  #define bgLCY ""
+  #define bgBRI ""
+
+#endif                                                                                                /* ^USE_COLOR */
+
+/*************************
+ * Box drawing sequences *
+ *************************/
+
+#ifdef FANCY_BOXES
+
+  #define SET_G1 "\x1b)0"                                                              /* Set G1 for box drawing    */
+  #define RESET_G1 "\x1b)B"                                                            /* Reset G1 to ASCII         */
+  #define bSTART "\x0e"                                                                /* Enter G1 drawing mode     */
+  #define bSTOP "\x0f"                                                                 /* Leave G1 drawing mode     */
+  #define bH "q"                                                                       /* Horizontal line           */
+  #define bV "x"                                                                       /* Vertical line             */
+  #define bLT "l"                                                                      /* Left top corner           */
+  #define bRT "k"                                                                      /* Right top corner          */
+  #define bLB "m"                                                                      /* Left bottom corner        */
+  #define bRB "j"                                                                      /* Right bottom corner       */
+  #define bX "n"                                                                       /* Cross                     */
+  #define bVR "t"                                                                      /* Vertical, branch right    */
+  #define bVL "u"                                                                      /* Vertical, branch left     */
+  #define bHT "v"                                                                      /* Horizontal, branch top    */
+  #define bHB "w"                                                                      /* Horizontal, branch bottom */
+
+#else
+
+  #define SET_G1 ""
+  #define RESET_G1 ""
+  #define bSTART ""
+  #define bSTOP ""
+  #define bH "-"
+  #define bV "|"
+  #define bLT "+"
+  #define bRT "+"
+  #define bLB "+"
+  #define bRB "+"
+  #define bX "+"
+  #define bVR "+"
+  #define bVL "+"
+  #define bHT "+"
+  #define bHB "+"
+
+#endif                                                                                              /* ^FANCY_BOXES */
+
+/***********************
+ * Misc terminal codes *
+ ***********************/
+
+#define TERM_HOME "\x1b[H"
+#define TERM_CLEAR TERM_HOME "\x1b[2J"
+#define cEOL "\x1b[0K"
+#define CURSOR_HIDE "\x1b[?25l"
+#define CURSOR_SHOW "\x1b[?25h"
+
+/************************
+ * Debug & error macros *
+ ************************/
+
+/* Just print stuff to the appropriate stream. */
+
+#ifdef MESSAGES_TO_STDOUT
+  #define SAYF(...) printf(__VA_ARGS__)
+#else
+  #define SAYF(...) fprintf(stderr, __VA_ARGS__)
+#endif                                                                                       /* ^MESSAGES_TO_STDOUT */
+
+/* Show a prefixed warning. */
+
+#define WARNF(...)                                       \
+  do {                                                   \
+                                                         \
+    SAYF(cYEL "[!] " cBRI "WARNING: " cRST __VA_ARGS__); \
+    SAYF(cRST "\n");                                     \
+                                                         \
+  } while (0)
+
+/* Show a prefixed "doing something" message. */
+
+#define ACTF(...)                       \
+  do {                                  \
+                                        \
+    SAYF(cLBL "[*] " cRST __VA_ARGS__); \
+    SAYF(cRST "\n");                    \
+                                        \
+  } while (0)
+
+/* Show a prefixed "success" message. */
+
+#define OKF(...)                        \
+  do {                                  \
+                                        \
+    SAYF(cLGN "[+] " cRST __VA_ARGS__); \
+    SAYF(cRST "\n");                    \
+                                        \
+  } while (0)
+
+/* Show a prefixed fatal error message (not used in afl). */
+
+#define BADF(...)                         \
+  do {                                    \
+                                          \
+    SAYF(cLRD "\n[-] " cRST __VA_ARGS__); \
+    SAYF(cRST "\n");                      \
+                                          \
+  } while (0)
+
+#ifdef DEBUG
+  #define DBG(...)                                                                      \
+    do {                                                                                \
+                                                                                        \
+      SAYF(cMGN "[D]" cGRA " [" __FILE__ ":" TOSTRING(__LINE__) "] " cRST __VA_ARGS__); \
+      SAYF(cRST "\n");                                                                  \
+      fflush(stdout);                                                                   \
+                                                                                        \
+    } while (0)
+
+#else
+  #define DBG(...) \
+    {}
+#endif
+
+/* Die with a verbose non-OS fatal error message. */
+
+#define FATAL(...)                                                                            \
+  do {                                                                                        \
+                                                                                              \
+    SAYF(bSTOP RESET_G1 CURSOR_SHOW cRST cLRD "\n[-] PROGRAM ABORT : " cRST __VA_ARGS__);     \
+    SAYF(cLRD "\n         Location : " cRST "%s(), %s:%u\n\n", __func__, __FILE__, __LINE__); \
+    exit(1);                                                                                  \
+                                                                                              \
+  } while (0)
+
+/* Die by calling abort() to provide a core dump. */
+
+#define ABORT(...)                                                                                \
+  do {                                                                                            \
+                                                                                                  \
+    SAYF(bSTOP RESET_G1 CURSOR_SHOW cRST cLRD "\n[-] PROGRAM ABORT : " cRST __VA_ARGS__);         \
+    SAYF(cLRD "\n    Stop location : " cRST "%s(), %s:%u\n\n", __FUNCTION__, __FILE__, __LINE__); \
+    abort();                                                                                      \
+                                                                                                  \
+  } while (0)
+
+/* Die while also including the output of perror(). */
+
+#define PFATAL(...)                                                                             \
+  do {                                                                                          \
+                                                                                                \
+    fflush(stdout);                                                                             \
+    SAYF(bSTOP RESET_G1 CURSOR_SHOW cRST cLRD "\n[-]  SYSTEM ERROR : " cRST __VA_ARGS__);       \
+    SAYF(cLRD "\n    Stop location : " cRST "%s(), %s:%u\n", __FUNCTION__, __FILE__, __LINE__); \
+    SAYF(cLRD "       OS message : " cRST "%s\n", strerror(errno));                             \
+    exit(1);                                                                                    \
+                                                                                                \
+  } while (0)
+
+/* Die with FATAL() or PFATAL() depending on the value of res (used to
+   interpret different failure modes for read(), write(), etc). */
+
+#define RPFATAL(res, ...)  \
+  do {                     \
+                           \
+    if (res < 0)           \
+      PFATAL(__VA_ARGS__); \
+    else                   \
+      FATAL(__VA_ARGS__);  \
+                           \
+  } while (0)
+
+/* Error-checking versions of read() and write() that call RPFATAL() as
+   appropriate. */
+
+#endif                                                                                           /* ! _HAVE_DEBUG_H */
+
diff -ruN qemu/include/libAFL/engine.h qemu_patched/include/libAFL/engine.h
--- qemu/include/libAFL/engine.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/engine.h	2023-12-29 01:19:42.645445391 +0100
@@ -0,0 +1,115 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   The Engine is the main and central part of the fuzzer. It contains the
+   queues, feedbacks, executor and the fuzz_one (which in turn has stages)
+
+ */
+
+#ifndef ENGINE_H
+#define ENGINE_H
+
+#include <unistd.h>
+#include <fcntl.h>
+
+#include "libAFL/common.h"
+#include "libAFL/queue.h"
+#include "libAFL/feedback.h"
+#include "libAFL/afl-returns.h"
+#include "libAFL/xxh3.h"
+#include "libAFL/xxhash.h"
+#include "libAFL/rand.h"
+//#include "llmp.h"
+
+struct afl_engine_func {
+
+  afl_queue_global_t *(*get_queue)(afl_engine_t *);
+  afl_fuzz_one_t *(*get_fuzz_one)(afl_engine_t *);
+  u64 (*get_execs)(afl_engine_t *);
+  u64 (*get_start_time)(afl_engine_t *);
+
+  void (*set_fuzz_one)(afl_engine_t *, afl_fuzz_one_t *);
+  afl_ret_t (*add_feedback)(afl_engine_t *, afl_feedback_t *);
+  void (*set_global_queue)(afl_engine_t *, afl_queue_global_t *);
+
+  u8 (*execute)(afl_engine_t *, afl_input_t *);
+  //afl_ret_t (*handle_new_message)(afl_engine_t *, llmp_message_t *);
+  afl_ret_t (*load_testcases_from_dir)(afl_engine_t *, char *);
+  //afl_ret_t (*load_testcases_from_dir)(afl_engine_t *);
+  void (*load_zero_testcase)(size_t);
+
+  afl_ret_t (*loop)(afl_engine_t *);
+
+};
+
+struct afl_engine {
+
+  afl_fuzz_one_t *      fuzz_one;
+  afl_queue_global_t *  global_queue;
+  afl_executor_t *      executor;
+  afl_queue_feedback_t *current_feedback_queue;
+  afl_feedback_t **     feedbacks;  // We're keeping a pointer of feedbacks here
+                                    // to save memory, consideting the original
+                                    // feedback would already be allocated
+  u64   executions, start_time, last_update, crashes, feedbacks_count;
+  u32   id;
+  u8    verbose;
+  //s32    cpu_bound;  // 1 if we want to bind to a cpu, 0 else 
+  char *in_dir;  // Input corpus directory
+
+  afl_rand_t rand;
+
+  u8 *                   buf;  // Reusable buf for realloc
+  struct afl_engine_func funcs;
+  //llmp_client_t *        llmp_client;  // Our IPC for fuzzer communication
+
+};
+
+/* TODO: Add default implementations for load_testcases and execute */
+afl_queue_global_t *afl_engine_get_queue(afl_engine_t *);
+afl_fuzz_one_t *    afl_engine_get_fuzz_one(afl_engine_t *);
+u64                 afl_get_execs(afl_engine_t *);
+u64                 afl_engine_get_start_time(afl_engine_t *);
+
+void      afl_set_fuzz_one(afl_engine_t *, afl_fuzz_one_t *);
+afl_ret_t afl_engine_add_feedback(afl_engine_t *, afl_feedback_t *);
+void      afl_set_global_queue(afl_engine_t *engine, afl_queue_global_t *global_queue);
+
+u8        afl_engine_execute(afl_engine_t *, afl_input_t *);
+afl_ret_t afl_engine_load_testcases_from_dir(afl_engine_t *, char *);
+// afl_ret_t afl_engine_load_testcases_from_dir(afl_engine_t *);
+void      afl_engine_load_zero_testcase(size_t);
+//afl_ret_t afl_engine_handle_new_message(afl_engine_t *, llmp_message_t *);
+
+afl_ret_t afl_engine_loop(afl_engine_t *);  // Not sure about this functions
+                                            // use-case. Was in FFF though.
+
+afl_ret_t afl_engine_init(afl_engine_t *, afl_executor_t *, afl_fuzz_one_t *, afl_queue_global_t *);
+void      afl_engine_deinit(afl_engine_t *);
+
+afl_ret_t afl_engine_check_configuration(afl_engine_t *engine);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_engine,
+                                   AFL_DECL_PARAMS(afl_executor_t *executor, afl_fuzz_one_t *fuzz_one,
+                                                   afl_queue_global_t *global_queue),
+                                   AFL_CALL_PARAMS(executor, fuzz_one, global_queue))
+
+#endif
+
diff -ruN qemu/include/libAFL/feedback.h qemu_patched/include/libAFL/feedback.h
--- qemu/include/libAFL/feedback.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/feedback.h	2023-11-13 17:08:59.124061362 +0100
@@ -0,0 +1,99 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#ifndef LIBFEEDBACK_H
+#define LIBFEEDBACK_H
+
+#include "libAFL/queue.h"
+#include "libAFL/observer.h"
+
+#define AFL_FEEDBACK_TAG_BASE (0xFEEDB43E)
+#define AFL_FEEDBACK_TAG_COV (0xFEEDC0F8)
+
+typedef struct afl_queue_feedback afl_queue_feedback_t;
+typedef struct afl_feedback       afl_feedback_t;
+
+struct afl_feedback_funcs {
+
+  float (*is_interesting)(afl_feedback_t *, afl_executor_t *);
+  void (*set_feedback_queue)(afl_feedback_t *, afl_queue_feedback_t *);
+  afl_queue_feedback_t *(*get_feedback_queue)(afl_feedback_t *);
+
+};
+
+struct afl_feedback {
+
+  afl_queue_feedback_t *queue;
+
+  struct afl_feedback_funcs funcs;
+  u32                       tag;
+
+};
+
+// Default implementation of the functions
+
+void                  afl_feedback_set_queue(afl_feedback_t *, afl_queue_feedback_t *);
+afl_queue_feedback_t *afl_feedback_get_queue(afl_feedback_t *);
+
+// "Constructors" and "destructors" for the feedback
+void      afl_feedback_deinit(afl_feedback_t *);
+afl_ret_t afl_feedback_init(afl_feedback_t *, afl_queue_feedback_t *queue);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_feedback, AFL_DECL_PARAMS(afl_queue_feedback_t *queue), AFL_CALL_PARAMS(queue))
+
+/* Simple MaximizeMapFeedback implementation */
+
+/* Coverage Feedback */
+typedef struct afl_feedback_cov {
+
+  afl_feedback_t base;
+
+  /* This array holds the coveragemap observation channels the feedback is looking at */
+  afl_observer_covmap_t *observer_cov;
+
+  /* questa per fare analisi e confrontare block con edge coverage*/
+  afl_observer_covmap_t *observer_block_cov; 
+  u8 *   virgin_bits;
+  size_t size;
+
+} afl_feedback_cov_t;
+
+afl_ret_t afl_feedback_cov_init(afl_feedback_cov_t *feedback, afl_queue_feedback_t *queue,
+                                afl_observer_covmap_t *map_observer);
+void      afl_feedback_cov_deinit();
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_feedback_cov,
+                                   AFL_DECL_PARAMS(afl_queue_feedback_t *queue, afl_observer_covmap_t *map_observer),
+                                   AFL_CALL_PARAMS(queue, map_observer))
+
+/* Set virgin bits according to the map passed into the func */
+afl_ret_t afl_feedback_cov_set_virgin_bits(afl_feedback_cov_t *feedback, u8 *virgin_bits_copy_from, size_t size);
+
+/* Returns the "interestingness" of the current feedback */
+float afl_feedback_cov_is_interesting(afl_feedback_t *feedback, afl_executor_t *fsrv);
+
+#endif
+
diff -ruN qemu/include/libAFL/fuzzone.h qemu_patched/include/libAFL/fuzzone.h
--- qemu/include/libAFL/fuzzone.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/fuzzone.h	2023-10-31 00:29:36.621151171 +0100
@@ -0,0 +1,59 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+
+
+ */
+
+#ifndef LIBFUZZONE_H
+#define LIBFUZZONE_H
+
+#include "libAFL/types.h"
+#include "libAFL/common.h"
+
+struct afl_fuzz_one_funcs {
+
+  afl_ret_t (*perform)(afl_fuzz_one_t *);
+  afl_ret_t (*add_stage)(afl_fuzz_one_t *, afl_stage_t *);
+  afl_ret_t (*set_engine)(afl_fuzz_one_t *, afl_engine_t *);
+
+};
+
+struct afl_fuzz_one {
+
+  afl_engine_t *engine;
+  afl_stage_t **stages;
+  size_t        stages_count;
+
+  struct afl_fuzz_one_funcs funcs;
+
+};
+
+afl_ret_t afl_fuzz_one_perform(afl_fuzz_one_t *);
+afl_ret_t afl_fuzz_one_add_stage(afl_fuzz_one_t *, afl_stage_t *);
+afl_ret_t afl_fuzz_one_set_engine(afl_fuzz_one_t *, afl_engine_t *);
+
+afl_ret_t afl_fuzz_one_init(afl_fuzz_one_t *, afl_engine_t *);
+void      afl_fuzz_one_deinit(afl_fuzz_one_t *);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_fuzz_one, AFL_DECL_PARAMS(afl_engine_t *engine), AFL_CALL_PARAMS(engine))
+
+#endif
+
diff -ruN qemu/include/libAFL/input.h qemu_patched/include/libAFL/input.h
--- qemu/include/libAFL/input.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/input.h	2023-11-28 18:54:50.128265407 +0100
@@ -0,0 +1,92 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#ifndef LIBINPUT_H
+#define LIBINPUT_H
+
+#include "libAFL/common.h"
+#include "libAFL/afl-returns.h"
+
+typedef struct afl_input afl_input_t;
+
+struct afl_input_funcs {
+
+  void (*deserialize)(afl_input_t *this_input, u8 *bytes, size_t len);
+  u8 *(*serialize)(afl_input_t *this_input);
+  afl_input_t *(*copy)(afl_input_t *this_input);
+  void (*restore)(afl_input_t *this_input, afl_input_t *input);
+  /* Deve essere modificata/eliminata -> capire da dove deve leggere gli input */
+  afl_ret_t (*load_from_file)(afl_input_t *this_input, char *fname);
+    /* Deve essere modificata per salvare gli input nell'unico file esistente */
+  afl_ret_t (*save_to_file)(afl_input_t *this_input, char *fname, char* prova);
+ 
+  void (*clear)(afl_input_t *this_input);
+  u8 *(*get_bytes)(afl_input_t *this_input);
+
+  void (*delete)(afl_input_t *this_input);
+
+};
+
+struct afl_input {
+
+  u8 *   bytes;  // Raw input bytes
+  size_t len;    // Length of the input
+
+  u8 *copy_buf;
+
+  struct afl_input_funcs funcs;
+
+};
+
+afl_ret_t afl_input_init(afl_input_t *input);
+void      afl_input_deinit(afl_input_t *input);
+
+// Default implementations of the functions for raw input vtable
+
+void         afl_input_deserialize(afl_input_t *this_input, u8 *bytes, size_t len);
+u8 *         afl_input_serialize(afl_input_t *this_input);
+afl_input_t *afl_input_copy(afl_input_t *this_input);
+void         afl_input_restore(afl_input_t *this_input, afl_input_t *input);
+afl_ret_t    afl_input_load_from_file(afl_input_t *this_inputinput, char *fname);
+afl_ret_t    afl_input_write_to_file(afl_input_t *this_input, char *fname);
+void         afl_input_clear(afl_input_t *this_input);
+u8 *         afl_input_get_bytes(afl_input_t *this_input);
+
+/* Write the contents of the input to a file at the given loc */
+afl_ret_t afl_input_write_to_file(afl_input_t *data, char *filename);
+
+/* Write the contents of the input to a timeoutfile */
+afl_ret_t afl_input_dump_to_timeoutfile(afl_input_t *data, char *);
+
+/* Write the contents of the input which causes a crash in the target to a crashfile */
+afl_ret_t afl_input_dump_to_crashfile(afl_input_t *, char *);
+
+/* Function to create and destroy a new input, allocates memory and initializes
+  it. In destroy, it first deinitializes the struct and then frees it. */
+
+AFL_NEW_AND_DELETE_FOR(afl_input);
+
+#endif
diff -ruN qemu/include/libAFL/mutator.h qemu_patched/include/libAFL/mutator.h
--- qemu/include/libAFL/mutator.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/mutator.h	2023-11-12 03:39:00.426663523 +0100
@@ -0,0 +1,176 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+
+ */
+
+#ifndef LIBMUTATOR_H
+#define LIBMUTATOR_H
+
+#include "libAFL/input.h"
+#include "libAFL/queue.h"
+#include "libAFL/engine.h"
+
+// Mutator struct will have many internal functions like mutate, trimming etc.
+// This is based on both the FFF prototype and the custom mutators that we have
+// in AFL++ without the AFL++ specific parts
+
+struct afl_mutator_funcs {
+
+  void (*init)(afl_mutator_t *);
+
+  // The params here are in_buf and out_buf.
+  size_t (*trim)(afl_mutator_t *, afl_input_t *);
+
+  // Mutate function
+  size_t (*mutate)(afl_mutator_t *, afl_input_t *);
+
+  // Checks if the queue entry is to be fuzzed or not
+  afl_ret_t (*custom_queue_get)(afl_mutator_t *, afl_input_t *);
+  void (*custom_queue_new_entry)(afl_mutator_t *, afl_entry_t *);
+  // Post process API AFL++
+  void (*post_process)(afl_mutator_t *, afl_input_t *);
+
+  afl_stage_t *(*get_stage)(afl_mutator_t *);
+
+};
+
+struct afl_mutator {
+
+  afl_engine_t *engine;
+  u8 *          mutate_buf;  // Extra buf for mutators to work with for afl_realloc
+
+  struct afl_mutator_funcs funcs;
+
+};
+
+size_t       afl_mutator_trim(afl_mutator_t *, u8 *, u8 *);
+afl_stage_t *afl_mutator_get_stage(afl_mutator_t *);
+
+afl_ret_t afl_mutator_init(afl_mutator_t *, afl_engine_t *);
+void      afl_mutator_deinit(afl_mutator_t *);
+
+// A simple scheduled mutator based on the above mutator. Will act something
+// similar to the havoc stage
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_mutator, AFL_DECL_PARAMS(afl_engine_t *engine), AFL_CALL_PARAMS(engine))
+
+typedef struct afl_mutator_scheduled afl_mutator_scheduled_t;
+typedef void (*afl_mutator_func)(afl_mutator_t *, afl_input_t *);
+
+struct afl_mutator_scheduled_funcs {
+
+  size_t (*schedule)(afl_mutator_scheduled_t *);
+  afl_ret_t (*add_func)(afl_mutator_scheduled_t *, afl_mutator_func);
+  afl_ret_t (*add_default_funcs)(afl_mutator_scheduled_t *);
+  size_t (*get_iters)(afl_mutator_scheduled_t *);
+
+};
+
+struct afl_mutator_scheduled {
+
+  afl_mutator_t     base;
+  afl_mutator_func *mutations;  // A ptr to an array of mutation operator
+                                // functions
+  size_t                             mutators_count;
+  struct afl_mutator_scheduled_funcs funcs;
+  size_t                             max_iterations;
+
+};
+
+/* TODO add implementation for the _schedule_ and _iterations_ functions, need a
+ * random list element pop type implementation for this */
+size_t afl_iterations(afl_mutator_scheduled_t *);
+/* Add a mutator func to this mutators */
+afl_ret_t afl_mutator_add_func(afl_mutator_scheduled_t *, afl_mutator_func);
+/* Add all default mutator funcs */
+afl_ret_t afl_mutator_scheduled_add_havoc_funcs(afl_mutator_scheduled_t *mutator);
+size_t    afl_schedule(afl_mutator_scheduled_t *);
+size_t    afl_mutate_scheduled_mutator(afl_mutator_t *, afl_input_t *);
+
+afl_ret_t afl_mutator_scheduled_init(afl_mutator_scheduled_t *sched_mut, afl_engine_t *engine, size_t max_iterations);
+void      afl_mutator_scheduled_deinit(afl_mutator_scheduled_t *);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_mutator_scheduled, AFL_DECL_PARAMS(afl_engine_t *engine, size_t max_iterations),
+                                   AFL_CALL_PARAMS(engine, max_iterations))
+
+void afl_mutfunc_flip_bit(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_flip_2_bits(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_flip_4_bits(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_flip_byte(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_flip_2_bytes(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_flip_4_bytes(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_random_byte_add_sub(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_random_byte(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_delete_bytes(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_clone_bytes(afl_mutator_t *mutator, afl_input_t *input);
+void afl_mutfunc_splice(afl_mutator_t *mutator, afl_input_t *input);
+
+/* Let's create a deterministic mutator structure now. Unlike havoc, deterministic stage isn't "completely" random
+ * mutation AND we need to keep the state of previous mutations*/
+
+typedef struct afl_mutator_deterministic afl_mutator_deterministic_t;
+
+struct afl_mutator_deterministic_func {
+
+  size_t (*get_iters)(afl_mutator_deterministic_t *, afl_input_t *);
+
+};
+
+struct afl_mutator_deterministic {
+
+  afl_mutator_t                         base;
+  struct afl_mutator_deterministic_func funcs;
+  s32 stage_cur;  /* Keeps track of the state of random mutations such as "bitflip" etc */
+  s32 stage_max;
+
+};
+
+typedef size_t (*det_mutate_func)(afl_mutator_t *, afl_input_t *);
+
+afl_ret_t afl_mutator_deterministic_init(afl_mutator_deterministic_t *det_mut, det_mutate_func mutate_func,
+                                         size_t (*get_iters)(afl_mutator_deterministic_t *, afl_input_t *));
+
+void afl_mutator_deterministic_deinit(afl_mutator_deterministic_t *det_mut);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_mutator_deterministic,
+                                   AFL_DECL_PARAMS(det_mutate_func mutate_func,
+                                                   size_t (*get_iters)(afl_mutator_deterministic_t *, afl_input_t *)),
+                                   AFL_CALL_PARAMS(mutate_func, get_iters))
+
+size_t afl_mutate_bitflip_det(afl_mutator_t *mutator, afl_input_t *input);
+size_t afl_get_iters_bitflip_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input);
+
+size_t afl_mutate_det_flip_two(afl_mutator_t *mutator, afl_input_t *input);
+size_t afl_get_iters_flip_two_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input);
+
+size_t afl_mutate_det_flip_four(afl_mutator_t *mutator, afl_input_t *input);
+size_t afl_get_iters_flip_four_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input);
+
+size_t afl_mutate_det_flip_byte(afl_mutator_t *mutator, afl_input_t *input);
+size_t afl_get_iters_flip_byte_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input);
+
+size_t afl_mutate_det_flip_two_byte(afl_mutator_t *mutator, afl_input_t *input);
+size_t afl_get_iters_flip_two_byte_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input);
+
+size_t afl_mutate_det_flip_four_byte(afl_mutator_t *mutator, afl_input_t *input);
+size_t afl_get_iters_flip_four_byte_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input);
+
+#endif
\ Manca newline alla fine del file
diff -ruN qemu/include/libAFL/observer.h qemu_patched/include/libAFL/observer.h
--- qemu/include/libAFL/observer.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/observer.h	2023-11-13 17:10:00.670129474 +0100
@@ -0,0 +1,105 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#ifndef LIBOBSERVATIONCHANNEL_H
+#define LIBOBSERVATIONCHANNEL_H
+
+#include "libAFL/common.h"
+#include "libAFL/shmem.h"
+#include "libAFL/afl-returns.h"
+
+#define AFL_OBSERVER_TAG_BASE (0x0B5EB45E)
+#define AFL_OBSERVER_TAG_COVMAP (0x0B5EC0FE)
+
+typedef struct afl_observer afl_observer_t;
+
+// vtable for the observation channel
+
+struct afl_observer_funcs {
+
+  void (*flush)(afl_observer_t *);
+  void (*reset)(afl_observer_t *);
+  void (*post_exec)(afl_observer_t *, afl_engine_t *);
+
+};
+
+struct afl_observer {
+
+  u32                       tag;
+  struct afl_observer_funcs funcs;
+
+};
+
+/* They're void now, but I think post_exec should have some return type? Since,
+ * they'll mostly be implemented by user */
+void afl_observer_flush(afl_observer_t *);
+void afl_observer_reset(afl_observer_t *);
+void afl_observer_post_exec(afl_observer_t *);
+// Functions to initialize and deinitialize the generic observation channel. P.S
+// You probably will need to extend it the way we've done below.
+
+afl_ret_t afl_observer_init(afl_observer_t *channel);
+void      afl_observer_deinit(afl_observer_t *);
+
+/* Function to create and destroy a new observation channel, allocates memory
+  and initializes it. In destroy, it first deinitializes the struct and then
+  frees it. */
+
+AFL_NEW_AND_DELETE_FOR(afl_observer)
+
+typedef struct afl_observer_covmap afl_observer_covmap_t;
+
+struct afl_observer_covmap_funcs {
+
+  u8 *(*get_trace_bits)(afl_observer_covmap_t *);
+  size_t (*get_map_size)(afl_observer_covmap_t *);
+
+};
+
+struct afl_observer_covmap {
+
+  afl_observer_t base;  // Base observation channel "class"
+
+  afl_shmem_t shared_map;
+  afl_shmem_t shared_map_block_coverage; // to debug
+
+  struct afl_observer_covmap_funcs funcs;
+
+};
+
+u8 *   afl_observer_covmap_get_trace_bits(afl_observer_covmap_t *obs_channel);
+size_t afl_observer_covmap_get_map_size(afl_observer_covmap_t *obs_channel);
+
+// Functions to initialize and delete a map based observation channel
+
+afl_ret_t afl_observer_covmap_init(afl_observer_covmap_t *, size_t map_size);
+void      afl_observer_covmap_deinit(afl_observer_covmap_t *);
+void      afl_observer_covmap_reset(afl_observer_t *);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_observer_covmap, AFL_DECL_PARAMS(size_t map_size), AFL_CALL_PARAMS(map_size))
+
+#endif
+
diff -ruN qemu/include/libAFL/os.h qemu_patched/include/libAFL/os.h
--- qemu/include/libAFL/os.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/os.h	2023-10-31 00:29:49.361184883 +0100
@@ -0,0 +1,108 @@
+#ifndef LIBOS_H
+#define LIBOS_H
+
+#include "libAFL/common.h"
+#include "libAFL/input.h"
+
+#include <stdbool.h>
+#include <sys/types.h>
+
+// This has a few parts, the first deals with crash handling.
+
+/* afl_exit_t is for the fuzzed target, as opposed to afl_ret_t
+which is for internal functions. */
+/*typedef enum afl_exit {
+
+  AFL_EXIT_OK,
+  AFL_EXIT_STOP,
+  AFL_EXIT_CRASH,
+  AFL_EXIT_SEGV,
+  AFL_EXIT_BUS,
+  AFL_EXIT_ABRT,
+  AFL_EXIT_ILL,
+  AFL_EXIT_FPE,
+  AFL_EXIT_TIMEOUT,
+  AFL_EXIT_OOM,
+
+} afl_exit_t; old*/
+
+
+/* afl_exit_t is for the fuzzed target, as opposed to afl_ret_t
+which is for internal functions. */
+typedef enum afl_exit {
+
+  AFL_EXIT_OK,
+  AFL_EXIT_STOP,
+  AFL_EXIT_CRASH,
+  AFL_EXIT_SEGV,
+  AFL_EXIT_BUS,
+  AFL_EXIT_ABRT,
+  AFL_EXIT_ILL,
+  AFL_EXIT_FPE,
+  AFL_EXIT_TIMEOUT,
+  AFL_EXIT_OOM,
+  AFL_EXIT_ERROR_DETECTED,
+
+} afl_exit_t;
+
+
+
+/* TODO: Add implementations for installing crash handlers */
+typedef void (*afl_crash_handler_func)(afl_exit_t type, void *data);
+
+void install_crash_handler(afl_crash_handler_func callback);
+
+typedef enum afl_fork_result { FORK_FAILED, CHILD, PARENT } afl_fork_result_t;
+
+typedef struct afl_os {
+
+  struct afl_os *(*current)(struct afl_os *);
+  afl_fork_result_t (*fork)(struct afl_os *);
+  void (*suspend)(struct afl_os *);
+  void (*resume)(struct afl_os *);
+  afl_exit_t (*wait)(struct afl_os *, bool untraced);
+
+  pid_t handler_process;  // Something similar to the child process
+
+} afl_os_t;
+
+void _afl_process_init_internal(afl_os_t *);
+
+static inline afl_os_t *afl_process_init(afl_os_t *process, pid_t handler_pid) {
+
+  afl_os_t *new_process;
+
+  if (process) {
+
+    _afl_process_init_internal(process);
+    process->handler_process = handler_pid;
+    return process;
+
+  }
+
+  else {
+
+    new_process = calloc(1, sizeof(afl_os_t));
+    if (!new_process) { return NULL; }
+    _afl_process_init_internal(new_process);
+    new_process->handler_process = (handler_pid);
+
+  }
+
+  return new_process;
+
+}
+
+afl_fork_result_t afl_proc_fork(afl_os_t *);
+void              afl_proc_suspend(afl_os_t *);
+void              afl_proc_resume(afl_os_t *);
+afl_exit_t        afl_proc_wait(afl_os_t *, bool);
+
+afl_ret_t bind_to_cpu();
+/* Run `handle_file` for each file in the dirpath, recursively.
+void *data will be passed to handle_file as 2nd param.
+if handle_file returns false, further execution stops. */
+afl_ret_t afl_for_each_file(char *dirpath, bool (*handle_file)(char *filename, void *data), void *data);
+
+#endif
+
diff -ruN qemu/include/libAFL/queue.h qemu_patched/include/libAFL/queue.h
--- qemu/include/libAFL/queue.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/queue.h	2023-11-12 03:40:35.302046274 +0100
@@ -0,0 +1,231 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#ifndef LIBQUEUE_H
+#define LIBQUEUE_H
+
+#include <stdbool.h>
+#include <limits.h>
+
+#include "libAFL/input.h"
+#include "libAFL/shmem.h"
+#include "libAFL/feedback.h"
+
+/*
+This is the generic interface implementation for the queue and queue entries.
+We've tried to keep it generic and yet including, but if you want to extend the
+queue/entry, simply "inherit" this struct by including it in your custom struct
+and keeping it as the first member of your struct.
+*/
+
+typedef struct afl_queue    afl_queue_t;
+typedef struct afl_entry    afl_entry_t;
+typedef struct afl_feedback afl_feedback_t;
+
+struct afl_entry_funcs {
+
+  afl_input_t *(*get_input)(afl_entry_t *);
+  bool (*is_on_disk)(afl_entry_t *);
+  afl_entry_t *(*get_next)(afl_entry_t *);
+  afl_entry_t *(*get_prev)(afl_entry_t *);
+  afl_entry_t *(*get_parent)(afl_entry_t *);
+  afl_entry_t *(*get_child)(afl_entry_t *, size_t);        /*TODO: Still need to add a base implementation for this.*/
+
+};
+
+typedef struct __attribute__((__packed__)) afl_entry_info {
+
+  u64 hash, exec_us;
+  u32 bytes_set, bits_set;
+  u8  trimmed, has_new_coverage, variable, skip_entry, det_done;
+
+} afl_entry_info_t;
+
+struct afl_entry {
+
+  afl_entry_info_t *info;
+  afl_input_t *     input;
+  u8 *              map;
+  bool              on_disk, info_calloc;
+  char              filename[FILENAME_LEN_MAX];
+  struct afl_queue *queue;
+  struct afl_entry *next;
+  struct afl_entry *prev;
+  struct afl_entry *parent;
+
+  struct afl_entry_funcs funcs;
+
+};
+
+afl_ret_t afl_entry_init(afl_entry_t *, afl_input_t *, afl_entry_info_t *);
+void      afl_entry_deinit(afl_entry_t *);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_entry, AFL_DECL_PARAMS(afl_input_t *input, afl_entry_info_t *info),
+                                   AFL_CALL_PARAMS(input, info))
+// AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_queue_feedback, AFL_DECL_PARAMS(afl_feedback_t *feedback, char *name),
+//                                   AFL_CALL_PARAMS(feedback, name));
+
+// Default implementations for the functions for queue_entry vtable
+afl_input_t *afl_entry_get_input(afl_entry_t *entry);
+afl_entry_t *afl_entry_get_next(afl_entry_t *entry);
+afl_entry_t *afl_entry_get_prev(afl_entry_t *entry);
+afl_entry_t *afl_entry_get_parent(afl_entry_t *entry);
+
+typedef struct afl_queue afl_queue_t;
+
+struct afl_queue_funcs {
+
+  afl_ret_t (*insert)(afl_queue_t *, afl_entry_t *);
+  void (*remove_from_queue)(afl_queue_t *);
+
+  afl_entry_t *(*get)(afl_queue_t *);
+  afl_entry_t *(*get_next_in_queue)(afl_queue_t *, int);
+  afl_entry_t *(*get_queue_entry)(afl_queue_t *, u32);
+  afl_entry_t *(*get_queue_base)(afl_queue_t *);
+  size_t (*get_size)(afl_queue_t *);
+  char *(*get_dirpath)(afl_queue_t *);
+  size_t (*get_names_id)(afl_queue_t *);
+  bool (*get_save_to_files)(afl_queue_t *);
+
+  void (*set_dirpath)(afl_queue_t *, char *);
+  void (*set_engine)(afl_queue_t *, afl_engine_t *);
+
+};
+
+struct afl_queue {
+
+  afl_entry_t **         entries;
+  size_t                 entries_count;
+  afl_entry_t *          base;
+  u64                    current;
+  int                    engine_id;
+  afl_engine_t *         engine;
+  afl_entry_t *          end;
+  char                   dirpath[PATH_MAX];
+  size_t                 names_id;
+  bool                   save_to_files;
+  bool                   fuzz_started;
+  struct afl_queue_funcs funcs;
+
+};
+
+/* TODO: Add the base  */
+
+afl_ret_t afl_queue_init(afl_queue_t *);
+void      afl_queue_deinit(afl_queue_t *);
+
+afl_ret_t    afl_queue_insert(afl_queue_t *, afl_entry_t *);
+size_t       afl_queue_get_size(afl_queue_t *);
+char *       afl_queue_get_dirpath(afl_queue_t *);
+size_t       afl_queue_get_names_id(afl_queue_t *);
+bool         afl_queue_should_save_to_file(afl_queue_t *);
+void         afl_queue_set_dirpath(afl_queue_t *, char *);
+void         afl_queue_set_engine(afl_queue_t *queue, afl_engine_t *engine);
+void         afl_queue_global_set_engine(afl_queue_t *, afl_engine_t *);
+afl_entry_t *afl_queue_next_base_queue(afl_queue_t *queue, int engine_id);
+afl_entry_t *afl_queue_get_entry(afl_queue_t *queue, u32 entry);
+
+AFL_NEW_AND_DELETE_FOR(afl_queue)
+
+typedef struct afl_queue_feedback {
+
+  afl_queue_t base;  // Inheritence from base queue
+
+  afl_feedback_t *feedback;
+  char *          name;
+
+} afl_queue_feedback_t;
+
+afl_ret_t afl_queue_feedback_init(afl_queue_feedback_t *, afl_feedback_t *,
+                                  char *);  // "constructor" for the above feedback queue
+
+void afl_queue_feedback_deinit(afl_queue_feedback_t *);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_queue_feedback, AFL_DECL_PARAMS(afl_feedback_t *feedback, char *name),
+                                   AFL_CALL_PARAMS(feedback, name));
+
+typedef struct afl_queue_global afl_queue_global_t;
+
+struct afl_queue_global_funcs {
+
+  int (*schedule)(afl_queue_global_t *);
+  afl_ret_t (*add_feedback_queue)(afl_queue_global_t *, afl_queue_feedback_t *);
+
+};
+
+struct afl_queue_global {
+
+  afl_queue_t            base;
+  afl_queue_feedback_t **feedback_queues;  // One global queue can have
+                                           // multiple feedback queues
+
+  size_t feedback_queues_count;
+
+  struct afl_queue_global_funcs funcs;
+  /*TODO: Add a map of Engine:feedback_queue
+    UPDATE: Engine will have a ptr to current feedback queue rather than this*/
+
+};
+
+// Default implementations of global queue vtable functions
+afl_ret_t afl_queue_global_add_feedback_queue(afl_queue_global_t *, afl_queue_feedback_t *);
+int       afl_queue_global_schedule(afl_queue_global_t *);
+void      afl_queue_global_set_engine(afl_queue_t *, afl_engine_t *);
+
+// Function to get next entry from queue, we override the base_queue
+// implementation
+afl_entry_t *afl_queue_next_global_queue(afl_queue_t *queue, int engine_id);
+
+/* Register this as global queue for the engine.
+TODO: Make this a method of engine instead */
+//void afl_queue_global_set_engine(afl_queue_t *global_queue_base, afl_engine_t *engine);
+
+/* TODO: ADD defualt implementation for the schedule function based on random.
+ */
+
+afl_ret_t afl_queue_global_init(afl_queue_global_t *);
+/*
+inline afl_ret_t afl_queue_global_init(afl_queue_global_t *global_queue){
+    afl_queue_init(&(global_queue->base));
+
+  global_queue->feedback_queues_count = 0;
+  global_queue->feedback_queues = NULL;
+
+  global_queue->base.funcs.set_engine = afl_queue_global_set_engine;
+
+  global_queue->funcs.add_feedback_queue = afl_queue_global_add_feedback_queue;
+  global_queue->funcs.schedule = afl_queue_global_schedule;
+  global_queue->base.funcs.get_next_in_queue = afl_queue_next_global_queue;
+  global_queue->base.funcs.set_engine = afl_queue_global_set_engine;
+
+  return AFL_RET_SUCCESS;
+}*/
+void      afl_queue_global_deinit(afl_queue_global_t *);
+
+AFL_NEW_AND_DELETE_FOR(afl_queue_global)
+
+#endif
+
diff -ruN qemu/include/libAFL/rand.h qemu_patched/include/libAFL/rand.h
--- qemu/include/libAFL/rand.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/rand.h	2023-11-12 03:28:08.146364420 +0100
@@ -0,0 +1,31 @@
+#ifndef AFL_RAND_H
+#define AFL_RAND_H
+
+#include <fcntl.h>
+#include "libAFL/types.h"
+#include "libAFL/common.h"
+#include "libAFL/xxh3.h"
+
+//#define UINT64_MAX  0xffffffffffffffffULL 
+
+typedef struct afl_rand {
+
+  u32  rand_cnt;                                                                            /* Random number counter*/
+  u64  rand_seed[4];
+  s32  dev_urandom_fd;
+  s64  init_seed;
+  bool fixed_seed;
+} afl_rand_t;
+
+
+u64 afl_rand_below(afl_rand_t *rnd, u64 limit);
+u64 afl_rand_between(afl_rand_t *rand, u64 min, u64 max);
+afl_ret_t afl_rand_init_fixed_seed(afl_rand_t *rnd, s64 init_seed);
+afl_ret_t afl_rand_init(afl_rand_t *rnd);
+u64 afl_rand_next(afl_rand_t *rnd);
+void afl_rand_deinit(afl_rand_t *rnd);
+
+AFL_NEW_AND_DELETE_FOR(afl_rand);
+
+#endif                                                                                                /* AFL_RAND_H */
+
diff -ruN qemu/include/libAFL/shmem.h qemu_patched/include/libAFL/shmem.h
--- qemu/include/libAFL/shmem.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/shmem.h	2023-10-31 00:30:02.505218806 +0100
@@ -0,0 +1,38 @@
+#ifndef AFL_SHMEM_H
+#define AFL_SHMEM_H
+
+#include "libAFL/types.h"
+
+#define AFL_SHMEM_STRLEN_MAX (20)
+
+// A generic sharememory region to be used by any functions (queues or feedbacks
+// too.)
+
+typedef struct afl_shmem {
+
+  /* Serialized map id */
+  char shm_str[AFL_SHMEM_STRLEN_MAX];
+#ifdef USEMMAP
+  int g_shm_fd;
+#else
+  int shm_id;
+#endif
+
+  u8 *   map;
+  size_t map_size;
+
+} afl_shmem_t;
+
+// Functions to create Shared memory region, for observation channels and
+// opening inputs and stuff.
+u8 * afl_shmem_init(afl_shmem_t *sharedmem, size_t map_size);
+u8 * afl_shmem_by_str(afl_shmem_t *shm, char *shm_str, size_t map_size);
+void afl_shmem_deinit(afl_shmem_t *sharedmem);
+
+/* Write sharedmap as env var */
+afl_ret_t afl_shmem_to_env_var(afl_shmem_t *shmem, char *env_name);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_shmem, AFL_DECL_PARAMS(size_t map_size), AFL_CALL_PARAMS(map_size))
+
+#endif                                                                                               /* AFL_SHMEM_H */
+
diff -ruN qemu/include/libAFL/snapshot-inl.h qemu_patched/include/libAFL/snapshot-inl.h
--- qemu/include/libAFL/snapshot-inl.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/snapshot-inl.h	2023-10-27 20:26:54.000000000 +0200
@@ -0,0 +1,111 @@
+/*
+   american fuzzy lop++ - snapshot helpers routines
+   ------------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Forkserver design by Jann Horn <jannhorn@googlemail.com>
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+// From AFL-Snapshot-LKM/include/afl_snapshot.h (must be kept synced)
+
+#include <sys/ioctl.h>
+#include <stdlib.h>
+#include <fcntl.h>
+
+#define AFL_SNAPSHOT_FILE_NAME "/dev/afl_snapshot"
+
+#define AFL_SNAPSHOT_IOCTL_MAGIC 44313
+
+#define AFL_SNAPSHOT_IOCTL_DO _IO(AFL_SNAPSHOT_IOCTL_MAGIC, 1)
+#define AFL_SNAPSHOT_IOCTL_CLEAN _IO(AFL_SNAPSHOT_IOCTL_MAGIC, 2)
+#define AFL_SNAPSHOT_EXCLUDE_VMRANGE _IOR(AFL_SNAPSHOT_IOCTL_MAGIC, 3, struct afl_snapshot_vmrange_args *)
+#define AFL_SNAPSHOT_INCLUDE_VMRANGE _IOR(AFL_SNAPSHOT_IOCTL_MAGIC, 4, struct afl_snapshot_vmrange_args *)
+#define AFL_SNAPSHOT_IOCTL_TAKE _IOR(AFL_SNAPSHOT_IOCTL_MAGIC, 5, int)
+#define AFL_SNAPSHOT_IOCTL_RESTORE _IO(AFL_SNAPSHOT_IOCTL_MAGIC, 6)
+
+// Trace new mmaped ares and unmap them on restore.
+#define AFL_SNAPSHOT_MMAP 1
+// Do not snapshot any page (by default all writeable not-shared pages
+// are shanpshotted.
+#define AFL_SNAPSHOT_BLOCK 2
+// Snapshot file descriptor state, close newly opened descriptors
+#define AFL_SNAPSHOT_FDS 4
+// Snapshot registers state
+#define AFL_SNAPSHOT_REGS 8
+// Perform a restore when exit_group is invoked
+#define AFL_SNAPSHOT_EXIT 16
+// TODO(andrea) allow not COW snapshots (high perf on small processes)
+// Disable COW, restore all the snapshotted pages
+#define AFL_SNAPSHOT_NOCOW 32
+// Do not snapshot Stack pages
+#define AFL_SNAPSHOT_NOSTACK 64
+
+struct afl_snapshot_vmrange_args {
+
+  unsigned long start, end;
+
+};
+
+static int afl_snapshot_dev_fd;
+
+static int afl_snapshot_init(void) {
+
+  afl_snapshot_dev_fd = open(AFL_SNAPSHOT_FILE_NAME, 0);
+  return afl_snapshot_dev_fd;
+
+}
+
+static void afl_snapshot_exclude_vmrange(void *start, void *end) {
+
+  struct afl_snapshot_vmrange_args args = {(unsigned long)start, (unsigned long)end};
+  ioctl(afl_snapshot_dev_fd, AFL_SNAPSHOT_EXCLUDE_VMRANGE, &args);
+
+}
+
+static void afl_snapshot_include_vmrange(void *start, void *end) {
+
+  struct afl_snapshot_vmrange_args args = {(unsigned long)start, (unsigned long)end};
+  ioctl(afl_snapshot_dev_fd, AFL_SNAPSHOT_INCLUDE_VMRANGE, &args);
+
+}
+
+static int afl_snapshot_take(int config) {
+
+  return ioctl(afl_snapshot_dev_fd, AFL_SNAPSHOT_IOCTL_TAKE, config);
+
+}
+
+static int afl_snapshot_do(void) {
+
+  return ioctl(afl_snapshot_dev_fd, AFL_SNAPSHOT_IOCTL_DO);
+
+}
+
+static void afl_snapshot_restore(void) {
+
+  ioctl(afl_snapshot_dev_fd, AFL_SNAPSHOT_IOCTL_RESTORE);
+
+}
+
+static void afl_snapshot_clean(void) {
+
+  ioctl(afl_snapshot_dev_fd, AFL_SNAPSHOT_IOCTL_CLEAN);
+
+}
+
diff -ruN qemu/include/libAFL/stage.h qemu_patched/include/libAFL/stage.h
--- qemu/include/libAFL/stage.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/stage.h	2023-10-31 00:30:14.553249168 +0100
@@ -0,0 +1,70 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+#ifndef LIBSTAGE_H
+#define LIBSTAGE_H
+
+#include "libAFL/input.h"
+#include "libAFL/queue.h"
+
+struct afl_stage_funcs {
+  
+  afl_ret_t (*perform)(afl_stage_t *, afl_entry_t *);
+  //afl_ret_t (*perform)(afl_stage_t *, afl_input_t *input);
+  size_t (*get_iters)(afl_stage_t *);  // A function which tells how many mutated
+                                       // inputs to generate out of a given input
+  /* Change the void pointer to a mutator * once it is ready */
+  afl_ret_t (*add_mutator_to_stage)(afl_stage_t *, afl_mutator_t *);
+
+};
+
+struct afl_stage {
+
+  afl_engine_t *         engine;
+  struct afl_stage_funcs funcs;
+  /* The list of mutator operators that this stage has */
+  afl_mutator_t **mutators;
+
+  size_t mutators_count;
+
+};
+
+afl_ret_t afl_stage_run(afl_stage_t *, afl_input_t *, bool);
+float     afl_stage_is_interesting(afl_stage_t *);
+//afl_ret_t afl_stage_perform(afl_stage_t *, afl_input_t *);
+afl_ret_t afl_stage_perform(afl_stage_t *, afl_entry_t *);
+size_t    afl_stage_get_iters(afl_stage_t *);
+afl_ret_t afl_stage_init(afl_stage_t *, afl_engine_t *);
+void      afl_stage_deinit(afl_stage_t *);
+afl_ret_t afl_stage_add_mutator(afl_stage_t *, afl_mutator_t *);
+
+AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(afl_stage, AFL_DECL_PARAMS(afl_engine_t *engine), AFL_CALL_PARAMS(engine))
+
+//added by me
+
+afl_ret_t afl_det_stage_perform(afl_stage_t *det_stage, afl_entry_t *entry);  /* The perform function for the det stage. */
+afl_ret_t afl_det_stage_get_iters(afl_stage_t *det_stage);
+afl_ret_t afl_det_stage_init(afl_stage_t *det_stage, afl_engine_t *engine);
+
+
+#endif
+
diff -ruN qemu/include/libAFL/types.h qemu_patched/include/libAFL/types.h
--- qemu/include/libAFL/types.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/types.h	2023-10-31 00:30:17.037255322 +0100
@@ -0,0 +1,192 @@
+/*
+   american fuzzy lop++ - type definitions and minor macros
+   --------------------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+#ifndef TYPES_H
+#define TYPES_H
+
+#include <stdint.h>
+#include <stdlib.h>
+
+#include "libAFL/afl-returns.h"
+
+typedef uint8_t  u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+
+/* Extended forkserver option values */
+
+/* Reporting errors */
+#define FS_OPT_ERROR 0xf800008f
+#define FS_OPT_GET_ERROR(x) ((x & 0x00ffff00) >> 8)
+#define FS_OPT_SET_ERROR(x) ((x & 0x0000ffff) << 8)
+#define FS_ERROR_MAP_SIZE 1
+#define FS_ERROR_MAP_ADDR 2
+#define FS_ERROR_SHM_OPEN 4
+#define FS_ERROR_SHMAT 8
+#define FS_ERROR_MMAP 16
+
+/* Reporting options */
+#define FS_OPT_ENABLED 0x80000001
+#define FS_OPT_MAPSIZE 0x40000000
+#define FS_OPT_SNAPSHOT 0x20000000
+#define FS_OPT_AUTODICT 0x10000000
+#define FS_OPT_SHDMEM_FUZZ 0x01000000
+#define FS_OPT_OLD_AFLPP_WORKAROUND 0x0f000000
+/* FS_OPT_MAX_MAPSIZE is 8388608 = 0x800000 = 2^23 = 1 << 22 */
+#define FS_OPT_MAX_MAPSIZE ((0x00fffffe >> 1) + 1)
+#define FS_OPT_GET_MAPSIZE(x) (((x & 0x00fffffe) >> 1) + 1)
+#define FS_OPT_SET_MAPSIZE(x) (x <= 1 || x > FS_OPT_MAX_MAPSIZE ? 0 : ((x - 1) << 1))
+
+typedef unsigned long long u64;
+
+typedef int8_t  s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#ifndef MIN
+  #define MIN(a, b)           \
+    ({                        \
+                              \
+      __typeof__(a) _a = (a); \
+      __typeof__(b) _b = (b); \
+      _a < _b ? _a : _b;      \
+                              \
+    })
+
+  #define MAX(a, b)           \
+    ({                        \
+                              \
+      __typeof__(a) _a = (a); \
+      __typeof__(b) _b = (b); \
+      _a > _b ? _a : _b;      \
+                              \
+    })
+
+#endif                                                                                                      /* !MIN */
+
+#define SWAP16(_x)                    \
+  ({                                  \
+                                      \
+    u16 _ret = (_x);                  \
+    (u16)((_ret << 8) | (_ret >> 8)); \
+                                      \
+  })
+
+#define SWAP32(_x)                                                                                \
+  ({                                                                                              \
+                                                                                                  \
+    u32 _ret = (_x);                                                                              \
+    (u32)((_ret << 24) | (_ret >> 24) | ((_ret << 8) & 0x00FF0000) | ((_ret >> 8) & 0x0000FF00)); \
+                                                                                                  \
+  })
+
+#define SWAP64(_x)                                                                \
+  ({                                                                              \
+                                                                                  \
+    u64 _ret = (_x);                                                              \
+    _ret = (_ret & 0x00000000FFFFFFFF) << 32 | (_ret & 0xFFFFFFFF00000000) >> 32; \
+    _ret = (_ret & 0x0000FFFF0000FFFF) << 16 | (_ret & 0xFFFF0000FFFF0000) >> 16; \
+    _ret = (_ret & 0x00FF00FF00FF00FF) << 8 | (_ret & 0xFF00FF00FF00FF00) >> 8;   \
+    _ret;                                                                         \
+                                                                                  \
+  })
+
+#ifdef AFL_LLVM_PASS
+  #if defined(__linux__) || !defined(__ANDROID__)
+    #define AFL_SR(s) (srandom(s))
+    #define AFL_R(x) (random() % (x))
+  #else
+    #define AFL_SR(s) ((void)s)
+    #define AFL_R(x) (arc4random_uniform(x))
+  #endif
+#else
+  #if defined(__linux__) || !defined(__ANDROID__)
+    #define SR(s) (srandom(s))
+    #define R(x) (random() % (x))
+  #else
+    #define SR(s) ((void)s)
+    #define R(x) (arc4random_uniform(x))
+  #endif
+#endif                                                                                            /* ^AFL_LLVM_PASS */
+
+#define STRINGIFY_INTERNAL(x) #x
+#define STRINGIFY(x) STRINGIFY_INTERNAL(x)
+
+#define MEM_BARRIER() __asm__ volatile("" ::: "memory")
+
+#if __GNUC__ < 6
+  #ifndef likely
+    #define likely(_x) (_x)
+  #endif
+  #ifndef unlikely
+    #define unlikely(_x) (_x)
+  #endif
+#else
+  #ifndef likely
+    #define likely(_x) __builtin_expect(!!(_x), 1)
+  #endif
+  #ifndef unlikely
+    #define unlikely(_x) __builtin_expect(!!(_x), 0)
+  #endif
+#endif
+
+ #define AFL_NEW_AND_DELETE_FOR(init_type) AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(init_type, AFL_DECL_PARAMS(void), ret) 
+
+/*
+This makro wraps all our afl_ ... _init and _deinit calls with _new and _delete wrappers.
+The _new wrapper allocates memory, and return NULL or the pointer, depending on result.
+The _delete wrapper calls _denit and deallocates the pointer, as created by _new.
+For decl and call, use AFL_DECL/CALL_PARAMS
+*/
+
+
+#define AFL_NEW_AND_DELETE_FOR_WITH_PARAMS(init_type, decl_params, call_params)      \
+  static init_type##_t *init_type##_new(decl_params) {                        \
+                                                                                     \
+    /*printf("Allocating " #init_type " with decl_params " #decl_params */           \
+    /*" and call params " #call_params " and size %ld\n", sizeof(init_type##_t) );*/ \
+    init_type##_t *ret = calloc(1, sizeof(init_type##_t));                           \
+    if (!ret) { return NULL; }                                                       \
+    if (init_type##_init(call_params) != AFL_RET_SUCCESS) {                          \
+                                                                                     \
+      free(ret);                                                                     \
+      return NULL;                                                                   \
+                                                                                     \
+    }                                                                                \
+    return ret;                                                                      \
+                                                                                     \
+  }                                                                                  \
+                                                                                     \
+  static void init_type##_delete(init_type##_t *init_type) {                  \
+                                                                                     \
+    init_type##_deinit(init_type);                                                   \
+    free(init_type);                                                                 \
+                                                                                     \
+  }
+
+
+#define AFL_DECL_PARAMS(...) __VA_ARGS__
+#define AFL_CALL_PARAMS(...) ret, __VA_ARGS__
+
+#endif                                                                                           /* ! _HAVE_TYPES_H */
+
diff -ruN qemu/include/libAFL/xxh3.h qemu_patched/include/libAFL/xxh3.h
--- qemu/include/libAFL/xxh3.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/xxh3.h	2023-10-31 00:30:23.777271921 +0100
@@ -0,0 +1,2959 @@
+/*
+ * xxHash - Extremely Fast Hash algorithm
+ * Development source file for `xxh3`
+ * Copyright (C) 2019-2020 Yann Collet
+ *
+ * BSD 2-Clause License (https://www.opensource.org/licenses/bsd-license.php)
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *    * Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *    * Redistributions in binary form must reproduce the above
+ *      copyright notice, this list of conditions and the following disclaimer
+ *      in the documentation and/or other materials provided with the
+ *      distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * You can contact the author at:
+ *   - xxHash homepage: https://www.xxhash.com
+ *   - xxHash source repository: https://github.com/Cyan4973/xxHash
+ */
+
+/*
+ * Note: This file is separated for development purposes.
+ * It will be integrated into `xxhash.h` when development stage is completed.
+ *
+ * Credit: most of the work on vectorial and asm variants comes from
+ * @easyaspi314
+ */
+
+#ifndef XXH3_H_1397135465
+#define XXH3_H_1397135465
+
+/* ===   Dependencies   === */
+#ifndef XXHASH_H_5627135585666179
+  /* special: when including `xxh3.h` directly, turn on XXH_INLINE_ALL */
+  #undef XXH_INLINE_ALL                                                                       /* avoid redefinition */
+  #define XXH_INLINE_ALL
+#endif
+#include "libAFL/xxhash.h"
+
+/* ===   Compiler specifics   === */
+
+#if defined(__STDC_VERSION__) && __STDC_VERSION__ >= 199901L                                              /* >= C99 */
+  #define XXH_RESTRICT restrict
+#else
+  /* Note: it might be useful to define __restrict or __restrict__ for some C++
+   * compilers */
+  #define XXH_RESTRICT                                                                                   /* disable */
+#endif
+
+#if (defined(__GNUC__) && (__GNUC__ >= 3)) || (defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 800)) || \
+    defined(__clang__)
+  #define XXH_likely(x) __builtin_expect(x, 1)
+  #define XXH_unlikely(x) __builtin_expect(x, 0)
+#else
+  #define XXH_likely(x) (x)
+  #define XXH_unlikely(x) (x)
+#endif
+
+#if defined(__GNUC__)
+  #if defined(__AVX2__)
+    #include <immintrin.h>
+  #elif defined(__SSE2__)
+    #include <emmintrin.h>
+  #elif defined(__ARM_NEON__) || defined(__ARM_NEON)
+    #define inline __inline__                                                                          /* clang bug */
+    #include <arm_neon.h>
+    #undef inline
+  #endif
+#elif defined(_MSC_VER)
+  #include <intrin.h>
+#endif
+
+/*
+ * One goal of XXH3 is to make it fast on both 32-bit and 64-bit, while
+ * remaining a true 64-bit/128-bit hash function.
+ *
+ * This is done by prioritizing a subset of 64-bit operations that can be
+ * emulated without too many steps on the average 32-bit machine.
+ *
+ * For example, these two lines seem similar, and run equally fast on 64-bit:
+ *
+ *   xxh_u64 x;
+ *   x ^= (x >> 47); // good
+ *   x ^= (x >> 13); // bad
+ *
+ * However, to a 32-bit machine, there is a major difference.
+ *
+ * x ^= (x >> 47) looks like this:
+ *
+ *   x.lo ^= (x.hi >> (47 - 32));
+ *
+ * while x ^= (x >> 13) looks like this:
+ *
+ *   // note: funnel shifts are not usually cheap.
+ *   x.lo ^= (x.lo >> 13) | (x.hi << (32 - 13));
+ *   x.hi ^= (x.hi >> 13);
+ *
+ * The first one is significantly faster than the second, simply because the
+ * shift is larger than 32. This means:
+ *  - All the bits we need are in the upper 32 bits, so we can ignore the lower
+ *    32 bits in the shift.
+ *  - The shift result will always fit in the lower 32 bits, and therefore,
+ *    we can ignore the upper 32 bits in the xor.
+ *
+ * Thanks to this optimization, XXH3 only requires these features to be
+ * efficient:
+ *
+ *  - Usable unaligned access
+ *  - A 32-bit or 64-bit ALU
+ *      - If 32-bit, a decent ADC instruction
+ *  - A 32 or 64-bit multiply with a 64-bit result
+ *  - For the 128-bit variant, a decent byteswap helps short inputs.
+ *
+ * The first two are already required by XXH32, and almost all 32-bit and 64-bit
+ * platforms which can run XXH32 can run XXH3 efficiently.
+ *
+ * Thumb-1, the classic 16-bit only subset of ARM's instruction set, is one
+ * notable exception.
+ *
+ * First of all, Thumb-1 lacks support for the UMULL instruction which
+ * performs the important long multiply. This means numerous __aeabi_lmul
+ * calls.
+ *
+ * Second of all, the 8 functional registers are just not enough.
+ * Setup for __aeabi_lmul, byteshift loads, pointers, and all arithmetic need
+ * Lo registers, and this shuffling results in thousands more MOVs than A32.
+ *
+ * A32 and T32 don't have this limitation. They can access all 14 registers,
+ * do a 32->64 multiply with UMULL, and the flexible operand allowing free
+ * shifts is helpful, too.
+ *
+ * Therefore, we do a quick sanity check.
+ *
+ * If compiling Thumb-1 for a target which supports ARM instructions, we will
+ * emit a warning, as it is not a "sane" platform to compile for.
+ *
+ * Usually, if this happens, it is because of an accident and you probably need
+ * to specify -march, as you likely meant to compile for a newer architecture.
+ */
+#if defined(__thumb__) && !defined(__thumb2__) && defined(__ARM_ARCH_ISA_ARM)
+  #warning "XXH3 is highly inefficient without ARM or Thumb-2."
+#endif
+
+/* ==========================================
+ * Vectorization detection
+ * ========================================== */
+#define XXH_SCALAR 0                                                                     /* Portable scalar version */
+#define XXH_SSE2 1                                                             /* SSE2 for Pentium 4 and all x86_64 */
+#define XXH_AVX2 2                                                                /* AVX2 for Haswell and Bulldozer */
+#define XXH_AVX512 3                                                              /* AVX512 for Skylake and Icelake */
+#define XXH_NEON 4                                                         /* NEON for most ARMv7-A and all AArch64 */
+#define XXH_VSX 5                                                                 /* VSX and ZVector for POWER8/z13 */
+
+#ifndef XXH_VECTOR                                                                /* can be defined on command line */
+  #if defined(__AVX512F__)
+    #define XXH_VECTOR XXH_AVX512
+  #elif defined(__AVX2__)
+    #define XXH_VECTOR XXH_AVX2
+  #elif defined(__SSE2__) || defined(_M_AMD64) || defined(_M_X64) || (defined(_M_IX86_FP) && (_M_IX86_FP == 2))
+    #define XXH_VECTOR XXH_SSE2
+  #elif defined(__GNUC__) /* msvc support maybe later */                   \
+      && (defined(__ARM_NEON__) || defined(__ARM_NEON)) &&                 \
+      (defined(__LITTLE_ENDIAN__) /* We only support little endian NEON */ \
+       || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__))
+    #define XXH_VECTOR XXH_NEON
+  #elif (defined(__PPC64__) && defined(__POWER8_VECTOR__)) || \
+      (defined(__s390x__) && defined(__VEC__)) && defined(__GNUC__)                                 /* TODO: IBM XL */
+    #define XXH_VECTOR XXH_VSX
+  #else
+    #define XXH_VECTOR XXH_SCALAR
+  #endif
+#endif
+
+/*
+ * Controls the alignment of the accumulator,
+ * for compatibility with aligned vector loads, which are usually faster.
+ */
+#ifndef XXH_ACC_ALIGN
+  #if defined(XXH_X86DISPATCH)
+    #define XXH_ACC_ALIGN 64                                                       /* for compatibility with avx512 */
+  #elif XXH_VECTOR == XXH_SCALAR                                                                          /* scalar */
+    #define XXH_ACC_ALIGN 8
+  #elif XXH_VECTOR == XXH_SSE2                                                                              /* sse2 */
+    #define XXH_ACC_ALIGN 16
+  #elif XXH_VECTOR == XXH_AVX2                                                                              /* avx2 */
+    #define XXH_ACC_ALIGN 32
+  #elif XXH_VECTOR == XXH_NEON                                                                              /* neon */
+    #define XXH_ACC_ALIGN 16
+  #elif XXH_VECTOR == XXH_VSX                                                                                /* vsx */
+    #define XXH_ACC_ALIGN 16
+  #elif XXH_VECTOR == XXH_AVX512                                                                          /* avx512 */
+    #define XXH_ACC_ALIGN 64
+  #endif
+#endif
+
+#if defined(XXH_X86DISPATCH) || XXH_VECTOR == XXH_SSE2 || XXH_VECTOR == XXH_AVX2 || XXH_VECTOR == XXH_AVX512
+  #define XXH_SEC_ALIGN XXH_ACC_ALIGN
+#else
+  #define XXH_SEC_ALIGN 8
+#endif
+
+/*
+ * UGLY HACK:
+ * GCC usually generates the best code with -O3 for xxHash.
+ *
+ * However, when targeting AVX2, it is overzealous in its unrolling resulting
+ * in code roughly 3/4 the speed of Clang.
+ *
+ * There are other issues, such as GCC splitting _mm256_loadu_si256 into
+ * _mm_loadu_si128 + _mm256_inserti128_si256. This is an optimization which
+ * only applies to Sandy and Ivy Bridge... which don't even support AVX2.
+ *
+ * That is why when compiling the AVX2 version, it is recommended to use either
+ *   -O2 -mavx2 -march=haswell
+ * or
+ *   -O2 -mavx2 -mno-avx256-split-unaligned-load
+ * for decent performance, or to use Clang instead.
+ *
+ * Fortunately, we can control the first one with a pragma that forces GCC into
+ * -O2, but the other one we can't control without "failed to inline always
+ * inline function due to target mismatch" warnings.
+ */
+#if XXH_VECTOR == XXH_AVX2                                  /* AVX2 */           \
+    && defined(__GNUC__) && !defined(__clang__)             /* GCC, not Clang */ \
+    && defined(__OPTIMIZE__) && !defined(__OPTIMIZE_SIZE__)                                  /* respect -O0 and -Os */
+  #pragma GCC push_options
+  #pragma GCC optimize("-O2")
+#endif
+
+#if XXH_VECTOR == XXH_NEON
+  /*
+   * NEON's setup for vmlal_u32 is a little more complicated than it is on
+   * SSE2, AVX2, and VSX.
+   *
+   * While PMULUDQ and VMULEUW both perform a mask, VMLAL.U32 performs an
+   * upcast.
+   *
+   * To do the same operation, the 128-bit 'Q' register needs to be split into
+   * two 64-bit 'D' registers, performing this operation::
+   *
+   *   [                a                 |                 b                ]
+   *            |              '---------. .--------'                |
+   *            |                         x                          |
+   *            |              .---------' '--------.                |
+   *   [ a & 0xFFFFFFFF | b & 0xFFFFFFFF ],[    a >> 32     |     b >> 32    ]
+   *
+   * Due to significant changes in aarch64, the fastest method for aarch64 is
+   * completely different than the fastest method for ARMv7-A.
+   *
+   * ARMv7-A treats D registers as unions overlaying Q registers, so modifying
+   * D11 will modify the high half of Q5. This is similar to how modifying AH
+   * will only affect bits 8-15 of AX on x86.
+   *
+   * VZIP takes two registers, and puts even lanes in one register and odd lanes
+   * in the other.
+   *
+   * On ARMv7-A, this strangely modifies both parameters in place instead of
+   * taking the usual 3-operand form.
+   *
+   * Therefore, if we want to do this, we can simply use a D-form VZIP.32 on the
+   * lower and upper halves of the Q register to end up with the high and low
+   * halves where we want - all in one instruction.
+   *
+   *   vzip.32   d10, d11       @ d10 = { d10[0], d11[0] }; d11 = { d10[1],
+   * d11[1] }
+   *
+   * Unfortunately we need inline assembly for this: Instructions modifying two
+   * registers at once is not possible in GCC or Clang's IR, and they have to
+   * create a copy.
+   *
+   * aarch64 requires a different approach.
+   *
+   * In order to make it easier to write a decent compiler for aarch64, many
+   * quirks were removed, such as conditional execution.
+   *
+   * NEON was also affected by this.
+   *
+   * aarch64 cannot access the high bits of a Q-form register, and writes to a
+   * D-form register zero the high bits, similar to how writes to W-form scalar
+   * registers (or DWORD registers on x86_64) work.
+   *
+   * The formerly free vget_high intrinsics now require a vext (with a few
+   * exceptions)
+   *
+   * Additionally, VZIP was replaced by ZIP1 and ZIP2, which are the equivalent
+   * of PUNPCKL* and PUNPCKH* in SSE, respectively, in order to only modify one
+   * operand.
+   *
+   * The equivalent of the VZIP.32 on the lower and upper halves would be this
+   * mess:
+   *
+   *   ext     v2.4s, v0.4s, v0.4s, #2 // v2 = { v0[2], v0[3], v0[0], v0[1] }
+   *   zip1    v1.2s, v0.2s, v2.2s     // v1 = { v0[0], v2[0] }
+   *   zip2    v0.2s, v0.2s, v1.2s     // v0 = { v0[1], v2[1] }
+   *
+   * Instead, we use a literal downcast, vmovn_u64 (XTN), and vshrn_n_u64
+   * (SHRN):
+   *
+   *   shrn    v1.2s, v0.2d, #32  // v1 = (uint32x2_t)(v0 >> 32);
+   *   xtn     v0.2s, v0.2d       // v0 = (uint32x2_t)(v0 & 0xFFFFFFFF);
+   *
+   * This is available on ARMv7-A, but is less efficient than a single VZIP.32.
+   */
+
+  /*
+   * Function-like macro:
+   * void XXH_SPLIT_IN_PLACE(uint64x2_t &in, uint32x2_t &outLo, uint32x2_t
+   * &outHi)
+   * {
+
+   *     outLo = (uint32x2_t)(in & 0xFFFFFFFF);
+   *     outHi = (uint32x2_t)(in >> 32);
+   *     in = UNDEFINED;
+   * }
+   */
+  #if !defined(XXH_NO_VZIP_HACK) /* define to disable */ \
+      && defined(__GNUC__) && !defined(__aarch64__) && !defined(__arm64__)
+    #define XXH_SPLIT_IN_PLACE(in, outLo, outHi)                                                   \
+      do {                                                                                         \
+                                                                                                   \
+        /* Undocumented GCC/Clang operand modifier: %e0 = lower D half, %f0 =                      \
+         * upper D half */                                                                         \
+        /* https://github.com/gcc-mirror/gcc/blob/38cf91e5/gcc/config/arm/arm.c#L22486             \
+         */                                                                                        \
+        /* https://github.com/llvm-mirror/llvm/blob/2c4ca683/lib/Target/ARM/ARMAsmPrinter.cpp#L399 \
+         */                                                                                        \
+        __asm__("vzip.32  %e0, %f0" : "+w"(in));                                                   \
+        (outLo) = vget_low_u32(vreinterpretq_u32_u64(in));                                         \
+        (outHi) = vget_high_u32(vreinterpretq_u32_u64(in));                                        \
+                                                                                                   \
+      } while (0)
+
+  #else
+    #define XXH_SPLIT_IN_PLACE(in, outLo, outHi) \
+      do {                                       \
+                                                 \
+        (outLo) = vmovn_u64(in);                 \
+        (outHi) = vshrn_n_u64((in), 32);         \
+                                                 \
+      } while (0)
+
+  #endif
+#endif                                                                                    /* XXH_VECTOR == XXH_NEON */
+
+/*
+ * VSX and Z Vector helpers.
+ *
+ * This is very messy, and any pull requests to clean this up are welcome.
+ *
+ * There are a lot of problems with supporting VSX and s390x, due to
+ * inconsistent intrinsics, spotty coverage, and multiple endiannesses.
+ */
+#if XXH_VECTOR == XXH_VSX
+  #if defined(__s390x__)
+    #include <s390intrin.h>
+  #else
+    #include <altivec.h>
+  #endif
+
+  #undef vector                                                                               /* Undo the pollution */
+
+typedef __vector unsigned long long xxh_u64x2;
+typedef __vector unsigned char      xxh_u8x16;
+typedef __vector unsigned           xxh_u32x4;
+
+  #ifndef XXH_VSX_BE
+    #if defined(__BIG_ENDIAN__) || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)
+      #define XXH_VSX_BE 1
+    #elif defined(__VEC_ELEMENT_REG_ORDER__) && __VEC_ELEMENT_REG_ORDER__ == __ORDER_BIG_ENDIAN__
+      #warning "-maltivec=be is not recommended. Please use native endianness."
+      #define XXH_VSX_BE 1
+    #else
+      #define XXH_VSX_BE 0
+    #endif
+  #endif                                                                                    /* !defined(XXH_VSX_BE) */
+
+  #if XXH_VSX_BE
+    /* A wrapper for POWER9's vec_revb. */
+    #if defined(__POWER9_VECTOR__) || (defined(__clang__) && defined(__s390x__))
+      #define XXH_vec_revb vec_revb
+    #else
+XXH_FORCE_INLINE xxh_u64x2 XXH_vec_revb(xxh_u64x2 val) {
+
+  xxh_u8x16 const vByteSwap = {0x07, 0x06, 0x05, 0x04, 0x03, 0x02, 0x01, 0x00,
+                               0x0F, 0x0E, 0x0D, 0x0C, 0x0B, 0x0A, 0x09, 0x08};
+  return vec_perm(val, val, vByteSwap);
+
+}
+
+    #endif
+  #endif                                                                                              /* XXH_VSX_BE */
+
+/*
+ * Performs an unaligned load and byte swaps it on big endian.
+ */
+XXH_FORCE_INLINE xxh_u64x2 XXH_vec_loadu(const void *ptr) {
+
+  xxh_u64x2 ret;
+  memcpy(&ret, ptr, sizeof(xxh_u64x2));
+  #if XXH_VSX_BE
+  ret = XXH_vec_revb(ret);
+  #endif
+  return ret;
+
+}
+
+  /*
+   * vec_mulo and vec_mule are very problematic intrinsics on PowerPC
+   *
+   * These intrinsics weren't added until GCC 8, despite existing for a while,
+   * and they are endian dependent. Also, their meaning swap depending on
+   * version.
+   * */
+  #if defined(__s390x__)
+    /* s390x is always big endian, no issue on this platform */
+    #define XXH_vec_mulo vec_mulo
+    #define XXH_vec_mule vec_mule
+  #elif defined(__clang__) && XXH_HAS_BUILTIN(__builtin_altivec_vmuleuw)
+    /* Clang has a better way to control this, we can just use the builtin which
+     * doesn't swap. */
+    #define XXH_vec_mulo __builtin_altivec_vmulouw
+    #define XXH_vec_mule __builtin_altivec_vmuleuw
+  #else
+/* gcc needs inline assembly */
+/* Adapted from
+ * https://github.com/google/highwayhash/blob/master/highwayhash/hh_vsx.h. */
+XXH_FORCE_INLINE xxh_u64x2 XXH_vec_mulo(xxh_u32x4 a, xxh_u32x4 b) {
+
+  xxh_u64x2 result;
+  __asm__("vmulouw %0, %1, %2" : "=v"(result) : "v"(a), "v"(b));
+  return result;
+
+}
+
+XXH_FORCE_INLINE xxh_u64x2 XXH_vec_mule(xxh_u32x4 a, xxh_u32x4 b) {
+
+  xxh_u64x2 result;
+  __asm__("vmuleuw %0, %1, %2" : "=v"(result) : "v"(a), "v"(b));
+  return result;
+
+}
+
+  #endif                                                                              /* XXH_vec_mulo, XXH_vec_mule */
+#endif                                                                                     /* XXH_VECTOR == XXH_VSX */
+
+/* prefetch
+ * can be disabled, by declaring XXH_NO_PREFETCH build macro */
+#if defined(XXH_NO_PREFETCH)
+  #define XXH_PREFETCH(ptr) (void)(ptr)                                                                 /* disabled */
+#else
+  #if defined(_MSC_VER) && (defined(_M_X64) || defined(_M_I86)) /* _mm_prefetch() is not defined outside of x86/x64 */
+    #include <mmintrin.h>                        /* https://msdn.microsoft.com/fr-fr/library/84szxsww(v=vs.90).aspx */
+    #define XXH_PREFETCH(ptr) _mm_prefetch((const char *)(ptr), _MM_HINT_T0)
+  #elif defined(__GNUC__) && ((__GNUC__ >= 4) || ((__GNUC__ == 3) && (__GNUC_MINOR__ >= 1)))
+    #define XXH_PREFETCH(ptr) __builtin_prefetch((ptr), 0 /* rw==read */, 3 /* locality */)
+  #else
+    #define XXH_PREFETCH(ptr) (void)(ptr)                                                               /* disabled */
+  #endif
+#endif                                                                                           /* XXH_NO_PREFETCH */
+
+/* ==========================================
+ * XXH3 default settings
+ * ========================================== */
+
+#define XXH_SECRET_DEFAULT_SIZE 192                                                 /* minimum XXH3_SECRET_SIZE_MIN */
+
+#if (XXH_SECRET_DEFAULT_SIZE < XXH3_SECRET_SIZE_MIN)
+  #error "default keyset is not large enough"
+#endif
+
+/* Pseudorandom secret taken directly from FARSH */
+XXH_ALIGN(64)
+static const xxh_u8 XXH3_kSecret[XXH_SECRET_DEFAULT_SIZE] = {
+
+    0xb8, 0xfe, 0x6c, 0x39, 0x23, 0xa4, 0x4b, 0xbe, 0x7c, 0x01, 0x81, 0x2c, 0xf7, 0x21, 0xad, 0x1c, 0xde, 0xd4, 0x6d,
+    0xe9, 0x83, 0x90, 0x97, 0xdb, 0x72, 0x40, 0xa4, 0xa4, 0xb7, 0xb3, 0x67, 0x1f, 0xcb, 0x79, 0xe6, 0x4e, 0xcc, 0xc0,
+    0xe5, 0x78, 0x82, 0x5a, 0xd0, 0x7d, 0xcc, 0xff, 0x72, 0x21, 0xb8, 0x08, 0x46, 0x74, 0xf7, 0x43, 0x24, 0x8e, 0xe0,
+    0x35, 0x90, 0xe6, 0x81, 0x3a, 0x26, 0x4c, 0x3c, 0x28, 0x52, 0xbb, 0x91, 0xc3, 0x00, 0xcb, 0x88, 0xd0, 0x65, 0x8b,
+    0x1b, 0x53, 0x2e, 0xa3, 0x71, 0x64, 0x48, 0x97, 0xa2, 0x0d, 0xf9, 0x4e, 0x38, 0x19, 0xef, 0x46, 0xa9, 0xde, 0xac,
+    0xd8, 0xa8, 0xfa, 0x76, 0x3f, 0xe3, 0x9c, 0x34, 0x3f, 0xf9, 0xdc, 0xbb, 0xc7, 0xc7, 0x0b, 0x4f, 0x1d, 0x8a, 0x51,
+    0xe0, 0x4b, 0xcd, 0xb4, 0x59, 0x31, 0xc8, 0x9f, 0x7e, 0xc9, 0xd9, 0x78, 0x73, 0x64,
+
+    0xea, 0xc5, 0xac, 0x83, 0x34, 0xd3, 0xeb, 0xc3, 0xc5, 0x81, 0xa0, 0xff, 0xfa, 0x13, 0x63, 0xeb, 0x17, 0x0d, 0xdd,
+    0x51, 0xb7, 0xf0, 0xda, 0x49, 0xd3, 0x16, 0x55, 0x26, 0x29, 0xd4, 0x68, 0x9e, 0x2b, 0x16, 0xbe, 0x58, 0x7d, 0x47,
+    0xa1, 0xfc, 0x8f, 0xf8, 0xb8, 0xd1, 0x7a, 0xd0, 0x31, 0xce, 0x45, 0xcb, 0x3a, 0x8f, 0x95, 0x16, 0x04, 0x28, 0xaf,
+    0xd7, 0xfb, 0xca, 0xbb, 0x4b, 0x40, 0x7e,
+
+};
+
+#ifdef XXH_OLD_NAMES
+  #define kSecret XXH3_kSecret
+#endif
+
+/*
+ * Calculates a 32-bit to 64-bit long multiply.
+ *
+ * Wraps __emulu on MSVC x86 because it tends to call __allmul when it doesn't
+ * need to (but it shouldn't need to anyways, it is about 7 instructions to do
+ * a 64x64 multiply...). Since we know that this will _always_ emit MULL, we
+ * use that instead of the normal method.
+ *
+ * If you are compiling for platforms like Thumb-1 and don't have a better
+ * option, you may also want to write your own long multiply routine here.
+ *
+ * XXH_FORCE_INLINE xxh_u64 XXH_mult32to64(xxh_u64 x, xxh_u64 y)
+ * {
+
+ *    return (x & 0xFFFFFFFF) * (y & 0xFFFFFFFF);
+ * }
+ */
+#if defined(_MSC_VER) && defined(_M_IX86)
+  #include <intrin.h>
+  #define XXH_mult32to64(x, y) __emulu((unsigned)(x), (unsigned)(y))
+#else
+  /*
+   * Downcast + upcast is usually better than masking on older compilers like
+   * GCC 4.2 (especially 32-bit ones), all without affecting newer compilers.
+   *
+   * The other method, (x & 0xFFFFFFFF) * (y & 0xFFFFFFFF), will AND both
+   * operands and perform a full 64x64 multiply -- entirely redundant on 32-bit.
+   */
+  #define XXH_mult32to64(x, y) ((xxh_u64)(xxh_u32)(x) * (xxh_u64)(xxh_u32)(y))
+#endif
+
+/*
+ * Calculates a 64->128-bit long multiply.
+ *
+ * Uses __uint128_t and _umul128 if available, otherwise uses a scalar version.
+ */
+static XXH128_hash_t XXH_mult64to128(xxh_u64 lhs, xxh_u64 rhs) {
+
+  /*
+   * GCC/Clang __uint128_t method.
+   *
+   * On most 64-bit targets, GCC and Clang define a __uint128_t type.
+   * This is usually the best way as it usually uses a native long 64-bit
+   * multiply, such as MULQ on x86_64 or MUL + UMULH on aarch64.
+   *
+   * Usually.
+   *
+   * Despite being a 32-bit platform, Clang (and emscripten) define this type
+   * despite not having the arithmetic for it. This results in a laggy
+   * compiler builtin call which calculates a full 128-bit multiply.
+   * In that case it is best to use the portable one.
+   * https://github.com/Cyan4973/xxHash/issues/211#issuecomment-515575677
+   */
+#if defined(__GNUC__) && !defined(__wasm__) && defined(__SIZEOF_INT128__) || \
+    (defined(_INTEGRAL_MAX_BITS) && _INTEGRAL_MAX_BITS >= 128)
+
+  __uint128_t const product = (__uint128_t)lhs * (__uint128_t)rhs;
+  XXH128_hash_t     r128;
+  r128.low64 = (xxh_u64)(product);
+  r128.high64 = (xxh_u64)(product >> 64);
+  return r128;
+
+    /*
+     * MSVC for x64's _umul128 method.
+     *
+     * xxh_u64 _umul128(xxh_u64 Multiplier, xxh_u64 Multiplicand, xxh_u64
+     * *HighProduct);
+     *
+     * This compiles to single operand MUL on x64.
+     */
+#elif defined(_M_X64) || defined(_M_IA64)
+
+  #ifndef _MSC_VER
+    #pragma intrinsic(_umul128)
+  #endif
+  xxh_u64       product_high;
+  xxh_u64 const product_low = _umul128(lhs, rhs, &product_high);
+  XXH128_hash_t r128;
+  r128.low64 = product_low;
+  r128.high64 = product_high;
+  return r128;
+
+#else
+  /*
+   * Portable scalar method. Optimized for 32-bit and 64-bit ALUs.
+   *
+   * This is a fast and simple grade school multiply, which is shown below
+   * with base 10 arithmetic instead of base 0x100000000.
+   *
+   *           9 3 // D2 lhs = 93
+   *         x 7 5 // D2 rhs = 75
+   *     ----------
+   *           1 5 // D2 lo_lo = (93 % 10) * (75 % 10) = 15
+   *         4 5 | // D2 hi_lo = (93 / 10) * (75 % 10) = 45
+   *         2 1 | // D2 lo_hi = (93 % 10) * (75 / 10) = 21
+   *     + 6 3 | | // D2 hi_hi = (93 / 10) * (75 / 10) = 63
+   *     ---------
+   *         2 7 | // D2 cross = (15 / 10) + (45 % 10) + 21 = 27
+   *     + 6 7 | | // D2 upper = (27 / 10) + (45 / 10) + 63 = 67
+   *     ---------
+   *       6 9 7 5 // D4 res = (27 * 10) + (15 % 10) + (67 * 100) = 6975
+   *
+   * The reasons for adding the products like this are:
+   *  1. It avoids manual carry tracking. Just like how
+   *     (9 * 9) + 9 + 9 = 99, the same applies with this for UINT64_MAX.
+   *     This avoids a lot of complexity.
+   *
+   *  2. It hints for, and on Clang, compiles to, the powerful UMAAL
+   *     instruction available in ARM's Digital Signal Processing extension
+   *     in 32-bit ARMv6 and later, which is shown below:
+   *
+   *         void UMAAL(xxh_u32 *RdLo, xxh_u32 *RdHi, xxh_u32 Rn, xxh_u32 Rm)
+   *         {
+
+   *             xxh_u64 product = (xxh_u64)*RdLo * (xxh_u64)*RdHi + Rn + Rm;
+   *             *RdLo = (xxh_u32)(product & 0xFFFFFFFF);
+   *             *RdHi = (xxh_u32)(product >> 32);
+   *         }
+   *
+   *     This instruction was designed for efficient long multiplication, and
+   *     allows this to be calculated in only 4 instructions at speeds
+   *     comparable to some 64-bit ALUs.
+   *
+   *  3. It isn't terrible on other platforms. Usually this will be a couple
+   *     of 32-bit ADD/ADCs.
+   */
+
+  /* First calculate all of the cross products. */
+  xxh_u64 const lo_lo = XXH_mult32to64(lhs & 0xFFFFFFFF, rhs & 0xFFFFFFFF);
+  xxh_u64 const hi_lo = XXH_mult32to64(lhs >> 32, rhs & 0xFFFFFFFF);
+  xxh_u64 const lo_hi = XXH_mult32to64(lhs & 0xFFFFFFFF, rhs >> 32);
+  xxh_u64 const hi_hi = XXH_mult32to64(lhs >> 32, rhs >> 32);
+
+  /* Now add the products together. These will never overflow. */
+  xxh_u64 const cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;
+  xxh_u64 const upper = (hi_lo >> 32) + (cross >> 32) + hi_hi;
+  xxh_u64 const lower = (cross << 32) | (lo_lo & 0xFFFFFFFF);
+
+  XXH128_hash_t r128;
+  r128.low64 = lower;
+  r128.high64 = upper;
+  return r128;
+#endif
+
+}
+
+/*
+ * Does a 64-bit to 128-bit multiply, then XOR folds it.
+ *
+ * The reason for the separate function is to prevent passing too many structs
+ * around by value. This will hopefully inline the multiply, but we don't force
+ * it.
+ */
+static xxh_u64 XXH3_mul128_fold64(xxh_u64 lhs, xxh_u64 rhs) {
+
+  XXH128_hash_t product = XXH_mult64to128(lhs, rhs);
+  return product.low64 ^ product.high64;
+
+}
+
+/* Seems to produce slightly better code on GCC for some reason. */
+XXH_FORCE_INLINE xxh_u64 XXH_xorshift64(xxh_u64 v64, int shift) {
+
+  XXH_ASSERT(0 <= shift && shift < 64);
+  return v64 ^ (v64 >> shift);
+
+}
+
+/*
+ * We don't need to (or want to) mix as much as XXH64.
+ *
+ * Short hashes are more evenly distributed, so it isn't necessary.
+ */
+static XXH64_hash_t XXH3_avalanche(xxh_u64 h64) {
+
+  h64 = XXH_xorshift64(h64, 37);
+  h64 *= 0x165667919E3779F9ULL;
+  h64 = XXH_xorshift64(h64, 32);
+  return h64;
+
+}
+
+/* ==========================================
+ * Short keys
+ * ==========================================
+ * One of the shortcomings of XXH32 and XXH64 was that their performance was
+ * sub-optimal on short lengths. It used an iterative algorithm which strongly
+ * favored lengths that were a multiple of 4 or 8.
+ *
+ * Instead of iterating over individual inputs, we use a set of single shot
+ * functions which piece together a range of lengths and operate in constant
+ * time.
+ *
+ * Additionally, the number of multiplies has been significantly reduced. This
+ * reduces latency, especially when emulating 64-bit multiplies on 32-bit.
+ *
+ * Depending on the platform, this may or may not be faster than XXH32, but it
+ * is almost guaranteed to be faster than XXH64.
+ */
+
+/*
+ * At very short lengths, there isn't enough input to fully hide secrets, or use
+ * the entire secret.
+ *
+ * There is also only a limited amount of mixing we can do before significantly
+ * impacting performance.
+ *
+ * Therefore, we use different sections of the secret and always mix two secret
+ * samples with an XOR. This should have no effect on performance on the
+ * seedless or withSeed variants because everything _should_ be constant folded
+ * by modern compilers.
+ *
+ * The XOR mixing hides individual parts of the secret and increases entropy.
+ *
+ * This adds an extra layer of strength for custom secrets.
+ */
+XXH_FORCE_INLINE XXH64_hash_t XXH3_len_1to3_64b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                XXH64_hash_t seed) {
+
+  XXH_ASSERT(input != NULL);
+  XXH_ASSERT(1 <= len && len <= 3);
+  XXH_ASSERT(secret != NULL);
+  /*
+   * len = 1: combined = { input[0], 0x01, input[0], input[0] }
+   * len = 2: combined = { input[1], 0x02, input[0], input[1] }
+   * len = 3: combined = { input[2], 0x03, input[0], input[1] }
+   */
+  {
+
+    xxh_u8 const  c1 = input[0];
+    xxh_u8 const  c2 = input[len >> 1];
+    xxh_u8 const  c3 = input[len - 1];
+    xxh_u32 const combined = ((xxh_u32)c1 << 16) | ((xxh_u32)c2 << 24) | ((xxh_u32)c3 << 0) | ((xxh_u32)len << 8);
+    xxh_u64 const bitflip = (XXH_readLE32(secret) ^ XXH_readLE32(secret + 4)) + seed;
+    xxh_u64 const keyed = (xxh_u64)combined ^ bitflip;
+    xxh_u64 const mixed = keyed * XXH_PRIME64_1;
+    return XXH3_avalanche(mixed);
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH64_hash_t XXH3_len_4to8_64b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                XXH64_hash_t seed) {
+
+  XXH_ASSERT(input != NULL);
+  XXH_ASSERT(secret != NULL);
+  XXH_ASSERT(4 <= len && len < 8);
+  seed ^= (xxh_u64)XXH_swap32((xxh_u32)seed) << 32;
+  {
+
+    xxh_u32 const input1 = XXH_readLE32(input);
+    xxh_u32 const input2 = XXH_readLE32(input + len - 4);
+    xxh_u64 const bitflip = (XXH_readLE64(secret + 8) ^ XXH_readLE64(secret + 16)) - seed;
+    xxh_u64 const input64 = input2 + (((xxh_u64)input1) << 32);
+    xxh_u64       x = input64 ^ bitflip;
+    /* this mix is inspired by Pelle Evensen's rrmxmx */
+    x ^= XXH_rotl64(x, 49) ^ XXH_rotl64(x, 24);
+    x *= 0x9FB21C651E98DF25ULL;
+    x ^= (x >> 35) + len;
+    x *= 0x9FB21C651E98DF25ULL;
+    return XXH_xorshift64(x, 28);
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH64_hash_t XXH3_len_9to16_64b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                 XXH64_hash_t seed) {
+
+  XXH_ASSERT(input != NULL);
+  XXH_ASSERT(secret != NULL);
+  XXH_ASSERT(8 <= len && len <= 16);
+  {
+
+    xxh_u64 const bitflip1 = (XXH_readLE64(secret + 24) ^ XXH_readLE64(secret + 32)) + seed;
+    xxh_u64 const bitflip2 = (XXH_readLE64(secret + 40) ^ XXH_readLE64(secret + 48)) - seed;
+    xxh_u64 const input_lo = XXH_readLE64(input) ^ bitflip1;
+    xxh_u64 const input_hi = XXH_readLE64(input + len - 8) ^ bitflip2;
+    xxh_u64 const acc = len + XXH_swap64(input_lo) + input_hi + XXH3_mul128_fold64(input_lo, input_hi);
+    return XXH3_avalanche(acc);
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH64_hash_t XXH3_len_0to16_64b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                 XXH64_hash_t seed) {
+
+  XXH_ASSERT(len <= 16);
+  {
+
+    if (XXH_likely(len > 8)) return XXH3_len_9to16_64b(input, len, secret, seed);
+    if (XXH_likely(len >= 4)) return XXH3_len_4to8_64b(input, len, secret, seed);
+    if (len) return XXH3_len_1to3_64b(input, len, secret, seed);
+    return XXH3_avalanche((XXH_PRIME64_1 + seed) ^ (XXH_readLE64(secret + 56) ^ XXH_readLE64(secret + 64)));
+
+  }
+
+}
+
+/*
+ * DISCLAIMER: There are known *seed-dependent* multicollisions here due to
+ * multiplication by zero, affecting hashes of lengths 17 to 240.
+ *
+ * However, they are very unlikely.
+ *
+ * Keep this in mind when using the unseeded XXH3_64bits() variant: As with all
+ * unseeded non-cryptographic hashes, it does not attempt to defend itself
+ * against specially crafted inputs, only random inputs.
+ *
+ * Compared to classic UMAC where a 1 in 2^31 chance of 4 consecutive bytes
+ * cancelling out the secret is taken an arbitrary number of times (addressed
+ * in XXH3_accumulate_512), this collision is very unlikely with random inputs
+ * and/or proper seeding:
+ *
+ * This only has a 1 in 2^63 chance of 8 consecutive bytes cancelling out, in a
+ * function that is only called up to 16 times per hash with up to 240 bytes of
+ * input.
+ *
+ * This is not too bad for a non-cryptographic hash function, especially with
+ * only 64 bit outputs.
+ *
+ * The 128-bit variant (which trades some speed for strength) is NOT affected
+ * by this, although it is always a good idea to use a proper seed if you care
+ * about strength.
+ */
+XXH_FORCE_INLINE xxh_u64 XXH3_mix16B(const xxh_u8 *XXH_RESTRICT input, const xxh_u8 *XXH_RESTRICT secret,
+                                     xxh_u64 seed64) {
+
+#if defined(__GNUC__) && !defined(__clang__)  /* GCC, not Clang */ \
+    && defined(__i386__) && defined(__SSE2__) /* x86 + SSE2 */     \
+    && !defined(XXH_ENABLE_AUTOVECTORIZE)                                      /* Define to disable like XXH32 hack */
+  /*
+   * UGLY HACK:
+   * GCC for x86 tends to autovectorize the 128-bit multiply, resulting in
+   * slower code.
+   *
+   * By forcing seed64 into a register, we disrupt the cost model and
+   * cause it to scalarize. See `XXH32_round()`
+   *
+   * FIXME: Clang's output is still _much_ faster -- On an AMD Ryzen 3600,
+   * XXH3_64bits @ len=240 runs at 4.6 GB/s with Clang 9, but 3.3 GB/s on
+   * GCC 9.2, despite both emitting scalar code.
+   *
+   * GCC generates much better scalar code than Clang for the rest of XXH3,
+   * which is why finding a more optimal codepath is an interest.
+   */
+  __asm__("" : "+r"(seed64));
+#endif
+  {
+
+    xxh_u64 const input_lo = XXH_readLE64(input);
+    xxh_u64 const input_hi = XXH_readLE64(input + 8);
+    return XXH3_mul128_fold64(input_lo ^ (XXH_readLE64(secret) + seed64),
+                              input_hi ^ (XXH_readLE64(secret + 8) - seed64));
+
+  }
+
+}
+
+/* For mid range keys, XXH3 uses a Mum-hash variant. */
+XXH_FORCE_INLINE XXH64_hash_t XXH3_len_17to128_64b(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                   const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                   XXH64_hash_t seed) {
+
+  XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);
+  (void)secretSize;
+  XXH_ASSERT(16 < len && len <= 128);
+
+  {
+
+    xxh_u64 acc = len * XXH_PRIME64_1;
+    if (len > 32) {
+
+      if (len > 64) {
+
+        if (len > 96) {
+
+          acc += XXH3_mix16B(input + 48, secret + 96, seed);
+          acc += XXH3_mix16B(input + len - 64, secret + 112, seed);
+
+        }
+
+        acc += XXH3_mix16B(input + 32, secret + 64, seed);
+        acc += XXH3_mix16B(input + len - 48, secret + 80, seed);
+
+      }
+
+      acc += XXH3_mix16B(input + 16, secret + 32, seed);
+      acc += XXH3_mix16B(input + len - 32, secret + 48, seed);
+
+    }
+
+    acc += XXH3_mix16B(input + 0, secret + 0, seed);
+    acc += XXH3_mix16B(input + len - 16, secret + 16, seed);
+
+    return XXH3_avalanche(acc);
+
+  }
+
+}
+
+#define XXH3_MIDSIZE_MAX 240
+
+XXH_NO_INLINE XXH64_hash_t XXH3_len_129to240_64b(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                 const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                 XXH64_hash_t seed) {
+
+  XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);
+  (void)secretSize;
+  XXH_ASSERT(128 < len && len <= XXH3_MIDSIZE_MAX);
+
+#define XXH3_MIDSIZE_STARTOFFSET 3
+#define XXH3_MIDSIZE_LASTOFFSET 17
+
+  {
+
+    xxh_u64   acc = len * XXH_PRIME64_1;
+    int const nbRounds = (int)len / 16;
+    int       i;
+    for (i = 0; i < 8; i++) {
+
+      acc += XXH3_mix16B(input + (16 * i), secret + (16 * i), seed);
+
+    }
+
+    acc = XXH3_avalanche(acc);
+    XXH_ASSERT(nbRounds >= 8);
+#if defined(__clang__)                                /* Clang */ \
+    && (defined(__ARM_NEON) || defined(__ARM_NEON__)) /* NEON */  \
+    && !defined(XXH_ENABLE_AUTOVECTORIZE)                                                      /* Define to disable */
+  /*
+   * UGLY HACK:
+   * Clang for ARMv7-A tries to vectorize this loop, similar to GCC x86.
+   * In everywhere else, it uses scalar code.
+   *
+   * For 64->128-bit multiplies, even if the NEON was 100% optimal, it
+   * would still be slower than UMAAL (see XXH_mult64to128).
+   *
+   * Unfortunately, Clang doesn't handle the long multiplies properly and
+   * converts them to the nonexistent "vmulq_u64" intrinsic, which is then
+   * scalarized into an ugly mess of VMOV.32 instructions.
+   *
+   * This mess is difficult to avoid without turning autovectorization
+   * off completely, but they are usually relatively minor and/or not
+   * worth it to fix.
+   *
+   * This loop is the easiest to fix, as unlike XXH32, this pragma
+   * _actually works_ because it is a loop vectorization instead of an
+   * SLP vectorization.
+   */
+  #pragma clang loop vectorize(disable)
+#endif
+    for (i = 8; i < nbRounds; i++) {
+
+      acc += XXH3_mix16B(input + (16 * i), secret + (16 * (i - 8)) + XXH3_MIDSIZE_STARTOFFSET, seed);
+
+    }
+
+    /* last bytes */
+    acc += XXH3_mix16B(input + len - 16, secret + XXH3_SECRET_SIZE_MIN - XXH3_MIDSIZE_LASTOFFSET, seed);
+    return XXH3_avalanche(acc);
+
+  }
+
+}
+
+/* =======     Long Keys     ======= */
+
+#define XXH_STRIPE_LEN 64
+#define XXH_SECRET_CONSUME_RATE 8                               /* nb of secret bytes consumed at each accumulation */
+#define XXH_ACC_NB (XXH_STRIPE_LEN / sizeof(xxh_u64))
+
+#ifdef XXH_OLD_NAMES
+  #define STRIPE_LEN XXH_STRIPE_LEN
+  #define ACC_NB XXH_ACC_NB
+#endif
+
+typedef enum { XXH3_acc_64bits, XXH3_acc_128bits } XXH3_accWidth_e;
+
+XXH_FORCE_INLINE void XXH_writeLE64(void *dst, xxh_u64 v64) {
+
+  if (!XXH_CPU_LITTLE_ENDIAN) v64 = XXH_swap64(v64);
+  memcpy(dst, &v64, sizeof(v64));
+
+}
+
+/* Several intrinsic functions below are supposed to accept __int64 as argument,
+ * as documented in
+ * https://software.intel.com/sites/landingpage/IntrinsicsGuide/ . However,
+ * several environments do not define __int64 type, requiring a workaround.
+ */
+#if !defined(__VMS) && (defined(__cplusplus) || (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 \
+                                                                                                             */))
+typedef int64_t xxh_i64;
+#else
+/* the following type must have a width of 64-bit */
+typedef long long xxh_i64;
+#endif
+
+/*
+ * XXH3_accumulate_512 is the tightest loop for long inputs, and it is the most
+ * optimized.
+ *
+ * It is a hardened version of UMAC, based off of FARSH's implementation.
+ *
+ * This was chosen because it adapts quite well to 32-bit, 64-bit, and SIMD
+ * implementations, and it is ridiculously fast.
+ *
+ * We harden it by mixing the original input to the accumulators as well as the
+ * product.
+ *
+ * This means that in the (relatively likely) case of a multiply by zero, the
+ * original input is preserved.
+ *
+ * On 128-bit inputs, we swap 64-bit pairs when we add the input to improve
+ * cross-pollination, as otherwise the upper and lower halves would be
+ * essentially independent.
+ *
+ * This doesn't matter on 64-bit hashes since they all get merged together in
+ * the end, so we skip the extra step.
+ *
+ * Both XXH3_64bits and XXH3_128bits use this subroutine.
+ */
+
+#if (XXH_VECTOR == XXH_AVX512) || defined(XXH_X86DISPATCH)
+
+  #ifndef XXH_TARGET_AVX512
+    #define XXH_TARGET_AVX512                                                           /* disable attribute target */
+  #endif
+
+XXH_FORCE_INLINE XXH_TARGET_AVX512 void XXH3_accumulate_512_avx512(void *XXH_RESTRICT       acc,
+                                                                   const void *XXH_RESTRICT input,
+                                                                   const void *XXH_RESTRICT secret,
+                                                                   XXH3_accWidth_e          accWidth) {
+
+  XXH_ALIGN(64) __m512i *const xacc = (__m512i *)acc;
+  XXH_ASSERT((((size_t)acc) & 63) == 0);
+  XXH_STATIC_ASSERT(XXH_STRIPE_LEN == sizeof(__m512i));
+
+  {
+
+    /* data_vec    = input[0]; */
+    __m512i const data_vec = _mm512_loadu_si512(input);
+    /* key_vec     = secret[0]; */
+    __m512i const key_vec = _mm512_loadu_si512(secret);
+    /* data_key    = data_vec ^ key_vec; */
+    __m512i const data_key = _mm512_xor_si512(data_vec, key_vec);
+    /* data_key_lo = data_key >> 32; */
+    __m512i const data_key_lo = _mm512_shuffle_epi32(data_key, _MM_SHUFFLE(0, 3, 0, 1));
+    /* product     = (data_key & 0xffffffff) * (data_key_lo & 0xffffffff); */
+    __m512i const product = _mm512_mul_epu32(data_key, data_key_lo);
+    if (accWidth == XXH3_acc_128bits) {
+
+      /* xacc[0] += swap(data_vec); */
+      __m512i const data_swap = _mm512_shuffle_epi32(data_vec, _MM_SHUFFLE(1, 0, 3, 2));
+      __m512i const sum = _mm512_add_epi64(*xacc, data_swap);
+      /* xacc[0] += product; */
+      *xacc = _mm512_add_epi64(product, sum);
+
+    } else {                                                                                     /* XXH3_acc_64bits */
+
+      /* xacc[0] += data_vec; */
+      __m512i const sum = _mm512_add_epi64(*xacc, data_vec);
+      /* xacc[0] += product; */
+      *xacc = _mm512_add_epi64(product, sum);
+
+    }
+
+  }
+
+}
+
+/*
+ * XXH3_scrambleAcc: Scrambles the accumulators to improve mixing.
+ *
+ * Multiplication isn't perfect, as explained by Google in HighwayHash:
+ *
+ *  // Multiplication mixes/scrambles bytes 0-7 of the 64-bit result to
+ *  // varying degrees. In descending order of goodness, bytes
+ *  // 3 4 2 5 1 6 0 7 have quality 228 224 164 160 100 96 36 32.
+ *  // As expected, the upper and lower bytes are much worse.
+ *
+ * Source:
+ * https://github.com/google/highwayhash/blob/0aaf66b/highwayhash/hh_avx2.h#L291
+ *
+ * Since our algorithm uses a pseudorandom secret to add some variance into the
+ * mix, we don't need to (or want to) mix as often or as much as HighwayHash
+ * does.
+ *
+ * This isn't as tight as XXH3_accumulate, but still written in SIMD to avoid
+ * extraction.
+ *
+ * Both XXH3_64bits and XXH3_128bits use this subroutine.
+ */
+
+XXH_FORCE_INLINE XXH_TARGET_AVX512 void XXH3_scrambleAcc_avx512(void *XXH_RESTRICT       acc,
+                                                                const void *XXH_RESTRICT secret) {
+
+  XXH_ASSERT((((size_t)acc) & 63) == 0);
+  XXH_STATIC_ASSERT(XXH_STRIPE_LEN == sizeof(__m512i));
+  {
+
+    XXH_ALIGN(64) __m512i *const xacc = (__m512i *)acc;
+    const __m512i                prime32 = _mm512_set1_epi32((int)XXH_PRIME32_1);
+
+    /* xacc[0] ^= (xacc[0] >> 47) */
+    __m512i const acc_vec = *xacc;
+    __m512i const shifted = _mm512_srli_epi64(acc_vec, 47);
+    __m512i const data_vec = _mm512_xor_si512(acc_vec, shifted);
+    /* xacc[0] ^= secret; */
+    __m512i const key_vec = _mm512_loadu_si512(secret);
+    __m512i const data_key = _mm512_xor_si512(data_vec, key_vec);
+
+    /* xacc[0] *= XXH_PRIME32_1; */
+    __m512i const data_key_hi = _mm512_shuffle_epi32(data_key, _MM_SHUFFLE(0, 3, 0, 1));
+    __m512i const prod_lo = _mm512_mul_epu32(data_key, prime32);
+    __m512i const prod_hi = _mm512_mul_epu32(data_key_hi, prime32);
+    *xacc = _mm512_add_epi64(prod_lo, _mm512_slli_epi64(prod_hi, 32));
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH_TARGET_AVX512 void XXH3_initCustomSecret_avx512(void *XXH_RESTRICT customSecret, xxh_u64 seed64) {
+
+  XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 63) == 0);
+  XXH_STATIC_ASSERT(XXH_SEC_ALIGN == 64);
+  XXH_ASSERT(((size_t)customSecret & 63) == 0);
+  (void)(&XXH_writeLE64);
+  {
+
+    int const     nbRounds = XXH_SECRET_DEFAULT_SIZE / sizeof(__m512i);
+    __m512i const seed = _mm512_mask_set1_epi64(_mm512_set1_epi64((xxh_i64)seed64), 0xAA, -(xxh_i64)seed64);
+
+    XXH_ALIGN(64) const __m512i *const src = (const __m512i *)XXH3_kSecret;
+    XXH_ALIGN(64) __m512i *const       dest = (__m512i *)customSecret;
+    int                                i;
+    for (i = 0; i < nbRounds; ++i) {
+
+      // GCC has a bug, _mm512_stream_load_si512 accepts 'void*', not 'void
+      // const*', this will warn "discards âconstâ qualifier".
+      union {
+
+        XXH_ALIGN(64) const __m512i *const cp;
+        XXH_ALIGN(64) void *const p;
+
+      } const remote_const_void = {.cp = src + i};
+
+      dest[i] = _mm512_add_epi64(_mm512_stream_load_si512(remote_const_void.p), seed);
+
+    }
+
+  }
+
+}
+
+#endif
+
+#if (XXH_VECTOR == XXH_AVX2) || defined(XXH_X86DISPATCH)
+
+  #ifndef XXH_TARGET_AVX2
+    #define XXH_TARGET_AVX2                                                             /* disable attribute target */
+  #endif
+
+XXH_FORCE_INLINE XXH_TARGET_AVX2 void XXH3_accumulate_512_avx2(void *XXH_RESTRICT acc, const void *XXH_RESTRICT input,
+                                                               const void *XXH_RESTRICT secret,
+                                                               XXH3_accWidth_e          accWidth) {
+
+  XXH_ASSERT((((size_t)acc) & 31) == 0);
+  {
+
+    XXH_ALIGN(32) __m256i *const xacc = (__m256i *)acc;
+    /* Unaligned. This is mainly for pointer arithmetic, and because
+     * _mm256_loadu_si256 requires  a const __m256i * pointer for some reason.
+     */
+    const __m256i *const xinput = (const __m256i *)input;
+    /* Unaligned. This is mainly for pointer arithmetic, and because
+     * _mm256_loadu_si256 requires a const __m256i * pointer for some reason. */
+    const __m256i *const xsecret = (const __m256i *)secret;
+
+    size_t i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(__m256i); i++) {
+
+      /* data_vec    = xinput[i]; */
+      __m256i const data_vec = _mm256_loadu_si256(xinput + i);
+      /* key_vec     = xsecret[i]; */
+      __m256i const key_vec = _mm256_loadu_si256(xsecret + i);
+      /* data_key    = data_vec ^ key_vec; */
+      __m256i const data_key = _mm256_xor_si256(data_vec, key_vec);
+      /* data_key_lo = data_key >> 32; */
+      __m256i const data_key_lo = _mm256_shuffle_epi32(data_key, _MM_SHUFFLE(0, 3, 0, 1));
+      /* product     = (data_key & 0xffffffff) * (data_key_lo & 0xffffffff); */
+      __m256i const product = _mm256_mul_epu32(data_key, data_key_lo);
+      if (accWidth == XXH3_acc_128bits) {
+
+        /* xacc[i] += swap(data_vec); */
+        __m256i const data_swap = _mm256_shuffle_epi32(data_vec, _MM_SHUFFLE(1, 0, 3, 2));
+        __m256i const sum = _mm256_add_epi64(xacc[i], data_swap);
+        /* xacc[i] += product; */
+        xacc[i] = _mm256_add_epi64(product, sum);
+
+      } else {                                                                                   /* XXH3_acc_64bits */
+
+        /* xacc[i] += data_vec; */
+        __m256i const sum = _mm256_add_epi64(xacc[i], data_vec);
+        /* xacc[i] += product; */
+        xacc[i] = _mm256_add_epi64(product, sum);
+
+      }
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH_TARGET_AVX2 void XXH3_scrambleAcc_avx2(void *XXH_RESTRICT acc, const void *XXH_RESTRICT secret) {
+
+  XXH_ASSERT((((size_t)acc) & 31) == 0);
+  {
+
+    XXH_ALIGN(32) __m256i *const xacc = (__m256i *)acc;
+    /* Unaligned. This is mainly for pointer arithmetic, and because
+     * _mm256_loadu_si256 requires a const __m256i * pointer for some reason. */
+    const __m256i *const xsecret = (const __m256i *)secret;
+    const __m256i        prime32 = _mm256_set1_epi32((int)XXH_PRIME32_1);
+
+    size_t i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(__m256i); i++) {
+
+      /* xacc[i] ^= (xacc[i] >> 47) */
+      __m256i const acc_vec = xacc[i];
+      __m256i const shifted = _mm256_srli_epi64(acc_vec, 47);
+      __m256i const data_vec = _mm256_xor_si256(acc_vec, shifted);
+      /* xacc[i] ^= xsecret; */
+      __m256i const key_vec = _mm256_loadu_si256(xsecret + i);
+      __m256i const data_key = _mm256_xor_si256(data_vec, key_vec);
+
+      /* xacc[i] *= XXH_PRIME32_1; */
+      __m256i const data_key_hi = _mm256_shuffle_epi32(data_key, _MM_SHUFFLE(0, 3, 0, 1));
+      __m256i const prod_lo = _mm256_mul_epu32(data_key, prime32);
+      __m256i const prod_hi = _mm256_mul_epu32(data_key_hi, prime32);
+      xacc[i] = _mm256_add_epi64(prod_lo, _mm256_slli_epi64(prod_hi, 32));
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH_TARGET_AVX2 void XXH3_initCustomSecret_avx2(void *XXH_RESTRICT customSecret, xxh_u64 seed64) {
+
+  XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 31) == 0);
+  XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE / sizeof(__m256i)) == 6);
+  XXH_STATIC_ASSERT(XXH_SEC_ALIGN <= 64);
+  (void)(&XXH_writeLE64);
+  XXH_PREFETCH(customSecret);
+  {
+
+    __m256i const seed = _mm256_set_epi64x(-(xxh_i64)seed64, (xxh_i64)seed64, -(xxh_i64)seed64, (xxh_i64)seed64);
+
+    XXH_ALIGN(64) const __m256i *const src = (const __m256i *)XXH3_kSecret;
+    XXH_ALIGN(64) __m256i *            dest = (__m256i *)customSecret;
+
+  #if defined(__GNUC__) || defined(__clang__)
+    /*
+     * On GCC & Clang, marking 'dest' as modified will cause the compiler:
+     *   - do not extract the secret from sse registers in the internal loop
+     *   - use less common registers, and avoid pushing these reg into stack
+     * The asm hack causes Clang to assume that XXH3_kSecretPtr aliases with
+     * customSecret, and on aarch64, this prevented LDP from merging two
+     * loads together for free. Putting the loads together before the stores
+     * properly generates LDP.
+     */
+    __asm__("" : "+r"(dest));
+  #endif
+
+    /* GCC -O2 need unroll loop manually */
+    dest[0] = _mm256_add_epi64(_mm256_stream_load_si256(src + 0), seed);
+    dest[1] = _mm256_add_epi64(_mm256_stream_load_si256(src + 1), seed);
+    dest[2] = _mm256_add_epi64(_mm256_stream_load_si256(src + 2), seed);
+    dest[3] = _mm256_add_epi64(_mm256_stream_load_si256(src + 3), seed);
+    dest[4] = _mm256_add_epi64(_mm256_stream_load_si256(src + 4), seed);
+    dest[5] = _mm256_add_epi64(_mm256_stream_load_si256(src + 5), seed);
+
+  }
+
+}
+
+#endif
+
+#if (XXH_VECTOR == XXH_SSE2) || defined(XXH_X86DISPATCH)
+
+  #ifndef XXH_TARGET_SSE2
+    #define XXH_TARGET_SSE2                                                             /* disable attribute target */
+  #endif
+
+XXH_FORCE_INLINE XXH_TARGET_SSE2 void XXH3_accumulate_512_sse2(void *XXH_RESTRICT acc, const void *XXH_RESTRICT input,
+                                                               const void *XXH_RESTRICT secret,
+                                                               XXH3_accWidth_e          accWidth) {
+
+  /* SSE2 is just a half-scale version of the AVX2 version. */
+  XXH_ASSERT((((size_t)acc) & 15) == 0);
+  {
+
+    XXH_ALIGN(16) __m128i *const xacc = (__m128i *)acc;
+    /* Unaligned. This is mainly for pointer arithmetic, and because
+     * _mm_loadu_si128 requires a const __m128i * pointer for some reason. */
+    const __m128i *const xinput = (const __m128i *)input;
+    /* Unaligned. This is mainly for pointer arithmetic, and because
+     * _mm_loadu_si128 requires a const __m128i * pointer for some reason. */
+    const __m128i *const xsecret = (const __m128i *)secret;
+
+    size_t i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(__m128i); i++) {
+
+      /* data_vec    = xinput[i]; */
+      __m128i const data_vec = _mm_loadu_si128(xinput + i);
+      /* key_vec     = xsecret[i]; */
+      __m128i const key_vec = _mm_loadu_si128(xsecret + i);
+      /* data_key    = data_vec ^ key_vec; */
+      __m128i const data_key = _mm_xor_si128(data_vec, key_vec);
+      /* data_key_lo = data_key >> 32; */
+      __m128i const data_key_lo = _mm_shuffle_epi32(data_key, _MM_SHUFFLE(0, 3, 0, 1));
+      /* product     = (data_key & 0xffffffff) * (data_key_lo & 0xffffffff); */
+      __m128i const product = _mm_mul_epu32(data_key, data_key_lo);
+      if (accWidth == XXH3_acc_128bits) {
+
+        /* xacc[i] += swap(data_vec); */
+        __m128i const data_swap = _mm_shuffle_epi32(data_vec, _MM_SHUFFLE(1, 0, 3, 2));
+        __m128i const sum = _mm_add_epi64(xacc[i], data_swap);
+        /* xacc[i] += product; */
+        xacc[i] = _mm_add_epi64(product, sum);
+
+      } else {                                                                                   /* XXH3_acc_64bits */
+
+        /* xacc[i] += data_vec; */
+        __m128i const sum = _mm_add_epi64(xacc[i], data_vec);
+        /* xacc[i] += product; */
+        xacc[i] = _mm_add_epi64(product, sum);
+
+      }
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH_TARGET_SSE2 void XXH3_scrambleAcc_sse2(void *XXH_RESTRICT acc, const void *XXH_RESTRICT secret) {
+
+  XXH_ASSERT((((size_t)acc) & 15) == 0);
+  {
+
+    XXH_ALIGN(16) __m128i *const xacc = (__m128i *)acc;
+    /* Unaligned. This is mainly for pointer arithmetic, and because
+     * _mm_loadu_si128 requires a const __m128i * pointer for some reason. */
+    const __m128i *const xsecret = (const __m128i *)secret;
+    const __m128i        prime32 = _mm_set1_epi32((int)XXH_PRIME32_1);
+
+    size_t i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(__m128i); i++) {
+
+      /* xacc[i] ^= (xacc[i] >> 47) */
+      __m128i const acc_vec = xacc[i];
+      __m128i const shifted = _mm_srli_epi64(acc_vec, 47);
+      __m128i const data_vec = _mm_xor_si128(acc_vec, shifted);
+      /* xacc[i] ^= xsecret[i]; */
+      __m128i const key_vec = _mm_loadu_si128(xsecret + i);
+      __m128i const data_key = _mm_xor_si128(data_vec, key_vec);
+
+      /* xacc[i] *= XXH_PRIME32_1; */
+      __m128i const data_key_hi = _mm_shuffle_epi32(data_key, _MM_SHUFFLE(0, 3, 0, 1));
+      __m128i const prod_lo = _mm_mul_epu32(data_key, prime32);
+      __m128i const prod_hi = _mm_mul_epu32(data_key_hi, prime32);
+      xacc[i] = _mm_add_epi64(prod_lo, _mm_slli_epi64(prod_hi, 32));
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH_TARGET_SSE2 void XXH3_initCustomSecret_sse2(void *XXH_RESTRICT customSecret, xxh_u64 seed64) {
+
+  XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 15) == 0);
+  (void)(&XXH_writeLE64);
+  {
+
+    int const nbRounds = XXH_SECRET_DEFAULT_SIZE / sizeof(__m128i);
+
+  #if defined(_MSC_VER) && defined(_M_IX86) && _MSC_VER < 1900
+    // MSVC 32bit mode does not support _mm_set_epi64x before 2015
+    XXH_ALIGN(16)
+    const xxh_i64 seed64x2[2] = {(xxh_i64)seed64, -(xxh_i64)seed64};
+    __m128i const seed = _mm_load_si128((__m128i const *)seed64x2);
+  #else
+    __m128i const seed = _mm_set_epi64x(-(xxh_i64)seed64, (xxh_i64)seed64);
+  #endif
+    int i;
+
+    XXH_ALIGN(64) const float *const  src = (float const *)XXH3_kSecret;
+    XXH_ALIGN(XXH_SEC_ALIGN) __m128i *dest = (__m128i *)customSecret;
+  #if defined(__GNUC__) || defined(__clang__)
+    /*
+     * On GCC & Clang, marking 'dest' as modified will cause the compiler:
+     *   - do not extract the secret from sse registers in the internal loop
+     *   - use less common registers, and avoid pushing these reg into stack
+     */
+    __asm__("" : "+r"(dest));
+  #endif
+
+    for (i = 0; i < nbRounds; ++i) {
+
+      dest[i] = _mm_add_epi64(_mm_castps_si128(_mm_load_ps(src + i * 4)), seed);
+
+    }
+
+  }
+
+}
+
+#endif
+
+#if (XXH_VECTOR == XXH_NEON)
+
+XXH_FORCE_INLINE void XXH3_accumulate_512_neon(void *XXH_RESTRICT acc, const void *XXH_RESTRICT input,
+                                               const void *XXH_RESTRICT secret, XXH3_accWidth_e accWidth) {
+
+  XXH_ASSERT((((size_t)acc) & 15) == 0);
+  {
+
+    XXH_ALIGN(16) uint64x2_t *const xacc = (uint64x2_t *)acc;
+    /* We don't use a uint32x4_t pointer because it causes bus errors on ARMv7.
+     */
+    uint8_t const *const xinput = (const uint8_t *)input;
+    uint8_t const *const xsecret = (const uint8_t *)secret;
+
+    size_t i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(uint64x2_t); i++) {
+
+      /* data_vec = xinput[i]; */
+      uint8x16_t data_vec = vld1q_u8(xinput + (i * 16));
+      /* key_vec  = xsecret[i];  */
+      uint8x16_t key_vec = vld1q_u8(xsecret + (i * 16));
+      uint64x2_t data_key;
+      uint32x2_t data_key_lo, data_key_hi;
+      if (accWidth == XXH3_acc_64bits) {
+
+        /* xacc[i] += data_vec; */
+        xacc[i] = vaddq_u64(xacc[i], vreinterpretq_u64_u8(data_vec));
+
+      } else {                                                                                  /* XXH3_acc_128bits */
+
+        /* xacc[i] += swap(data_vec); */
+        uint64x2_t const data64 = vreinterpretq_u64_u8(data_vec);
+        uint64x2_t const swapped = vextq_u64(data64, data64, 1);
+        xacc[i] = vaddq_u64(xacc[i], swapped);
+
+      }
+
+      /* data_key = data_vec ^ key_vec; */
+      data_key = vreinterpretq_u64_u8(veorq_u8(data_vec, key_vec));
+      /* data_key_lo = (uint32x2_t) (data_key & 0xFFFFFFFF);
+       * data_key_hi = (uint32x2_t) (data_key >> 32);
+       * data_key = UNDEFINED; */
+      XXH_SPLIT_IN_PLACE(data_key, data_key_lo, data_key_hi);
+      /* xacc[i] += (uint64x2_t) data_key_lo * (uint64x2_t) data_key_hi; */
+      xacc[i] = vmlal_u32(xacc[i], data_key_lo, data_key_hi);
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE void XXH3_scrambleAcc_neon(void *XXH_RESTRICT acc, const void *XXH_RESTRICT secret) {
+
+  XXH_ASSERT((((size_t)acc) & 15) == 0);
+
+  {
+
+    uint64x2_t *   xacc = (uint64x2_t *)acc;
+    uint8_t const *xsecret = (uint8_t const *)secret;
+    uint32x2_t     prime = vdup_n_u32(XXH_PRIME32_1);
+
+    size_t i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(uint64x2_t); i++) {
+
+      /* xacc[i] ^= (xacc[i] >> 47); */
+      uint64x2_t acc_vec = xacc[i];
+      uint64x2_t shifted = vshrq_n_u64(acc_vec, 47);
+      uint64x2_t data_vec = veorq_u64(acc_vec, shifted);
+
+      /* xacc[i] ^= xsecret[i]; */
+      uint8x16_t key_vec = vld1q_u8(xsecret + (i * 16));
+      uint64x2_t data_key = veorq_u64(data_vec, vreinterpretq_u64_u8(key_vec));
+
+      /* xacc[i] *= XXH_PRIME32_1 */
+      uint32x2_t data_key_lo, data_key_hi;
+      /* data_key_lo = (uint32x2_t) (xacc[i] & 0xFFFFFFFF);
+       * data_key_hi = (uint32x2_t) (xacc[i] >> 32);
+       * xacc[i] = UNDEFINED; */
+      XXH_SPLIT_IN_PLACE(data_key, data_key_lo, data_key_hi);
+      { /*
+         * prod_hi = (data_key >> 32) * XXH_PRIME32_1;
+         *
+         * Avoid vmul_u32 + vshll_n_u32 since Clang 6 and 7 will
+         * incorrectly "optimize" this:
+         *   tmp     = vmul_u32(vmovn_u64(a), vmovn_u64(b));
+         *   shifted = vshll_n_u32(tmp, 32);
+         * to this:
+         *   tmp     = "vmulq_u64"(a, b); // no such thing!
+         *   shifted = vshlq_n_u64(tmp, 32);
+         *
+         * However, unlike SSE, Clang lacks a 64-bit multiply routine
+         * for NEON, and it scalarizes two 64-bit multiplies instead.
+         *
+         * vmull_u32 has the same timing as vmul_u32, and it avoids
+         * this bug completely.
+         * See https://bugs.llvm.org/show_bug.cgi?id=39967
+         */
+        uint64x2_t prod_hi = vmull_u32(data_key_hi, prime);
+        /* xacc[i] = prod_hi << 32; */
+        xacc[i] = vshlq_n_u64(prod_hi, 32);
+        /* xacc[i] += (prod_hi & 0xFFFFFFFF) * XXH_PRIME32_1; */
+        xacc[i] = vmlal_u32(xacc[i], data_key_lo, prime);
+
+      }
+
+    }
+
+  }
+
+}
+
+#endif
+
+#if (XXH_VECTOR == XXH_VSX)
+
+XXH_FORCE_INLINE void XXH3_accumulate_512_vsx(void *XXH_RESTRICT acc, const void *XXH_RESTRICT input,
+                                              const void *XXH_RESTRICT secret, XXH3_accWidth_e accWidth) {
+
+  xxh_u64x2 *const       xacc = (xxh_u64x2 *)acc;                                               /* presumed aligned */
+  xxh_u64x2 const *const xinput = (xxh_u64x2 const *)input;                             /* no alignment restriction */
+  xxh_u64x2 const *const xsecret = (xxh_u64x2 const *)secret;                           /* no alignment restriction */
+  xxh_u64x2 const        v32 = {32, 32};
+  size_t                 i;
+  for (i = 0; i < XXH_STRIPE_LEN / sizeof(xxh_u64x2); i++) {
+
+    /* data_vec = xinput[i]; */
+    xxh_u64x2 const data_vec = XXH_vec_loadu(xinput + i);
+    /* key_vec = xsecret[i]; */
+    xxh_u64x2 const key_vec = XXH_vec_loadu(xsecret + i);
+    xxh_u64x2 const data_key = data_vec ^ key_vec;
+    /* shuffled = (data_key << 32) | (data_key >> 32); */
+    xxh_u32x4 const shuffled = (xxh_u32x4)vec_rl(data_key, v32);
+    /* product = ((xxh_u64x2)data_key & 0xFFFFFFFF) * ((xxh_u64x2)shuffled &
+     * 0xFFFFFFFF); */
+    xxh_u64x2 const product = XXH_vec_mulo((xxh_u32x4)data_key, shuffled);
+    xacc[i] += product;
+
+    if (accWidth == XXH3_acc_64bits) {
+
+      xacc[i] += data_vec;
+
+    } else {                                                                                    /* XXH3_acc_128bits */
+
+        /* swap high and low halves */
+  #ifdef __s390x__
+      xxh_u64x2 const data_swapped = vec_permi(data_vec, data_vec, 2);
+  #else
+      xxh_u64x2 const data_swapped = vec_xxpermdi(data_vec, data_vec, 2);
+  #endif
+      xacc[i] += data_swapped;
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE void XXH3_scrambleAcc_vsx(void *XXH_RESTRICT acc, const void *XXH_RESTRICT secret) {
+
+  XXH_ASSERT((((size_t)acc) & 15) == 0);
+
+  {
+
+    xxh_u64x2 *const       xacc = (xxh_u64x2 *)acc;
+    const xxh_u64x2 *const xsecret = (const xxh_u64x2 *)secret;
+    /* constants */
+    xxh_u64x2 const v32 = {32, 32};
+    xxh_u64x2 const v47 = {47, 47};
+    xxh_u32x4 const prime = {XXH_PRIME32_1, XXH_PRIME32_1, XXH_PRIME32_1, XXH_PRIME32_1};
+    size_t          i;
+    for (i = 0; i < XXH_STRIPE_LEN / sizeof(xxh_u64x2); i++) {
+
+      /* xacc[i] ^= (xacc[i] >> 47); */
+      xxh_u64x2 const acc_vec = xacc[i];
+      xxh_u64x2 const data_vec = acc_vec ^ (acc_vec >> v47);
+
+      /* xacc[i] ^= xsecret[i]; */
+      xxh_u64x2 const key_vec = XXH_vec_loadu(xsecret + i);
+      xxh_u64x2 const data_key = data_vec ^ key_vec;
+
+      /* xacc[i] *= XXH_PRIME32_1 */
+      /* prod_lo = ((xxh_u64x2)data_key & 0xFFFFFFFF) * ((xxh_u64x2)prime &
+       * 0xFFFFFFFF);  */
+      xxh_u64x2 const prod_even = XXH_vec_mule((xxh_u32x4)data_key, prime);
+      /* prod_hi = ((xxh_u64x2)data_key >> 32) * ((xxh_u64x2)prime >> 32);  */
+      xxh_u64x2 const prod_odd = XXH_vec_mulo((xxh_u32x4)data_key, prime);
+      xacc[i] = prod_odd + (prod_even << v32);
+
+    }
+
+  }
+
+}
+
+#endif
+
+/* scalar variants - universal */
+
+XXH_FORCE_INLINE void XXH3_accumulate_512_scalar(void *XXH_RESTRICT acc, const void *XXH_RESTRICT input,
+                                                 const void *XXH_RESTRICT secret, XXH3_accWidth_e accWidth) {
+
+  XXH_ALIGN(XXH_ACC_ALIGN)
+  xxh_u64 *const      xacc = (xxh_u64 *)acc;                                                    /* presumed aligned */
+  const xxh_u8 *const xinput = (const xxh_u8 *)input;                                   /* no alignment restriction */
+  const xxh_u8 *const xsecret = (const xxh_u8 *)secret;                                 /* no alignment restriction */
+  size_t              i;
+  XXH_ASSERT(((size_t)acc & (XXH_ACC_ALIGN - 1)) == 0);
+  for (i = 0; i < XXH_ACC_NB; i++) {
+
+    xxh_u64 const data_val = XXH_readLE64(xinput + 8 * i);
+    xxh_u64 const data_key = data_val ^ XXH_readLE64(xsecret + i * 8);
+
+    if (accWidth == XXH3_acc_64bits) {
+
+      xacc[i] += data_val;
+
+    } else {
+
+      xacc[i ^ 1] += data_val;                                                               /* swap adjacent lanes */
+
+    }
+
+    xacc[i] += XXH_mult32to64(data_key & 0xFFFFFFFF, data_key >> 32);
+
+  }
+
+}
+
+XXH_FORCE_INLINE void XXH3_scrambleAcc_scalar(void *XXH_RESTRICT acc, const void *XXH_RESTRICT secret) {
+
+  XXH_ALIGN(XXH_ACC_ALIGN)
+  xxh_u64 *const      xacc = (xxh_u64 *)acc;                                                    /* presumed aligned */
+  const xxh_u8 *const xsecret = (const xxh_u8 *)secret;                                 /* no alignment restriction */
+  size_t              i;
+  XXH_ASSERT((((size_t)acc) & (XXH_ACC_ALIGN - 1)) == 0);
+  for (i = 0; i < XXH_ACC_NB; i++) {
+
+    xxh_u64 const key64 = XXH_readLE64(xsecret + 8 * i);
+    xxh_u64       acc64 = xacc[i];
+    acc64 = XXH_xorshift64(acc64, 47);
+    acc64 ^= key64;
+    acc64 *= XXH_PRIME32_1;
+    xacc[i] = acc64;
+
+  }
+
+}
+
+XXH_FORCE_INLINE void XXH3_initCustomSecret_scalar(void *XXH_RESTRICT customSecret, xxh_u64 seed64) {
+
+  /*
+   * We need a separate pointer for the hack below,
+   * which requires a non-const pointer.
+   * Any decent compiler will optimize this out otherwise.
+   */
+  const xxh_u8 *kSecretPtr = XXH3_kSecret;
+  XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 15) == 0);
+
+#if defined(__clang__) && defined(__aarch64__)
+  /*
+   * UGLY HACK:
+   * Clang generates a bunch of MOV/MOVK pairs for aarch64, and they are
+   * placed sequentially, in order, at the top of the unrolled loop.
+   *
+   * While MOVK is great for generating constants (2 cycles for a 64-bit
+   * constant compared to 4 cycles for LDR), long MOVK chains stall the
+   * integer pipelines:
+   *   I   L   S
+   * MOVK
+   * MOVK
+   * MOVK
+   * MOVK
+   * ADD
+   * SUB      STR
+   *          STR
+   * By forcing loads from memory (as the asm line causes Clang to assume
+   * that XXH3_kSecretPtr has been changed), the pipelines are used more
+   * efficiently:
+   *   I   L   S
+   *      LDR
+   *  ADD LDR
+   *  SUB     STR
+   *          STR
+   * XXH3_64bits_withSeed, len == 256, Snapdragon 835
+   *   without hack: 2654.4 MB/s
+   *   with hack:    3202.9 MB/s
+   */
+  __asm__("" : "+r"(kSecretPtr));
+#endif
+  /*
+   * Note: in debug mode, this overrides the asm optimization
+   * and Clang will emit MOVK chains again.
+   */
+  XXH_ASSERT(kSecretPtr == XXH3_kSecret);
+
+  {
+
+    int const nbRounds = XXH_SECRET_DEFAULT_SIZE / 16;
+    int       i;
+    for (i = 0; i < nbRounds; i++) {
+
+      /*
+       * The asm hack causes Clang to assume that kSecretPtr aliases with
+       * customSecret, and on aarch64, this prevented LDP from merging two
+       * loads together for free. Putting the loads together before the stores
+       * properly generates LDP.
+       */
+      xxh_u64 lo = XXH_readLE64(kSecretPtr + 16 * i) + seed64;
+      xxh_u64 hi = XXH_readLE64(kSecretPtr + 16 * i + 8) - seed64;
+      XXH_writeLE64((xxh_u8 *)customSecret + 16 * i, lo);
+      XXH_writeLE64((xxh_u8 *)customSecret + 16 * i + 8, hi);
+
+    }
+
+  }
+
+}
+
+typedef void (*XXH3_f_accumulate_512)(void *XXH_RESTRICT, const void *, const void *, XXH3_accWidth_e);
+typedef void (*XXH3_f_scrambleAcc)(void *XXH_RESTRICT, const void *);
+typedef void (*XXH3_f_initCustomSecret)(void *XXH_RESTRICT, xxh_u64);
+
+#if (XXH_VECTOR == XXH_AVX512)
+
+  #define XXH3_accumulate_512 XXH3_accumulate_512_avx512
+  #define XXH3_scrambleAcc XXH3_scrambleAcc_avx512
+  #define XXH3_initCustomSecret XXH3_initCustomSecret_avx512
+
+#elif (XXH_VECTOR == XXH_AVX2)
+
+  #define XXH3_accumulate_512 XXH3_accumulate_512_avx2
+  #define XXH3_scrambleAcc XXH3_scrambleAcc_avx2
+  #define XXH3_initCustomSecret XXH3_initCustomSecret_avx2
+
+#elif (XXH_VECTOR == XXH_SSE2)
+
+  #define XXH3_accumulate_512 XXH3_accumulate_512_sse2
+  #define XXH3_scrambleAcc XXH3_scrambleAcc_sse2
+  #define XXH3_initCustomSecret XXH3_initCustomSecret_sse2
+
+#elif (XXH_VECTOR == XXH_NEON)
+
+  #define XXH3_accumulate_512 XXH3_accumulate_512_neon
+  #define XXH3_scrambleAcc XXH3_scrambleAcc_neon
+  #define XXH3_initCustomSecret XXH3_initCustomSecret_scalar
+
+#elif (XXH_VECTOR == XXH_VSX)
+
+  #define XXH3_accumulate_512 XXH3_accumulate_512_vsx
+  #define XXH3_scrambleAcc XXH3_scrambleAcc_vsx
+  #define XXH3_initCustomSecret XXH3_initCustomSecret_scalar
+
+#else                                                                                                     /* scalar */
+
+  #define XXH3_accumulate_512 XXH3_accumulate_512_scalar
+  #define XXH3_scrambleAcc XXH3_scrambleAcc_scalar
+  #define XXH3_initCustomSecret XXH3_initCustomSecret_scalar
+
+#endif
+
+#ifndef XXH_PREFETCH_DIST
+  #ifdef __clang__
+    #define XXH_PREFETCH_DIST 320
+  #else
+    #if (XXH_VECTOR == XXH_AVX512)
+      #define XXH_PREFETCH_DIST 512
+    #else
+      #define XXH_PREFETCH_DIST 384
+    #endif
+  #endif                                                                                               /* __clang__ */
+#endif                                                                                         /* XXH_PREFETCH_DIST */
+
+/*
+ * XXH3_accumulate()
+ * Loops over XXH3_accumulate_512().
+ * Assumption: nbStripes will not overflow the secret size
+ */
+XXH_FORCE_INLINE void XXH3_accumulate(xxh_u64 *XXH_RESTRICT acc, const xxh_u8 *XXH_RESTRICT input,
+                                      const xxh_u8 *XXH_RESTRICT secret, size_t nbStripes, XXH3_accWidth_e accWidth,
+                                      XXH3_f_accumulate_512 f_acc512) {
+
+  size_t n;
+  for (n = 0; n < nbStripes; n++) {
+
+    const xxh_u8 *const in = input + n * XXH_STRIPE_LEN;
+    XXH_PREFETCH(in + XXH_PREFETCH_DIST);
+    f_acc512(acc, in, secret + n * XXH_SECRET_CONSUME_RATE, accWidth);
+
+  }
+
+}
+
+XXH_FORCE_INLINE void XXH3_hashLong_internal_loop(xxh_u64 *XXH_RESTRICT acc, const xxh_u8 *XXH_RESTRICT input,
+                                                  size_t len, const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                  XXH3_accWidth_e accWidth, XXH3_f_accumulate_512 f_acc512,
+                                                  XXH3_f_scrambleAcc f_scramble) {
+
+  size_t const nb_rounds = (secretSize - XXH_STRIPE_LEN) / XXH_SECRET_CONSUME_RATE;
+  size_t const block_len = XXH_STRIPE_LEN * nb_rounds;
+  size_t const nb_blocks = len / block_len;
+
+  size_t n;
+
+  XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);
+
+  for (n = 0; n < nb_blocks; n++) {
+
+    XXH3_accumulate(acc, input + n * block_len, secret, nb_rounds, accWidth, f_acc512);
+    f_scramble(acc, secret + secretSize - XXH_STRIPE_LEN);
+
+  }
+
+  /* last partial block */
+  XXH_ASSERT(len > XXH_STRIPE_LEN);
+  {
+
+    size_t const nbStripes = (len - (block_len * nb_blocks)) / XXH_STRIPE_LEN;
+    XXH_ASSERT(nbStripes <= (secretSize / XXH_SECRET_CONSUME_RATE));
+    XXH3_accumulate(acc, input + nb_blocks * block_len, secret, nbStripes, accWidth, f_acc512);
+
+    /* last stripe */
+    if (len & (XXH_STRIPE_LEN - 1)) {
+
+      const xxh_u8 *const p = input + len - XXH_STRIPE_LEN;
+      /* Do not align on 8, so that the secret is different from the scrambler
+       */
+#define XXH_SECRET_LASTACC_START 7
+      f_acc512(acc, p, secret + secretSize - XXH_STRIPE_LEN - XXH_SECRET_LASTACC_START, accWidth);
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE xxh_u64 XXH3_mix2Accs(const xxh_u64 *XXH_RESTRICT acc, const xxh_u8 *XXH_RESTRICT secret) {
+
+  return XXH3_mul128_fold64(acc[0] ^ XXH_readLE64(secret), acc[1] ^ XXH_readLE64(secret + 8));
+
+}
+
+static XXH64_hash_t XXH3_mergeAccs(const xxh_u64 *XXH_RESTRICT acc, const xxh_u8 *XXH_RESTRICT secret, xxh_u64 start) {
+
+  xxh_u64 result64 = start;
+  size_t  i = 0;
+
+  for (i = 0; i < 4; i++) {
+
+    result64 += XXH3_mix2Accs(acc + 2 * i, secret + 16 * i);
+#if defined(__clang__)                                /* Clang */ \
+    && (defined(__arm__) || defined(__thumb__))       /* ARMv7 */ \
+    && (defined(__ARM_NEON) || defined(__ARM_NEON__)) /* NEON */  \
+    && !defined(XXH_ENABLE_AUTOVECTORIZE)                                                      /* Define to disable */
+    /*
+     * UGLY HACK:
+     * Prevent autovectorization on Clang ARMv7-a. Exact same problem as
+     * the one in XXH3_len_129to240_64b. Speeds up shorter keys > 240b.
+     * XXH3_64bits, len == 256, Snapdragon 835:
+     *   without hack: 2063.7 MB/s
+     *   with hack:    2560.7 MB/s
+     */
+    __asm__("" : "+r"(result64));
+#endif
+
+  }
+
+  return XXH3_avalanche(result64);
+
+}
+
+#define XXH3_INIT_ACC                                                                                        \
+  {                                                                                                          \
+                                                                                                             \
+    XXH_PRIME32_3, XXH_PRIME64_1, XXH_PRIME64_2, XXH_PRIME64_3, XXH_PRIME64_4, XXH_PRIME32_2, XXH_PRIME64_5, \
+        XXH_PRIME32_1                                                                                        \
+                                                                                                             \
+  }
+
+XXH_FORCE_INLINE XXH64_hash_t XXH3_hashLong_64b_internal(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                         const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                         XXH3_f_accumulate_512 f_acc512,
+                                                         XXH3_f_scrambleAcc    f_scramble) {
+
+  XXH_ALIGN(XXH_ACC_ALIGN) xxh_u64 acc[XXH_ACC_NB] = XXH3_INIT_ACC;
+
+  XXH3_hashLong_internal_loop(acc, input, len, secret, secretSize, XXH3_acc_64bits, f_acc512, f_scramble);
+
+  /* converge into final hash */
+  XXH_STATIC_ASSERT(sizeof(acc) == 64);
+  /* do not align on 8, so that the secret is different from the accumulator */
+#define XXH_SECRET_MERGEACCS_START 11
+  XXH_ASSERT(secretSize >= sizeof(acc) + XXH_SECRET_MERGEACCS_START);
+  return XXH3_mergeAccs(acc, secret + XXH_SECRET_MERGEACCS_START, (xxh_u64)len * XXH_PRIME64_1);
+
+}
+
+/*
+ * It's important for performance that XXH3_hashLong is not inlined.
+ */
+XXH_NO_INLINE XXH64_hash_t XXH3_hashLong_64b_withSecret(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                        XXH64_hash_t seed64, const xxh_u8 *XXH_RESTRICT secret,
+                                                        size_t secretLen) {
+
+  (void)seed64;
+  return XXH3_hashLong_64b_internal(input, len, secret, secretLen, XXH3_accumulate_512, XXH3_scrambleAcc);
+
+}
+
+/*
+ * XXH3_hashLong_64b_withSeed():
+ * Generate a custom key based on alteration of default XXH3_kSecret with the
+ * seed, and then use this key for long mode hashing.
+ *
+ * This operation is decently fast but nonetheless costs a little bit of time.
+ * Try to avoid it whenever possible (typically when seed==0).
+ *
+ * It's important for performance that XXH3_hashLong is not inlined. Not sure
+ * why (uop cache maybe?), but the difference is large and easily measurable.
+ */
+XXH_FORCE_INLINE XXH64_hash_t XXH3_hashLong_64b_withSeed_internal(const xxh_u8 *input, size_t len, XXH64_hash_t seed,
+                                                                  XXH3_f_accumulate_512   f_acc512,
+                                                                  XXH3_f_scrambleAcc      f_scramble,
+                                                                  XXH3_f_initCustomSecret f_initSec) {
+
+  if (seed == 0)
+    return XXH3_hashLong_64b_internal(input, len, XXH3_kSecret, sizeof(XXH3_kSecret), f_acc512, f_scramble);
+  {
+
+    XXH_ALIGN(XXH_SEC_ALIGN) xxh_u8 secret[XXH_SECRET_DEFAULT_SIZE];
+    f_initSec(secret, seed);
+    return XXH3_hashLong_64b_internal(input, len, secret, sizeof(secret), f_acc512, f_scramble);
+
+  }
+
+}
+
+/*
+ * It's important for performance that XXH3_hashLong is not inlined.
+ */
+XXH_NO_INLINE XXH64_hash_t XXH3_hashLong_64b_withSeed(const xxh_u8 *input, size_t len, XXH64_hash_t seed,
+                                                      const xxh_u8 *secret, size_t secretLen) {
+
+  (void)secret;
+  (void)secretLen;
+  return XXH3_hashLong_64b_withSeed_internal(input, len, seed, XXH3_accumulate_512, XXH3_scrambleAcc,
+                                             XXH3_initCustomSecret);
+
+}
+
+typedef XXH64_hash_t (*XXH3_hashLong64_f)(const xxh_u8 *XXH_RESTRICT, size_t, XXH64_hash_t, const xxh_u8 *XXH_RESTRICT,
+                                          size_t);
+
+XXH_FORCE_INLINE XXH64_hash_t XXH3_64bits_internal(const void *XXH_RESTRICT input, size_t len, XXH64_hash_t seed64,
+                                                   const void *XXH_RESTRICT secret, size_t secretLen,
+                                                   XXH3_hashLong64_f f_hashLong) {
+
+  XXH_ASSERT(secretLen >= XXH3_SECRET_SIZE_MIN);
+  /*
+   * If an action is to be taken if `secretLen` condition is not respected,
+   * it should be done here.
+   * For now, it's a contract pre-condition.
+   * Adding a check and a branch here would cost performance at every hash.
+   * Also, note that function signature doesn't offer room to return an error.
+   */
+  if (len <= 16) return XXH3_len_0to16_64b((const xxh_u8 *)input, len, (const xxh_u8 *)secret, seed64);
+  if (len <= 128) return XXH3_len_17to128_64b((const xxh_u8 *)input, len, (const xxh_u8 *)secret, secretLen, seed64);
+  if (len <= XXH3_MIDSIZE_MAX)
+    return XXH3_len_129to240_64b((const xxh_u8 *)input, len, (const xxh_u8 *)secret, secretLen, seed64);
+  return f_hashLong((const xxh_u8 *)input, len, seed64, (const xxh_u8 *)secret, secretLen);
+
+}
+
+/* ===   Public entry point   === */
+
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits(const void *input, size_t len) {
+
+  return XXH3_64bits_internal(input, len, 0, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_hashLong_64b_withSecret);
+
+}
+
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits_withSecret(const void *input, size_t len, const void *secret,
+                                                   size_t secretSize) {
+
+  return XXH3_64bits_internal(input, len, 0, secret, secretSize, XXH3_hashLong_64b_withSecret);
+
+}
+
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits_withSeed(const void *input, size_t len, XXH64_hash_t seed) {
+
+  return XXH3_64bits_internal(input, len, seed, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_hashLong_64b_withSeed);
+
+}
+
+/* ===   XXH3 streaming   === */
+
+/*
+ * Malloc's a pointer that is always aligned to align.
+ *
+ * This must be freed with `XXH_alignedFree()`.
+ *
+ * malloc typically guarantees 16 byte alignment on 64-bit systems and 8 byte
+ * alignment on 32-bit. This isn't enough for the 32 byte aligned loads in AVX2
+ * or on 32-bit, the 16 byte aligned loads in SSE2 and NEON.
+ *
+ * This underalignment previously caused a rather obvious crash which went
+ * completely unnoticed due to XXH3_createState() not actually being tested.
+ * Credit to RedSpah for noticing this bug.
+ *
+ * The alignment is done manually: Functions like posix_memalign or _mm_malloc
+ * are avoided: To maintain portability, we would have to write a fallback
+ * like this anyways, and besides, testing for the existence of library
+ * functions without relying on external build tools is impossible.
+ *
+ * The method is simple: Overallocate, manually align, and store the offset
+ * to the original behind the returned pointer.
+ *
+ * Align must be a power of 2 and 8 <= align <= 128.
+ */
+static void *XXH_alignedMalloc(size_t s, size_t align) {
+
+  XXH_ASSERT(align <= 128 && align >= 8);                                                            /* range check */
+  XXH_ASSERT((align & (align - 1)) == 0);                                                             /* power of 2 */
+  XXH_ASSERT(s != 0 && s < (s + align));                                                          /* empty/overflow */
+  {                                          /* Overallocate to make room for manual realignment and an offset byte */
+    xxh_u8 *base = (xxh_u8 *)XXH_malloc(s + align);
+    if (base != NULL) {
+
+      /*
+       * Get the offset needed to align this pointer.
+       *
+       * Even if the returned pointer is aligned, there will always be
+       * at least one byte to store the offset to the original pointer.
+       */
+      size_t offset = align - ((size_t)base & (align - 1));                                         /* base % align */
+      /* Add the offset for the now-aligned pointer */
+      xxh_u8 *ptr = base + offset;
+
+      XXH_ASSERT((size_t)ptr % align == 0);
+
+      /* Store the offset immediately before the returned pointer. */
+      ptr[-1] = (xxh_u8)offset;
+      return ptr;
+
+    }
+
+    return NULL;
+
+  }
+
+}
+
+/*
+ * Frees an aligned pointer allocated by XXH_alignedMalloc(). Don't pass
+ * normal malloc'd pointers, XXH_alignedMalloc has a specific data layout.
+ */
+static void XXH_alignedFree(void *p) {
+
+  if (p != NULL) {
+
+    xxh_u8 *ptr = (xxh_u8 *)p;
+    /* Get the offset byte we added in XXH_malloc. */
+    xxh_u8 offset = ptr[-1];
+    /* Free the original malloc'd pointer */
+    xxh_u8 *base = ptr - offset;
+    XXH_free(base);
+
+  }
+
+}
+
+XXH_PUBLIC_API XXH3_state_t *XXH3_createState(void) {
+
+  return (XXH3_state_t *)XXH_alignedMalloc(sizeof(XXH3_state_t), 64);
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_freeState(XXH3_state_t *statePtr) {
+
+  XXH_alignedFree(statePtr);
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API void XXH3_copyState(XXH3_state_t *dst_state, const XXH3_state_t *src_state) {
+
+  memcpy(dst_state, src_state, sizeof(*dst_state));
+
+}
+
+static void XXH3_64bits_reset_internal(XXH3_state_t *statePtr, XXH64_hash_t seed, const xxh_u8 *secret,
+                                       size_t secretSize) {
+
+  XXH_ASSERT(statePtr != NULL);
+  memset(statePtr, 0, sizeof(*statePtr));
+  statePtr->acc[0] = XXH_PRIME32_3;
+  statePtr->acc[1] = XXH_PRIME64_1;
+  statePtr->acc[2] = XXH_PRIME64_2;
+  statePtr->acc[3] = XXH_PRIME64_3;
+  statePtr->acc[4] = XXH_PRIME64_4;
+  statePtr->acc[5] = XXH_PRIME32_2;
+  statePtr->acc[6] = XXH_PRIME64_5;
+  statePtr->acc[7] = XXH_PRIME32_1;
+  statePtr->seed = seed;
+  XXH_ASSERT(secret != NULL);
+  statePtr->extSecret = secret;
+  XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);
+  statePtr->secretLimit = secretSize - XXH_STRIPE_LEN;
+  statePtr->nbStripesPerBlock = statePtr->secretLimit / XXH_SECRET_CONSUME_RATE;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset(XXH3_state_t *statePtr) {
+
+  if (statePtr == NULL) return XXH_ERROR;
+  XXH3_64bits_reset_internal(statePtr, 0, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset_withSecret(XXH3_state_t *statePtr, const void *secret,
+                                                          size_t secretSize) {
+
+  if (statePtr == NULL) return XXH_ERROR;
+  XXH3_64bits_reset_internal(statePtr, 0, (const xxh_u8 *)secret, secretSize);
+  if (secret == NULL) return XXH_ERROR;
+  if (secretSize < XXH3_SECRET_SIZE_MIN) return XXH_ERROR;
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset_withSeed(XXH3_state_t *statePtr, XXH64_hash_t seed) {
+
+  if (statePtr == NULL) return XXH_ERROR;
+  XXH3_64bits_reset_internal(statePtr, seed, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);
+  XXH3_initCustomSecret(statePtr->customSecret, seed);
+  statePtr->extSecret = NULL;
+  return XXH_OK;
+
+}
+
+XXH_FORCE_INLINE void XXH3_consumeStripes(xxh_u64 *XXH_RESTRICT acc, size_t *XXH_RESTRICT nbStripesSoFarPtr,
+                                          size_t nbStripesPerBlock, const xxh_u8 *XXH_RESTRICT input,
+                                          size_t totalStripes, const xxh_u8 *XXH_RESTRICT secret, size_t secretLimit,
+                                          XXH3_accWidth_e accWidth, XXH3_f_accumulate_512 f_acc512,
+                                          XXH3_f_scrambleAcc f_scramble) {
+
+  XXH_ASSERT(*nbStripesSoFarPtr < nbStripesPerBlock);
+  if (nbStripesPerBlock - *nbStripesSoFarPtr <= totalStripes) {
+
+    /* need a scrambling operation */
+    size_t const nbStripes = nbStripesPerBlock - *nbStripesSoFarPtr;
+    XXH3_accumulate(acc, input, secret + nbStripesSoFarPtr[0] * XXH_SECRET_CONSUME_RATE, nbStripes, accWidth, f_acc512);
+    f_scramble(acc, secret + secretLimit);
+    XXH3_accumulate(acc, input + nbStripes * XXH_STRIPE_LEN, secret, totalStripes - nbStripes, accWidth, f_acc512);
+    *nbStripesSoFarPtr = totalStripes - nbStripes;
+
+  } else {
+
+    XXH3_accumulate(acc, input, secret + nbStripesSoFarPtr[0] * XXH_SECRET_CONSUME_RATE, totalStripes, accWidth,
+                    f_acc512);
+    *nbStripesSoFarPtr += totalStripes;
+
+  }
+
+}
+
+/*
+ * Both XXH3_64bits_update and XXH3_128bits_update use this routine.
+ */
+XXH_FORCE_INLINE XXH_errorcode XXH3_update(XXH3_state_t *state, const xxh_u8 *input, size_t len,
+                                           XXH3_accWidth_e accWidth, XXH3_f_accumulate_512 f_acc512,
+                                           XXH3_f_scrambleAcc f_scramble) {
+
+  if (input == NULL)
+#if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER >= 1)
+    return XXH_OK;
+#else
+    return XXH_ERROR;
+#endif
+
+  {
+
+    const xxh_u8 *const        bEnd = input + len;
+    const unsigned char *const secret = (state->extSecret == NULL) ? state->customSecret : state->extSecret;
+
+    state->totalLen += len;
+
+    if (state->bufferedSize + len <= XXH3_INTERNALBUFFER_SIZE) {                              /* fill in tmp buffer */
+      XXH_memcpy(state->buffer + state->bufferedSize, input, len);
+      state->bufferedSize += (XXH32_hash_t)len;
+      return XXH_OK;
+
+    }
+
+    /* input is now > XXH3_INTERNALBUFFER_SIZE */
+
+#define XXH3_INTERNALBUFFER_STRIPES (XXH3_INTERNALBUFFER_SIZE / XXH_STRIPE_LEN)
+    XXH_STATIC_ASSERT(XXH3_INTERNALBUFFER_SIZE % XXH_STRIPE_LEN == 0);                            /* clean multiple */
+
+    /*
+     * There is some input left inside the internal buffer.
+     * Fill it, then consume it.
+     */
+    if (state->bufferedSize) {
+
+      size_t const loadSize = XXH3_INTERNALBUFFER_SIZE - state->bufferedSize;
+      XXH_memcpy(state->buffer + state->bufferedSize, input, loadSize);
+      input += loadSize;
+      XXH3_consumeStripes(state->acc, &state->nbStripesSoFar, state->nbStripesPerBlock, state->buffer,
+                          XXH3_INTERNALBUFFER_STRIPES, secret, state->secretLimit, accWidth, f_acc512, f_scramble);
+      state->bufferedSize = 0;
+
+    }
+
+    /* Consume input by full buffer quantities */
+    if (input + XXH3_INTERNALBUFFER_SIZE <= bEnd) {
+
+      const xxh_u8 *const limit = bEnd - XXH3_INTERNALBUFFER_SIZE;
+      do {
+
+        XXH3_consumeStripes(state->acc, &state->nbStripesSoFar, state->nbStripesPerBlock, input,
+                            XXH3_INTERNALBUFFER_STRIPES, secret, state->secretLimit, accWidth, f_acc512, f_scramble);
+        input += XXH3_INTERNALBUFFER_SIZE;
+
+      } while (input <= limit);
+
+      /* for last partial stripe */
+      memcpy(state->buffer + sizeof(state->buffer) - XXH_STRIPE_LEN, input - XXH_STRIPE_LEN, XXH_STRIPE_LEN);
+
+    }
+
+    if (input < bEnd) {                                                          /* Some remaining input: buffer it */
+      XXH_memcpy(state->buffer, input, (size_t)(bEnd - input));
+      state->bufferedSize = (XXH32_hash_t)(bEnd - input);
+
+    }
+
+  }
+
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_update(XXH3_state_t *state, const void *input, size_t len) {
+
+  return XXH3_update(state, (const xxh_u8 *)input, len, XXH3_acc_64bits, XXH3_accumulate_512, XXH3_scrambleAcc);
+
+}
+
+XXH_FORCE_INLINE void XXH3_digest_long(XXH64_hash_t *acc, const XXH3_state_t *state, const unsigned char *secret,
+                                       XXH3_accWidth_e accWidth) {
+
+  /*
+   * Digest on a local copy. This way, the state remains unaltered, and it can
+   * continue ingesting more input afterwards.
+   */
+  memcpy(acc, state->acc, sizeof(state->acc));
+  if (state->bufferedSize >= XXH_STRIPE_LEN) {
+
+    size_t const nbStripes = state->bufferedSize / XXH_STRIPE_LEN;
+    size_t       nbStripesSoFar = state->nbStripesSoFar;
+    XXH3_consumeStripes(acc, &nbStripesSoFar, state->nbStripesPerBlock, state->buffer, nbStripes, secret,
+                        state->secretLimit, accWidth, XXH3_accumulate_512, XXH3_scrambleAcc);
+    if (state->bufferedSize % XXH_STRIPE_LEN) {                                          /* one last partial stripe */
+      XXH3_accumulate_512(acc, state->buffer + state->bufferedSize - XXH_STRIPE_LEN,
+                          secret + state->secretLimit - XXH_SECRET_LASTACC_START, accWidth);
+
+    }
+
+  } else {                                                                         /* bufferedSize < XXH_STRIPE_LEN */
+
+    if (state->bufferedSize) {                                                                   /* one last stripe */
+      xxh_u8       lastStripe[XXH_STRIPE_LEN];
+      size_t const catchupSize = XXH_STRIPE_LEN - state->bufferedSize;
+      memcpy(lastStripe, state->buffer + sizeof(state->buffer) - catchupSize, catchupSize);
+      memcpy(lastStripe + catchupSize, state->buffer, state->bufferedSize);
+      XXH3_accumulate_512(acc, lastStripe, secret + state->secretLimit - XXH_SECRET_LASTACC_START, accWidth);
+
+    }
+
+  }
+
+}
+
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits_digest(const XXH3_state_t *state) {
+
+  const unsigned char *const secret = (state->extSecret == NULL) ? state->customSecret : state->extSecret;
+  if (state->totalLen > XXH3_MIDSIZE_MAX) {
+
+    XXH_ALIGN(XXH_ACC_ALIGN) XXH64_hash_t acc[XXH_ACC_NB];
+    XXH3_digest_long(acc, state, secret, XXH3_acc_64bits);
+    return XXH3_mergeAccs(acc, secret + XXH_SECRET_MERGEACCS_START, (xxh_u64)state->totalLen * XXH_PRIME64_1);
+
+  }
+
+  /* totalLen <= XXH3_MIDSIZE_MAX: digesting a short input */
+  if (state->seed) return XXH3_64bits_withSeed(state->buffer, (size_t)state->totalLen, state->seed);
+  return XXH3_64bits_withSecret(state->buffer, (size_t)(state->totalLen), secret, state->secretLimit + XXH_STRIPE_LEN);
+
+}
+
+#define XXH_MIN(x, y) (((x) > (y)) ? (y) : (x))
+
+XXH_PUBLIC_API void XXH3_generateSecret(void *secretBuffer, const void *customSeed, size_t customSeedSize) {
+
+  XXH_ASSERT(secretBuffer != NULL);
+  if (customSeedSize == 0) {
+
+    memcpy(secretBuffer, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);
+    return;
+
+  }
+
+  XXH_ASSERT(customSeed != NULL);
+
+  {
+
+    size_t const       segmentSize = sizeof(XXH128_hash_t);
+    size_t const       nbSegments = XXH_SECRET_DEFAULT_SIZE / segmentSize;
+    XXH128_canonical_t scrambler;
+    XXH64_hash_t       seeds[12];
+    size_t             segnb;
+    XXH_ASSERT(nbSegments == 12);
+    XXH_ASSERT(segmentSize * nbSegments == XXH_SECRET_DEFAULT_SIZE);                              /* exact multiple */
+    XXH128_canonicalFromHash(&scrambler, XXH128(customSeed, customSeedSize, 0));
+
+    /*
+     * Copy customSeed to seeds[], truncating or repeating as necessary.
+     */
+    {
+
+      size_t toFill = XXH_MIN(customSeedSize, sizeof(seeds));
+      size_t filled = toFill;
+      memcpy(seeds, customSeed, toFill);
+      while (filled < sizeof(seeds)) {
+
+        toFill = XXH_MIN(filled, sizeof(seeds) - filled);
+        memcpy((char *)seeds + filled, seeds, toFill);
+        filled += toFill;
+
+      }
+
+    }
+
+    /* generate secret */
+    memcpy(secretBuffer, &scrambler, sizeof(scrambler));
+    for (segnb = 1; segnb < nbSegments; segnb++) {
+
+      size_t const       segmentStart = segnb * segmentSize;
+      XXH128_canonical_t segment;
+      XXH128_canonicalFromHash(&segment, XXH128(&scrambler, sizeof(scrambler), XXH_readLE64(seeds + segnb) + segnb));
+      memcpy((char *)secretBuffer + segmentStart, &segment, sizeof(segment));
+
+    }
+
+  }
+
+}
+
+/* ==========================================
+ * XXH3 128 bits (a.k.a XXH128)
+ * ==========================================
+ * XXH3's 128-bit variant has better mixing and strength than the 64-bit
+ * variant, even without counting the significantly larger output size.
+ *
+ * For example, extra steps are taken to avoid the seed-dependent collisions
+ * in 17-240 byte inputs (See XXH3_mix16B and XXH128_mix32B).
+ *
+ * This strength naturally comes at the cost of some speed, especially on short
+ * lengths. Note that longer hashes are about as fast as the 64-bit version
+ * due to it using only a slight modification of the 64-bit loop.
+ *
+ * XXH128 is also more oriented towards 64-bit machines. It is still extremely
+ * fast for a _128-bit_ hash on 32-bit (it usually clears XXH64).
+ */
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_len_1to3_128b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                  XXH64_hash_t seed) {
+
+  /* A doubled version of 1to3_64b with different constants. */
+  XXH_ASSERT(input != NULL);
+  XXH_ASSERT(1 <= len && len <= 3);
+  XXH_ASSERT(secret != NULL);
+  /*
+   * len = 1: combinedl = { input[0], 0x01, input[0], input[0] }
+   * len = 2: combinedl = { input[1], 0x02, input[0], input[1] }
+   * len = 3: combinedl = { input[2], 0x03, input[0], input[1] }
+   */
+  {
+
+    xxh_u8 const  c1 = input[0];
+    xxh_u8 const  c2 = input[len >> 1];
+    xxh_u8 const  c3 = input[len - 1];
+    xxh_u32 const combinedl = ((xxh_u32)c1 << 16) | ((xxh_u32)c2 << 24) | ((xxh_u32)c3 << 0) | ((xxh_u32)len << 8);
+    xxh_u32 const combinedh = XXH_rotl32(XXH_swap32(combinedl), 13);
+    xxh_u64 const bitflipl = (XXH_readLE32(secret) ^ XXH_readLE32(secret + 4)) + seed;
+    xxh_u64 const bitfliph = (XXH_readLE32(secret + 8) ^ XXH_readLE32(secret + 12)) - seed;
+    xxh_u64 const keyed_lo = (xxh_u64)combinedl ^ bitflipl;
+    xxh_u64 const keyed_hi = (xxh_u64)combinedh ^ bitfliph;
+    xxh_u64 const mixedl = keyed_lo * XXH_PRIME64_1;
+    xxh_u64 const mixedh = keyed_hi * XXH_PRIME64_5;
+    XXH128_hash_t h128;
+    h128.low64 = XXH3_avalanche(mixedl);
+    h128.high64 = XXH3_avalanche(mixedh);
+    return h128;
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_len_4to8_128b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                  XXH64_hash_t seed) {
+
+  XXH_ASSERT(input != NULL);
+  XXH_ASSERT(secret != NULL);
+  XXH_ASSERT(4 <= len && len <= 8);
+  seed ^= (xxh_u64)XXH_swap32((xxh_u32)seed) << 32;
+  {
+
+    xxh_u32 const input_lo = XXH_readLE32(input);
+    xxh_u32 const input_hi = XXH_readLE32(input + len - 4);
+    xxh_u64 const input_64 = input_lo + ((xxh_u64)input_hi << 32);
+    xxh_u64 const bitflip = (XXH_readLE64(secret + 16) ^ XXH_readLE64(secret + 24)) + seed;
+    xxh_u64 const keyed = input_64 ^ bitflip;
+
+    /* Shift len to the left to ensure it is even, this avoids even multiplies.
+     */
+    XXH128_hash_t m128 = XXH_mult64to128(keyed, XXH_PRIME64_1 + (len << 2));
+
+    m128.high64 += (m128.low64 << 1);
+    m128.low64 ^= (m128.high64 >> 3);
+
+    m128.low64 = XXH_xorshift64(m128.low64, 35);
+    m128.low64 *= 0x9FB21C651E98DF25ULL;
+    m128.low64 = XXH_xorshift64(m128.low64, 28);
+    m128.high64 = XXH3_avalanche(m128.high64);
+    return m128;
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_len_9to16_128b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                   XXH64_hash_t seed) {
+
+  XXH_ASSERT(input != NULL);
+  XXH_ASSERT(secret != NULL);
+  XXH_ASSERT(9 <= len && len <= 16);
+  {
+
+    xxh_u64 const bitflipl = (XXH_readLE64(secret + 32) ^ XXH_readLE64(secret + 40)) - seed;
+    xxh_u64 const bitfliph = (XXH_readLE64(secret + 48) ^ XXH_readLE64(secret + 56)) + seed;
+    xxh_u64 const input_lo = XXH_readLE64(input);
+    xxh_u64       input_hi = XXH_readLE64(input + len - 8);
+    XXH128_hash_t m128 = XXH_mult64to128(input_lo ^ input_hi ^ bitflipl, XXH_PRIME64_1);
+    /*
+     * Put len in the middle of m128 to ensure that the length gets mixed to
+     * both the low and high bits in the 128x64 multiply below.
+     */
+    m128.low64 += (xxh_u64)(len - 1) << 54;
+    input_hi ^= bitfliph;
+    /*
+     * Add the high 32 bits of input_hi to the high 32 bits of m128, then
+     * add the long product of the low 32 bits of input_hi and XXH_PRIME32_2 to
+     * the high 64 bits of m128.
+     *
+     * The best approach to this operation is different on 32-bit and 64-bit.
+     */
+    if (sizeof(void *) < sizeof(xxh_u64)) {                                                               /* 32-bit */
+      /*
+       * 32-bit optimized version, which is more readable.
+       *
+       * On 32-bit, it removes an ADC and delays a dependency between the two
+       * halves of m128.high64, but it generates an extra mask on 64-bit.
+       */
+      m128.high64 += (input_hi & 0xFFFFFFFF00000000) + XXH_mult32to64((xxh_u32)input_hi, XXH_PRIME32_2);
+
+    } else {
+
+      /*
+       * 64-bit optimized (albeit more confusing) version.
+       *
+       * Uses some properties of addition and multiplication to remove the mask:
+       *
+       * Let:
+       *    a = input_hi.lo = (input_hi & 0x00000000FFFFFFFF)
+       *    b = input_hi.hi = (input_hi & 0xFFFFFFFF00000000)
+       *    c = XXH_PRIME32_2
+       *
+       *    a + (b * c)
+       * Inverse Property: x + y - x == y
+       *    a + (b * (1 + c - 1))
+       * Distributive Property: x * (y + z) == (x * y) + (x * z)
+       *    a + (b * 1) + (b * (c - 1))
+       * Identity Property: x * 1 == x
+       *    a + b + (b * (c - 1))
+       *
+       * Substitute a, b, and c:
+       *    input_hi.hi + input_hi.lo + ((xxh_u64)input_hi.lo * (XXH_PRIME32_2 -
+       * 1))
+       *
+       * Since input_hi.hi + input_hi.lo == input_hi, we get this:
+       *    input_hi + ((xxh_u64)input_hi.lo * (XXH_PRIME32_2 - 1))
+       */
+      m128.high64 += input_hi + XXH_mult32to64((xxh_u32)input_hi, XXH_PRIME32_2 - 1);
+
+    }
+
+    /* m128 ^= XXH_swap64(m128 >> 64); */
+    m128.low64 ^= XXH_swap64(m128.high64);
+
+    {                                                              /* 128x64 multiply: h128 = m128 * XXH_PRIME64_2; */
+      XXH128_hash_t h128 = XXH_mult64to128(m128.low64, XXH_PRIME64_2);
+      h128.high64 += m128.high64 * XXH_PRIME64_2;
+
+      h128.low64 = XXH3_avalanche(h128.low64);
+      h128.high64 = XXH3_avalanche(h128.high64);
+      return h128;
+
+    }
+
+  }
+
+}
+
+/*
+ * Assumption: `secret` size is >= XXH3_SECRET_SIZE_MIN
+ */
+XXH_FORCE_INLINE XXH128_hash_t XXH3_len_0to16_128b(const xxh_u8 *input, size_t len, const xxh_u8 *secret,
+                                                   XXH64_hash_t seed) {
+
+  XXH_ASSERT(len <= 16);
+  {
+
+    if (len > 8) return XXH3_len_9to16_128b(input, len, secret, seed);
+    if (len >= 4) return XXH3_len_4to8_128b(input, len, secret, seed);
+    if (len) return XXH3_len_1to3_128b(input, len, secret, seed);
+    {
+
+      XXH128_hash_t h128;
+      xxh_u64 const bitflipl = XXH_readLE64(secret + 64) ^ XXH_readLE64(secret + 72);
+      xxh_u64 const bitfliph = XXH_readLE64(secret + 80) ^ XXH_readLE64(secret + 88);
+      h128.low64 = XXH3_avalanche((XXH_PRIME64_1 + seed) ^ bitflipl);
+      h128.high64 = XXH3_avalanche((XXH_PRIME64_2 - seed) ^ bitfliph);
+      return h128;
+
+    }
+
+  }
+
+}
+
+/*
+ * A bit slower than XXH3_mix16B, but handles multiply by zero better.
+ */
+XXH_FORCE_INLINE XXH128_hash_t XXH128_mix32B(XXH128_hash_t acc, const xxh_u8 *input_1, const xxh_u8 *input_2,
+                                             const xxh_u8 *secret, XXH64_hash_t seed) {
+
+  acc.low64 += XXH3_mix16B(input_1, secret + 0, seed);
+  acc.low64 ^= XXH_readLE64(input_2) + XXH_readLE64(input_2 + 8);
+  acc.high64 += XXH3_mix16B(input_2, secret + 16, seed);
+  acc.high64 ^= XXH_readLE64(input_1) + XXH_readLE64(input_1 + 8);
+  return acc;
+
+}
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_len_17to128_128b(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                     const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                     XXH64_hash_t seed) {
+
+  XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);
+  (void)secretSize;
+  XXH_ASSERT(16 < len && len <= 128);
+
+  {
+
+    XXH128_hash_t acc;
+    acc.low64 = len * XXH_PRIME64_1;
+    acc.high64 = 0;
+    if (len > 32) {
+
+      if (len > 64) {
+
+        if (len > 96) { acc = XXH128_mix32B(acc, input + 48, input + len - 64, secret + 96, seed); }
+
+        acc = XXH128_mix32B(acc, input + 32, input + len - 48, secret + 64, seed);
+
+      }
+
+      acc = XXH128_mix32B(acc, input + 16, input + len - 32, secret + 32, seed);
+
+    }
+
+    acc = XXH128_mix32B(acc, input, input + len - 16, secret, seed);
+    {
+
+      XXH128_hash_t h128;
+      h128.low64 = acc.low64 + acc.high64;
+      h128.high64 = (acc.low64 * XXH_PRIME64_1) + (acc.high64 * XXH_PRIME64_4) + ((len - seed) * XXH_PRIME64_2);
+      h128.low64 = XXH3_avalanche(h128.low64);
+      h128.high64 = (XXH64_hash_t)0 - XXH3_avalanche(h128.high64);
+      return h128;
+
+    }
+
+  }
+
+}
+
+XXH_NO_INLINE XXH128_hash_t XXH3_len_129to240_128b(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                   const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                   XXH64_hash_t seed) {
+
+  XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);
+  (void)secretSize;
+  XXH_ASSERT(128 < len && len <= XXH3_MIDSIZE_MAX);
+
+  {
+
+    XXH128_hash_t acc;
+    int const     nbRounds = (int)len / 32;
+    int           i;
+    acc.low64 = len * XXH_PRIME64_1;
+    acc.high64 = 0;
+    for (i = 0; i < 4; i++) {
+
+      acc = XXH128_mix32B(acc, input + (32 * i), input + (32 * i) + 16, secret + (32 * i), seed);
+
+    }
+
+    acc.low64 = XXH3_avalanche(acc.low64);
+    acc.high64 = XXH3_avalanche(acc.high64);
+    XXH_ASSERT(nbRounds >= 4);
+    for (i = 4; i < nbRounds; i++) {
+
+      acc = XXH128_mix32B(acc, input + (32 * i), input + (32 * i) + 16,
+                          secret + XXH3_MIDSIZE_STARTOFFSET + (32 * (i - 4)), seed);
+
+    }
+
+    /* last bytes */
+    acc = XXH128_mix32B(acc, input + len - 16, input + len - 32,
+                        secret + XXH3_SECRET_SIZE_MIN - XXH3_MIDSIZE_LASTOFFSET - 16, 0ULL - seed);
+
+    {
+
+      XXH128_hash_t h128;
+      h128.low64 = acc.low64 + acc.high64;
+      h128.high64 = (acc.low64 * XXH_PRIME64_1) + (acc.high64 * XXH_PRIME64_4) + ((len - seed) * XXH_PRIME64_2);
+      h128.low64 = XXH3_avalanche(h128.low64);
+      h128.high64 = (XXH64_hash_t)0 - XXH3_avalanche(h128.high64);
+      return h128;
+
+    }
+
+  }
+
+}
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_hashLong_128b_internal(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                           const xxh_u8 *XXH_RESTRICT secret, size_t secretSize,
+                                                           XXH3_f_accumulate_512 f_acc512,
+                                                           XXH3_f_scrambleAcc    f_scramble) {
+
+  XXH_ALIGN(XXH_ACC_ALIGN) xxh_u64 acc[XXH_ACC_NB] = XXH3_INIT_ACC;
+
+  XXH3_hashLong_internal_loop(acc, input, len, secret, secretSize, XXH3_acc_128bits, f_acc512, f_scramble);
+
+  /* converge into final hash */
+  XXH_STATIC_ASSERT(sizeof(acc) == 64);
+  XXH_ASSERT(secretSize >= sizeof(acc) + XXH_SECRET_MERGEACCS_START);
+  {
+
+    XXH128_hash_t h128;
+    h128.low64 = XXH3_mergeAccs(acc, secret + XXH_SECRET_MERGEACCS_START, (xxh_u64)len * XXH_PRIME64_1);
+    h128.high64 = XXH3_mergeAccs(acc, secret + secretSize - sizeof(acc) - XXH_SECRET_MERGEACCS_START,
+                                 ~((xxh_u64)len * XXH_PRIME64_2));
+    return h128;
+
+  }
+
+}
+
+/*
+ * It's important for performance that XXH3_hashLong is not inlined.
+ */
+XXH_NO_INLINE XXH128_hash_t XXH3_hashLong_128bSecret(const xxh_u8 *XXH_RESTRICT input, size_t len, XXH64_hash_t seed64,
+                                                     const xxh_u8 *XXH_RESTRICT secret, size_t secretLen) {
+
+  (void)seed64;
+  (void)secret;
+  (void)secretLen;
+  return XXH3_hashLong_128b_internal(input, len, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_accumulate_512,
+                                     XXH3_scrambleAcc);
+
+}
+
+/*
+ * It's important for performance that XXH3_hashLong is not inlined.
+ */
+XXH_NO_INLINE XXH128_hash_t XXH3_hashLong_128b_withSecret(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                          XXH64_hash_t seed64, const xxh_u8 *XXH_RESTRICT secret,
+                                                          size_t secretLen) {
+
+  (void)seed64;
+  return XXH3_hashLong_128b_internal(input, len, secret, secretLen, XXH3_accumulate_512, XXH3_scrambleAcc);
+
+}
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_hashLong_128b_withSeed_internal(const xxh_u8 *XXH_RESTRICT input, size_t len,
+                                                                    XXH64_hash_t seed64, XXH3_f_accumulate_512 f_acc512,
+                                                                    XXH3_f_scrambleAcc      f_scramble,
+                                                                    XXH3_f_initCustomSecret f_initSec) {
+
+  if (seed64 == 0)
+    return XXH3_hashLong_128b_internal(input, len, XXH3_kSecret, sizeof(XXH3_kSecret), f_acc512, f_scramble);
+  {
+
+    XXH_ALIGN(XXH_SEC_ALIGN) xxh_u8 secret[XXH_SECRET_DEFAULT_SIZE];
+    f_initSec(secret, seed64);
+    return XXH3_hashLong_128b_internal(input, len, secret, sizeof(secret), f_acc512, f_scramble);
+
+  }
+
+}
+
+/*
+ * It's important for performance that XXH3_hashLong is not inlined.
+ */
+XXH_NO_INLINE XXH128_hash_t XXH3_hashLong_128b_withSeed(const xxh_u8 *input, size_t len, XXH64_hash_t seed64,
+                                                        const xxh_u8 *XXH_RESTRICT secret, size_t secretLen) {
+
+  (void)secret;
+  (void)secretLen;
+  return XXH3_hashLong_128b_withSeed_internal(input, len, seed64, XXH3_accumulate_512, XXH3_scrambleAcc,
+                                              XXH3_initCustomSecret);
+
+}
+
+typedef XXH128_hash_t (*XXH3_hashLong128_f)(const xxh_u8 *XXH_RESTRICT, size_t, XXH64_hash_t,
+                                            const xxh_u8 *XXH_RESTRICT, size_t);
+
+XXH_FORCE_INLINE XXH128_hash_t XXH3_128bits_internal(const void *input, size_t len, XXH64_hash_t seed64,
+                                                     const xxh_u8 *XXH_RESTRICT secret, size_t secretLen,
+                                                     XXH3_hashLong128_f f_hl128) {
+
+  XXH_ASSERT(secretLen >= XXH3_SECRET_SIZE_MIN);
+  /*
+   * If an action is to be taken if `secret` conditions are not respected,
+   * it should be done here.
+   * For now, it's a contract pre-condition.
+   * Adding a check and a branch here would cost performance at every hash.
+   */
+  if (len <= 16) return XXH3_len_0to16_128b((const xxh_u8 *)input, len, secret, seed64);
+  if (len <= 128) return XXH3_len_17to128_128b((const xxh_u8 *)input, len, secret, secretLen, seed64);
+  if (len <= XXH3_MIDSIZE_MAX) return XXH3_len_129to240_128b((const xxh_u8 *)input, len, secret, secretLen, seed64);
+  return f_hl128((const xxh_u8 *)input, len, seed64, secret, secretLen);
+
+}
+
+/* ===   Public XXH128 API   === */
+
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits(const void *input, size_t len) {
+
+  return XXH3_128bits_internal(input, len, 0, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_hashLong_128b_withSecret);
+
+}
+
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits_withSecret(const void *input, size_t len, const void *secret,
+                                                     size_t secretSize) {
+
+  return XXH3_128bits_internal(input, len, 0, (const xxh_u8 *)secret, secretSize, XXH3_hashLong_128bSecret);
+
+}
+
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits_withSeed(const void *input, size_t len, XXH64_hash_t seed) {
+
+  return XXH3_128bits_internal(input, len, seed, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_hashLong_128b_withSeed);
+
+}
+
+XXH_PUBLIC_API XXH128_hash_t XXH128(const void *input, size_t len, XXH64_hash_t seed) {
+
+  return XXH3_128bits_withSeed(input, len, seed);
+
+}
+
+/* ===   XXH3 128-bit streaming   === */
+
+/*
+ * All the functions are actually the same as for 64-bit streaming variant.
+ * The only difference is the finalizatiom routine.
+ */
+
+static void XXH3_128bits_reset_internal(XXH3_state_t *statePtr, XXH64_hash_t seed, const xxh_u8 *secret,
+                                        size_t secretSize) {
+
+  XXH3_64bits_reset_internal(statePtr, seed, secret, secretSize);
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset(XXH3_state_t *statePtr) {
+
+  if (statePtr == NULL) return XXH_ERROR;
+  XXH3_128bits_reset_internal(statePtr, 0, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset_withSecret(XXH3_state_t *statePtr, const void *secret,
+                                                           size_t secretSize) {
+
+  if (statePtr == NULL) return XXH_ERROR;
+  XXH3_128bits_reset_internal(statePtr, 0, (const xxh_u8 *)secret, secretSize);
+  if (secret == NULL) return XXH_ERROR;
+  if (secretSize < XXH3_SECRET_SIZE_MIN) return XXH_ERROR;
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset_withSeed(XXH3_state_t *statePtr, XXH64_hash_t seed) {
+
+  if (statePtr == NULL) return XXH_ERROR;
+  XXH3_128bits_reset_internal(statePtr, seed, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);
+  XXH3_initCustomSecret(statePtr->customSecret, seed);
+  statePtr->extSecret = NULL;
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_update(XXH3_state_t *state, const void *input, size_t len) {
+
+  return XXH3_update(state, (const xxh_u8 *)input, len, XXH3_acc_128bits, XXH3_accumulate_512, XXH3_scrambleAcc);
+
+}
+
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits_digest(const XXH3_state_t *state) {
+
+  const unsigned char *const secret = (state->extSecret == NULL) ? state->customSecret : state->extSecret;
+  if (state->totalLen > XXH3_MIDSIZE_MAX) {
+
+    XXH_ALIGN(XXH_ACC_ALIGN) XXH64_hash_t acc[XXH_ACC_NB];
+    XXH3_digest_long(acc, state, secret, XXH3_acc_128bits);
+    XXH_ASSERT(state->secretLimit + XXH_STRIPE_LEN >= sizeof(acc) + XXH_SECRET_MERGEACCS_START);
+    {
+
+      XXH128_hash_t h128;
+      h128.low64 = XXH3_mergeAccs(acc, secret + XXH_SECRET_MERGEACCS_START, (xxh_u64)state->totalLen * XXH_PRIME64_1);
+      h128.high64 =
+          XXH3_mergeAccs(acc, secret + state->secretLimit + XXH_STRIPE_LEN - sizeof(acc) - XXH_SECRET_MERGEACCS_START,
+                         ~((xxh_u64)state->totalLen * XXH_PRIME64_2));
+      return h128;
+
+    }
+
+  }
+
+  /* len <= XXH3_MIDSIZE_MAX : short code */
+  if (state->seed) return XXH3_128bits_withSeed(state->buffer, (size_t)state->totalLen, state->seed);
+  return XXH3_128bits_withSecret(state->buffer, (size_t)(state->totalLen), secret, state->secretLimit + XXH_STRIPE_LEN);
+
+}
+
+/* 128-bit utility functions */
+
+#include <string.h>                                                                               /* memcmp, memcpy */
+
+/* return : 1 is equal, 0 if different */
+XXH_PUBLIC_API int XXH128_isEqual(XXH128_hash_t h1, XXH128_hash_t h2) {
+
+  /* note : XXH128_hash_t is compact, it has no padding byte */
+  return !(memcmp(&h1, &h2, sizeof(h1)));
+
+}
+
+/* This prototype is compatible with stdlib's qsort().
+ * return : >0 if *h128_1  > *h128_2
+ *          <0 if *h128_1  < *h128_2
+ *          =0 if *h128_1 == *h128_2  */
+XXH_PUBLIC_API int XXH128_cmp(const void *h128_1, const void *h128_2) {
+
+  XXH128_hash_t const h1 = *(const XXH128_hash_t *)h128_1;
+  XXH128_hash_t const h2 = *(const XXH128_hash_t *)h128_2;
+  int const           hcmp = (h1.high64 > h2.high64) - (h2.high64 > h1.high64);
+  /* note : bets that, in most cases, hash values are different */
+  if (hcmp) return hcmp;
+  return (h1.low64 > h2.low64) - (h2.low64 > h1.low64);
+
+}
+
+/*======   Canonical representation   ======*/
+XXH_PUBLIC_API void XXH128_canonicalFromHash(XXH128_canonical_t *dst, XXH128_hash_t hash) {
+
+  XXH_STATIC_ASSERT(sizeof(XXH128_canonical_t) == sizeof(XXH128_hash_t));
+  if (XXH_CPU_LITTLE_ENDIAN) {
+
+    hash.high64 = XXH_swap64(hash.high64);
+    hash.low64 = XXH_swap64(hash.low64);
+
+  }
+
+  memcpy(dst, &hash.high64, sizeof(hash.high64));
+  memcpy((char *)dst + sizeof(hash.high64), &hash.low64, sizeof(hash.low64));
+
+}
+
+XXH_PUBLIC_API XXH128_hash_t XXH128_hashFromCanonical(const XXH128_canonical_t *src) {
+
+  XXH128_hash_t h;
+  h.high64 = XXH_readBE64(src);
+  h.low64 = XXH_readBE64(src->digest + 8);
+  return h;
+
+}
+
+/* Pop our optimization override from above */
+#if XXH_VECTOR == XXH_AVX2                                  /* AVX2 */           \
+    && defined(__GNUC__) && !defined(__clang__)             /* GCC, not Clang */ \
+    && defined(__OPTIMIZE__) && !defined(__OPTIMIZE_SIZE__)                                  /* respect -O0 and -Os */
+  #pragma GCC pop_options
+#endif
+
+#endif                                                                                         /* XXH3_H_1397135465 */
+
diff -ruN qemu/include/libAFL/xxhash.h qemu_patched/include/libAFL/xxhash.h
--- qemu/include/libAFL/xxhash.h	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/include/libAFL/xxhash.h	2023-10-31 00:30:39.145308915 +0100
@@ -0,0 +1,2329 @@
+/*
+ * xxHash - Extremely Fast Hash algorithm
+ * Header File
+ * Copyright (C) 2012-2020 Yann Collet
+ *
+ * BSD 2-Clause License (https://www.opensource.org/licenses/bsd-license.php)
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *    * Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *    * Redistributions in binary form must reproduce the above
+ *      copyright notice, this list of conditions and the following disclaimer
+ *      in the documentation and/or other materials provided with the
+ *      distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * You can contact the author at:
+ *   - xxHash homepage: https://www.xxhash.com
+ *   - xxHash source repository: https://github.com/Cyan4973/xxHash
+ */
+
+/* TODO: update */
+/* Notice extracted from xxHash homepage:
+
+xxHash is an extremely fast hash algorithm, running at RAM speed limits.
+It also successfully passes all tests from the SMHasher suite.
+
+Comparison (single thread, Windows Seven 32 bits, using SMHasher on a Core 2 Duo
+@3GHz)
+
+Name            Speed       Q.Score   Author
+xxHash          5.4 GB/s     10
+CrapWow         3.2 GB/s      2       Andrew
+MumurHash 3a    2.7 GB/s     10       Austin Appleby
+SpookyHash      2.0 GB/s     10       Bob Jenkins
+SBox            1.4 GB/s      9       Bret Mulvey
+Lookup3         1.2 GB/s      9       Bob Jenkins
+SuperFastHash   1.2 GB/s      1       Paul Hsieh
+CityHash64      1.05 GB/s    10       Pike & Alakuijala
+FNV             0.55 GB/s     5       Fowler, Noll, Vo
+CRC32           0.43 GB/s     9
+MD5-32          0.33 GB/s    10       Ronald L. Rivest
+SHA1-32         0.28 GB/s    10
+
+Q.Score is a measure of quality of the hash function.
+It depends on successfully passing SMHasher test set.
+10 is a perfect score.
+
+Note: SMHasher's CRC32 implementation is not the fastest one.
+Other speed-oriented implementations can be faster,
+especially in combination with PCLMUL instruction:
+https://fastcompression.blogspot.com/2019/03/presenting-xxh3.html?showComment=1552696407071#c3490092340461170735
+
+A 64-bit version, named XXH64, is available since r35.
+It offers much better speed, but for 64-bit applications only.
+Name     Speed on 64 bits    Speed on 32 bits
+XXH64       13.8 GB/s            1.9 GB/s
+XXH32        6.8 GB/s            6.0 GB/s
+*/
+
+#if defined(__cplusplus)
+extern "C" {
+
+#endif
+
+/* ****************************
+ *  INLINE mode
+ ******************************/
+/*!
+ * XXH_INLINE_ALL (and XXH_PRIVATE_API)
+ * Use these build macros to inline xxhash into the target unit.
+ * Inlining improves performance on small inputs, especially when the length is
+ * expressed as a compile-time constant:
+ *
+ *      https://fastcompression.blogspot.com/2018/03/xxhash-for-small-keys-impressive-power.html
+ *
+ * It also keeps xxHash symbols private to the unit, so they are not exported.
+ *
+ * Usage:
+ *     #define XXH_INLINE_ALL
+ *     #include "xxhash.h"
+ *
+ * Do not compile and link xxhash.o as a separate object, as it is not useful.
+ */
+#if (defined(XXH_INLINE_ALL) || defined(XXH_PRIVATE_API)) && !defined(XXH_INLINE_ALL_31684351384)
+/* this section should be traversed only once */
+  #define XXH_INLINE_ALL_31684351384
+/* give access to the advanced API, required to compile implementations */
+  #undef XXH_STATIC_LINKING_ONLY                                                               /* avoid macro redef */
+  #define XXH_STATIC_LINKING_ONLY
+/* make all functions private */
+  #undef XXH_PUBLIC_API
+  #if defined(__GNUC__)
+    #define XXH_PUBLIC_API static __inline __attribute__((unused))
+  #elif defined(__cplusplus) || (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */)
+    #define XXH_PUBLIC_API static inline
+  #elif defined(_MSC_VER)
+    #define XXH_PUBLIC_API static __inline
+  #else
+  /* note: this version may generate warnings for unused static functions */
+    #define XXH_PUBLIC_API static
+  #endif
+
+/*
+ * This part deals with the special case where a unit wants to inline xxHash,
+ * but "xxhash.h" has previously been included without XXH_INLINE_ALL, such
+ * as part of some previously included *.h header file.
+ * Without further action, the new include would just be ignored,
+ * and functions would effectively _not_ be inlined (silent failure).
+ * The following macros solve this situation by prefixing all inlined names,
+ * avoiding naming collision with previous inclusions.
+ */
+  #ifdef XXH_NAMESPACE
+    #error "XXH_INLINE_ALL with XXH_NAMESPACE is not supported"
+  /*
+   * Note: Alternative: #undef all symbols (it's a pretty large list).
+   * Without #error: it compiles, but functions are actually not inlined.
+   */
+  #endif
+  #define XXH_NAMESPACE XXH_INLINE_
+/*
+ * Some identifiers (enums, type names) are not symbols, but they must
+ * still be renamed to avoid redeclaration.
+ * Alternative solution: do not redeclare them.
+ * However, this requires some #ifdefs, and is a more dispersed action.
+ * Meanwhile, renaming can be achieved in a single block
+ */
+  #define XXH_IPREF(Id) XXH_INLINE_##Id
+  #define XXH_OK XXH_IPREF(XXH_OK)
+  #define XXH_ERROR XXH_IPREF(XXH_ERROR)
+  #define XXH_errorcode XXH_IPREF(XXH_errorcode)
+  #define XXH32_canonical_t XXH_IPREF(XXH32_canonical_t)
+  #define XXH64_canonical_t XXH_IPREF(XXH64_canonical_t)
+  #define XXH128_canonical_t XXH_IPREF(XXH128_canonical_t)
+  #define XXH32_state_s XXH_IPREF(XXH32_state_s)
+  #define XXH32_state_t XXH_IPREF(XXH32_state_t)
+  #define XXH64_state_s XXH_IPREF(XXH64_state_s)
+  #define XXH64_state_t XXH_IPREF(XXH64_state_t)
+  #define XXH3_state_s XXH_IPREF(XXH3_state_s)
+  #define XXH3_state_t XXH_IPREF(XXH3_state_t)
+  #define XXH128_hash_t XXH_IPREF(XXH128_hash_t)
+/* Ensure the header is parsed again, even if it was previously included */
+  #undef XXHASH_H_5627135585666179
+  #undef XXHASH_H_STATIC_13879238742
+#endif                                                                         /* XXH_INLINE_ALL || XXH_PRIVATE_API */
+
+/* ****************************************************************
+ *  Stable API
+ *****************************************************************/
+#ifndef XXHASH_H_5627135585666179
+  #define XXHASH_H_5627135585666179 1
+
+  /* specific declaration modes for Windows */
+  #if !defined(XXH_INLINE_ALL) && !defined(XXH_PRIVATE_API)
+    #if defined(WIN32) && defined(_MSC_VER) && (defined(XXH_IMPORT) || defined(XXH_EXPORT))
+      #ifdef XXH_EXPORT
+        #define XXH_PUBLIC_API __declspec(dllexport)
+      #elif XXH_IMPORT
+        #define XXH_PUBLIC_API __declspec(dllimport)
+      #endif
+    #else
+      #define XXH_PUBLIC_API                                                                          /* do nothing */
+    #endif
+  #endif
+
+  /*!
+   * XXH_NAMESPACE, aka Namespace Emulation:
+   *
+   * If you want to include _and expose_ xxHash functions from within your own
+   * library, but also want to avoid symbol collisions with other libraries
+   * which may also include xxHash, you can use XXH_NAMESPACE to automatically
+   * prefix any public symbol from xxhash library with the value of
+   * XXH_NAMESPACE (therefore, avoid empty or numeric values).
+   *
+   * Note that no change is required within the calling program as long as it
+   * includes `xxhash.h`: Regular symbol names will be automatically translated
+   * by this header.
+   */
+  #ifdef XXH_NAMESPACE
+    #define XXH_CAT(A, B) A##B
+    #define XXH_NAME2(A, B) XXH_CAT(A, B)
+    #define XXH_versionNumber XXH_NAME2(XXH_NAMESPACE, XXH_versionNumber)
+    #define XXH32 XXH_NAME2(XXH_NAMESPACE, XXH32)
+    #define XXH32_createState XXH_NAME2(XXH_NAMESPACE, XXH32_createState)
+    #define XXH32_freeState XXH_NAME2(XXH_NAMESPACE, XXH32_freeState)
+    #define XXH32_reset XXH_NAME2(XXH_NAMESPACE, XXH32_reset)
+    #define XXH32_update XXH_NAME2(XXH_NAMESPACE, XXH32_update)
+    #define XXH32_digest XXH_NAME2(XXH_NAMESPACE, XXH32_digest)
+    #define XXH32_copyState XXH_NAME2(XXH_NAMESPACE, XXH32_copyState)
+    #define XXH32_canonicalFromHash XXH_NAME2(XXH_NAMESPACE, XXH32_canonicalFromHash)
+    #define XXH32_hashFromCanonical XXH_NAME2(XXH_NAMESPACE, XXH32_hashFromCanonical)
+    #define XXH64 XXH_NAME2(XXH_NAMESPACE, XXH64)
+    #define XXH64_createState XXH_NAME2(XXH_NAMESPACE, XXH64_createState)
+    #define XXH64_freeState XXH_NAME2(XXH_NAMESPACE, XXH64_freeState)
+    #define XXH64_reset XXH_NAME2(XXH_NAMESPACE, XXH64_reset)
+    #define XXH64_update XXH_NAME2(XXH_NAMESPACE, XXH64_update)
+    #define XXH64_digest XXH_NAME2(XXH_NAMESPACE, XXH64_digest)
+    #define XXH64_copyState XXH_NAME2(XXH_NAMESPACE, XXH64_copyState)
+    #define XXH64_canonicalFromHash XXH_NAME2(XXH_NAMESPACE, XXH64_canonicalFromHash)
+    #define XXH64_hashFromCanonical XXH_NAME2(XXH_NAMESPACE, XXH64_hashFromCanonical)
+  #endif
+
+  /* *************************************
+   *  Version
+   ***************************************/
+  #define XXH_VERSION_MAJOR 0
+  #define XXH_VERSION_MINOR 7
+  #define XXH_VERSION_RELEASE 4
+  #define XXH_VERSION_NUMBER (XXH_VERSION_MAJOR * 100 * 100 + XXH_VERSION_MINOR * 100 + XXH_VERSION_RELEASE)
+XXH_PUBLIC_API unsigned XXH_versionNumber(void);
+
+  /* ****************************
+   *  Definitions
+   ******************************/
+  #include <stddef.h>                                                                                     /* size_t */
+typedef enum { XXH_OK = 0, XXH_ERROR } XXH_errorcode;
+
+  /*-**********************************************************************
+   *  32-bit hash
+   ************************************************************************/
+  #if !defined(__VMS) && \
+      (defined(__cplusplus) || (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */))
+    #include <stdint.h>
+typedef uint32_t XXH32_hash_t;
+  #else
+    #include <limits.h>
+    #if UINT_MAX == 0xFFFFFFFFUL
+typedef unsigned int  XXH32_hash_t;
+    #else
+      #if ULONG_MAX == 0xFFFFFFFFUL
+typedef unsigned long XXH32_hash_t;
+      #else
+        #error "unsupported platform: need a 32-bit type"
+      #endif
+    #endif
+  #endif
+
+/*!
+ * XXH32():
+ *  Calculate the 32-bit hash of sequence "length" bytes stored at memory
+ * address "input". The memory between input & input+length must be valid
+ * (allocated and read-accessible). "seed" can be used to alter the result
+ * predictably. Speed on Core 2 Duo @ 3 GHz (single thread, SMHasher
+ * benchmark): 5.4 GB/s
+ *
+ * Note: XXH3 provides competitive speed for both 32-bit and 64-bit systems,
+ * and offers true 64/128 bit hash results. It provides a superior level of
+ * dispersion, and greatly reduces the risks of collisions.
+ */
+XXH_PUBLIC_API XXH32_hash_t XXH32(const void *input, size_t length, XXH32_hash_t seed);
+
+/*******   Streaming   *******/
+
+/*
+ * Streaming functions generate the xxHash value from an incrememtal input.
+ * This method is slower than single-call functions, due to state management.
+ * For small inputs, prefer `XXH32()` and `XXH64()`, which are better optimized.
+ *
+ * An XXH state must first be allocated using `XXH*_createState()`.
+ *
+ * Start a new hash by initializing the state with a seed using `XXH*_reset()`.
+ *
+ * Then, feed the hash state by calling `XXH*_update()` as many times as
+ * necessary.
+ *
+ * The function returns an error code, with 0 meaning OK, and any other value
+ * meaning there is an error.
+ *
+ * Finally, a hash value can be produced anytime, by using `XXH*_digest()`.
+ * This function returns the nn-bits hash as an int or long long.
+ *
+ * It's still possible to continue inserting input into the hash state after a
+ * digest, and generate new hash values later on by invoking `XXH*_digest()`.
+ *
+ * When done, release the state using `XXH*_freeState()`.
+ */
+
+typedef struct XXH32_state_s XXH32_state_t;                                                      /* incomplete type */
+XXH_PUBLIC_API XXH32_state_t *XXH32_createState(void);
+XXH_PUBLIC_API XXH_errorcode  XXH32_freeState(XXH32_state_t *statePtr);
+XXH_PUBLIC_API void           XXH32_copyState(XXH32_state_t *dst_state, const XXH32_state_t *src_state);
+
+XXH_PUBLIC_API XXH_errorcode XXH32_reset(XXH32_state_t *statePtr, XXH32_hash_t seed);
+XXH_PUBLIC_API XXH_errorcode XXH32_update(XXH32_state_t *statePtr, const void *input, size_t length);
+XXH_PUBLIC_API XXH32_hash_t  XXH32_digest(const XXH32_state_t *statePtr);
+
+/*******   Canonical representation   *******/
+
+/*
+ * The default return values from XXH functions are unsigned 32 and 64 bit
+ * integers.
+ * This the simplest and fastest format for further post-processing.
+ *
+ * However, this leaves open the question of what is the order on the byte
+ * level, since little and big endian conventions will store the same number
+ * differently.
+ *
+ * The canonical representation settles this issue by mandating big-endian
+ * convention, the same convention as human-readable numbers (large digits
+ * first).
+ *
+ * When writing hash values to storage, sending them over a network, or printing
+ * them, it's highly recommended to use the canonical representation to ensure
+ * portability across a wider range of systems, present and future.
+ *
+ * The following functions allow transformation of hash values to and from
+ * canonical format.
+ */
+
+typedef struct {
+
+  unsigned char digest[4];
+
+} XXH32_canonical_t;
+
+XXH_PUBLIC_API void         XXH32_canonicalFromHash(XXH32_canonical_t *dst, XXH32_hash_t hash);
+XXH_PUBLIC_API XXH32_hash_t XXH32_hashFromCanonical(const XXH32_canonical_t *src);
+
+  #ifndef XXH_NO_LONG_LONG
+    /*-**********************************************************************
+     *  64-bit hash
+     ************************************************************************/
+    #if !defined(__VMS) && \
+        (defined(__cplusplus) || (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */))
+      #include <stdint.h>
+typedef uint64_t XXH64_hash_t;
+    #else
+/* the following type must have a width of 64-bit */
+typedef unsigned long long XXH64_hash_t;
+    #endif
+
+/*!
+ * XXH64():
+ * Returns the 64-bit hash of sequence of length @length stored at memory
+ * address @input.
+ * @seed can be used to alter the result predictably.
+ *
+ * This function usually runs faster on 64-bit systems, but slower on 32-bit
+ * systems (see benchmark).
+ *
+ * Note: XXH3 provides competitive speed for both 32-bit and 64-bit systems,
+ * and offers true 64/128 bit hash results. It provides a superior level of
+ * dispersion, and greatly reduces the risks of collisions.
+ */
+XXH_PUBLIC_API XXH64_hash_t XXH64(const void *input, size_t length, XXH64_hash_t seed);
+
+/*******   Streaming   *******/
+typedef struct XXH64_state_s XXH64_state_t;                                                      /* incomplete type */
+XXH_PUBLIC_API XXH64_state_t *XXH64_createState(void);
+XXH_PUBLIC_API XXH_errorcode  XXH64_freeState(XXH64_state_t *statePtr);
+XXH_PUBLIC_API void           XXH64_copyState(XXH64_state_t *dst_state, const XXH64_state_t *src_state);
+
+XXH_PUBLIC_API XXH_errorcode XXH64_reset(XXH64_state_t *statePtr, XXH64_hash_t seed);
+XXH_PUBLIC_API XXH_errorcode XXH64_update(XXH64_state_t *statePtr, const void *input, size_t length);
+XXH_PUBLIC_API XXH64_hash_t  XXH64_digest(const XXH64_state_t *statePtr);
+
+/*******   Canonical representation   *******/
+typedef struct {
+
+  unsigned char digest[sizeof(XXH64_hash_t)];
+
+} XXH64_canonical_t;
+
+XXH_PUBLIC_API void         XXH64_canonicalFromHash(XXH64_canonical_t *dst, XXH64_hash_t hash);
+XXH_PUBLIC_API XXH64_hash_t XXH64_hashFromCanonical(const XXH64_canonical_t *src);
+
+  #endif                                                                                        /* XXH_NO_LONG_LONG */
+
+#endif                                                                                 /* XXHASH_H_5627135585666179 */
+
+#if defined(XXH_STATIC_LINKING_ONLY) && !defined(XXHASH_H_STATIC_13879238742)
+  #define XXHASH_H_STATIC_13879238742
+/* ****************************************************************************
+ * This section contains declarations which are not guaranteed to remain stable.
+ * They may change in future versions, becoming incompatible with a different
+ * version of the library.
+ * These declarations should only be used with static linking.
+ * Never use them in association with dynamic linking!
+ *****************************************************************************
+ */
+
+/*
+ * These definitions are only present to allow static allocation of an XXH
+ * state, for example, on the stack or in a struct.
+ * Never **ever** access members directly.
+ */
+
+struct XXH32_state_s {
+
+  XXH32_hash_t total_len_32;
+  XXH32_hash_t large_len;
+  XXH32_hash_t v1;
+  XXH32_hash_t v2;
+  XXH32_hash_t v3;
+  XXH32_hash_t v4;
+  XXH32_hash_t mem32[4];
+  XXH32_hash_t memsize;
+  XXH32_hash_t reserved;                              /* never read nor write, might be removed in a future version */
+
+};                                                                                    /* typedef'd to XXH32_state_t */
+
+  #ifndef XXH_NO_LONG_LONG                                               /* defined when there is no 64-bit support */
+
+struct XXH64_state_s {
+
+  XXH64_hash_t total_len;
+  XXH64_hash_t v1;
+  XXH64_hash_t v2;
+  XXH64_hash_t v3;
+  XXH64_hash_t v4;
+  XXH64_hash_t mem64[4];
+  XXH32_hash_t memsize;
+  XXH32_hash_t reserved32;                                                           /* required for padding anyway */
+  XXH64_hash_t reserved64; /* never read nor write, might be removed in a future
+                              version */
+
+};                                                                                    /* typedef'd to XXH64_state_t */
+
+  /*-**********************************************************************
+   *  XXH3
+   *  New experimental hash
+   ************************************************************************/
+
+  /* ************************************************************************
+   * XXH3 is a new hash algorithm featuring:
+   *  - Improved speed for both small and large inputs
+   *  - True 64-bit and 128-bit outputs
+   *  - SIMD acceleration
+   *  - Improved 32-bit viability
+   *
+   * Speed analysis methodology is explained here:
+   *
+   *    https://fastcompression.blogspot.com/2019/03/presenting-xxh3.html
+   *
+   * In general, expect XXH3 to run about ~2x faster on large inputs and >3x
+   * faster on small ones compared to XXH64, though exact differences depend on
+   * the platform.
+   *
+   * The algorithm is portable: Like XXH32 and XXH64, it generates the same hash
+   * on all platforms.
+   *
+   * It benefits greatly from SIMD and 64-bit arithmetic, but does not require
+   * it.
+   *
+   * Almost all 32-bit and 64-bit targets that can run XXH32 smoothly can run
+   * XXH3 at competitive speeds, even if XXH64 runs slowly. Further details are
+   * explained in the implementation.
+   *
+   * Optimized implementations are provided for AVX512, AVX2, SSE2, NEON,
+   * POWER8, ZVector and scalar targets. This can be controlled with the
+   * XXH_VECTOR macro.
+   *
+   * XXH3 offers 2 variants, _64bits and _128bits.
+   * When only 64 bits are needed, prefer calling the _64bits variant, as it
+   * reduces the amount of mixing, resulting in faster speed on small inputs.
+   *
+   * It's also generally simpler to manipulate a scalar return type than a
+   * struct.
+   *
+   * The 128-bit version adds additional strength, but it is slightly slower.
+   *
+   * The XXH3 algorithm is still in development.
+   * The results it produces may still change in future versions.
+   *
+   * Results produced by v0.7.x are not comparable with results from v0.7.y.
+   * However, the API is completely stable, and it can safely be used for
+   * ephemeral data (local sessions).
+   *
+   * Avoid storing values in long-term storage until the algorithm is finalized.
+   *
+   * Since v0.7.3, XXH3 has reached "release candidate" status, meaning that, if
+   * everything remains fine, its current format will be "frozen" and become the
+   * final one.
+   *
+   * After which, return values of XXH3 and XXH128 will no longer change in
+   * future versions.
+   *
+   * XXH3's return values will be officially finalized upon reaching v0.8.0.
+   *
+   * The API supports one-shot hashing, streaming mode, and custom secrets.
+   */
+
+    #ifdef XXH_NAMESPACE
+      #define XXH3_64bits XXH_NAME2(XXH_NAMESPACE, XXH3_64bits)
+      #define XXH3_64bits_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_withSecret)
+      #define XXH3_64bits_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_withSeed)
+
+      #define XXH3_createState XXH_NAME2(XXH_NAMESPACE, XXH3_createState)
+      #define XXH3_freeState XXH_NAME2(XXH_NAMESPACE, XXH3_freeState)
+      #define XXH3_copyState XXH_NAME2(XXH_NAMESPACE, XXH3_copyState)
+
+      #define XXH3_64bits_reset XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_reset)
+      #define XXH3_64bits_reset_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_reset_withSeed)
+      #define XXH3_64bits_reset_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_reset_withSecret)
+      #define XXH3_64bits_update XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_update)
+      #define XXH3_64bits_digest XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_digest)
+
+      #define XXH3_generateSecret XXH_NAME2(XXH_NAMESPACE, XXH3_generateSecret)
+    #endif
+
+/* XXH3_64bits():
+ * default 64-bit variant, using default secret and default seed of 0.
+ * It's the fastest variant. */
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits(const void *data, size_t len);
+
+/*
+ * XXH3_64bits_withSeed():
+ * This variant generates a custom secret on the fly based on the default
+ * secret, altered using the `seed` value.
+ * While this operation is decently fast, note that it's not completely free.
+ * Note: seed==0 produces the same results as XXH3_64bits().
+ */
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits_withSeed(const void *data, size_t len, XXH64_hash_t seed);
+
+    /*
+     * XXH3_64bits_withSecret():
+     * It's possible to provide any blob of bytes as a "secret" to generate the
+     * hash. This makes it more difficult for an external actor to prepare an
+     * intentional collision. secretSize *must* be large enough (>=
+     * XXH3_SECRET_SIZE_MIN). The hash quality depends on the secret's high
+     * entropy, meaning that the secret should look like a bunch of random
+     * bytes. Avoid "trivial" sequences such as text or a bunch of repeated
+     * characters. If you are unsure of the "randonmess" of the blob of bytes,
+     * consider making it a "custom seed" instead,
+     * and use "XXH_generateSecret()" to generate a high quality secret.
+     */
+    #define XXH3_SECRET_SIZE_MIN 136
+XXH_PUBLIC_API XXH64_hash_t XXH3_64bits_withSecret(const void *data, size_t len, const void *secret, size_t secretSize);
+
+  /* streaming 64-bit */
+
+    #if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L)                                          /* C11+ */
+      #include <stdalign.h>
+      #define XXH_ALIGN(n) alignas(n)
+    #elif defined(__GNUC__)
+      #define XXH_ALIGN(n) __attribute__((aligned(n)))
+    #elif defined(_MSC_VER)
+      #define XXH_ALIGN(n) __declspec(align(n))
+    #else
+      #define XXH_ALIGN(n)                                                                              /* disabled */
+    #endif
+
+    /* Old GCC versions only accept the attribute after the type in structures.
+     */
+    #if !(defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L)) /* C11+ */ \
+        && defined(__GNUC__)
+      #define XXH_ALIGN_MEMBER(align, type) type XXH_ALIGN(align)
+    #else
+      #define XXH_ALIGN_MEMBER(align, type) XXH_ALIGN(align) type
+    #endif
+
+typedef struct XXH3_state_s XXH3_state_t;
+
+    #define XXH3_INTERNALBUFFER_SIZE 256
+    #define XXH3_SECRET_DEFAULT_SIZE 192
+struct XXH3_state_s {
+
+  XXH_ALIGN_MEMBER(64, XXH64_hash_t acc[8]);
+  /* used to store a custom secret generated from a seed */
+  XXH_ALIGN_MEMBER(64, unsigned char customSecret[XXH3_SECRET_DEFAULT_SIZE]);
+  XXH_ALIGN_MEMBER(64, unsigned char buffer[XXH3_INTERNALBUFFER_SIZE]);
+  XXH32_hash_t         bufferedSize;
+  XXH32_hash_t         reserved32;
+  size_t               nbStripesPerBlock;
+  size_t               nbStripesSoFar;
+  size_t               secretLimit;
+  XXH64_hash_t         totalLen;
+  XXH64_hash_t         seed;
+  XXH64_hash_t         reserved64;
+  const unsigned char *extSecret; /* reference to external secret;
+                                   * if == NULL, use .customSecret instead */
+  /* note: there may be some padding at the end due to alignment on 64 bytes */
+
+};                                                                                     /* typedef'd to XXH3_state_t */
+
+    #undef XXH_ALIGN_MEMBER
+
+/*
+ * Streaming requires state maintenance.
+ * This operation costs memory and CPU.
+ * As a consequence, streaming is slower than one-shot hashing.
+ * For better performance, prefer one-shot functions whenever possible.
+ */
+XXH_PUBLIC_API XXH3_state_t *XXH3_createState(void);
+XXH_PUBLIC_API XXH_errorcode XXH3_freeState(XXH3_state_t *statePtr);
+XXH_PUBLIC_API void          XXH3_copyState(XXH3_state_t *dst_state, const XXH3_state_t *src_state);
+
+/*
+ * XXH3_64bits_reset():
+ * Initialize with the default parameters.
+ * The result will be equivalent to `XXH3_64bits()`.
+ */
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset(XXH3_state_t *statePtr);
+/*
+ * XXH3_64bits_reset_withSeed():
+ * Generate a custom secret from `seed`, and store it into `statePtr`.
+ * digest will be equivalent to `XXH3_64bits_withSeed()`.
+ */
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset_withSeed(XXH3_state_t *statePtr, XXH64_hash_t seed);
+/*
+ * XXH3_64bits_reset_withSecret():
+ * `secret` is referenced, and must outlive the hash streaming session, so
+ * be careful when using stack arrays.
+ * `secretSize` must be >= `XXH3_SECRET_SIZE_MIN`.
+ */
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset_withSecret(XXH3_state_t *statePtr, const void *secret,
+                                                          size_t secretSize);
+
+XXH_PUBLIC_API XXH_errorcode XXH3_64bits_update(XXH3_state_t *statePtr, const void *input, size_t length);
+XXH_PUBLIC_API XXH64_hash_t  XXH3_64bits_digest(const XXH3_state_t *statePtr);
+
+  /* 128-bit */
+
+    #ifdef XXH_NAMESPACE
+      #define XXH128 XXH_NAME2(XXH_NAMESPACE, XXH128)
+      #define XXH3_128bits XXH_NAME2(XXH_NAMESPACE, XXH3_128bits)
+      #define XXH3_128bits_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_withSeed)
+      #define XXH3_128bits_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_withSecret)
+
+      #define XXH3_128bits_reset XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_reset)
+      #define XXH3_128bits_reset_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_reset_withSeed)
+      #define XXH3_128bits_reset_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_reset_withSecret)
+      #define XXH3_128bits_update XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_update)
+      #define XXH3_128bits_digest XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_digest)
+
+      #define XXH128_isEqual XXH_NAME2(XXH_NAMESPACE, XXH128_isEqual)
+      #define XXH128_cmp XXH_NAME2(XXH_NAMESPACE, XXH128_cmp)
+      #define XXH128_canonicalFromHash XXH_NAME2(XXH_NAMESPACE, XXH128_canonicalFromHash)
+      #define XXH128_hashFromCanonical XXH_NAME2(XXH_NAMESPACE, XXH128_hashFromCanonical)
+    #endif
+
+typedef struct {
+
+  XXH64_hash_t low64;
+  XXH64_hash_t high64;
+
+} XXH128_hash_t;
+
+XXH_PUBLIC_API XXH128_hash_t XXH128(const void *data, size_t len, XXH64_hash_t seed);
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits(const void *data, size_t len);
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits_withSeed(const void *data, size_t len, XXH64_hash_t seed); /* == XXH128() */
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits_withSecret(const void *data, size_t len, const void *secret,
+                                                     size_t secretSize);
+
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset(XXH3_state_t *statePtr);
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset_withSeed(XXH3_state_t *statePtr, XXH64_hash_t seed);
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset_withSecret(XXH3_state_t *statePtr, const void *secret,
+                                                           size_t secretSize);
+
+XXH_PUBLIC_API XXH_errorcode XXH3_128bits_update(XXH3_state_t *statePtr, const void *input, size_t length);
+XXH_PUBLIC_API XXH128_hash_t XXH3_128bits_digest(const XXH3_state_t *statePtr);
+
+/* Note: For better performance, these functions can be inlined using
+ * XXH_INLINE_ALL */
+
+/*!
+ * XXH128_isEqual():
+ * Return: 1 if `h1` and `h2` are equal, 0 if they are not.
+ */
+XXH_PUBLIC_API int XXH128_isEqual(XXH128_hash_t h1, XXH128_hash_t h2);
+
+/*!
+ * XXH128_cmp():
+ *
+ * This comparator is compatible with stdlib's `qsort()`/`bsearch()`.
+ *
+ * return: >0 if *h128_1  > *h128_2
+ *         =0 if *h128_1 == *h128_2
+ *         <0 if *h128_1  < *h128_2
+ */
+XXH_PUBLIC_API int XXH128_cmp(const void *h128_1, const void *h128_2);
+
+/*******   Canonical representation   *******/
+typedef struct {
+
+  unsigned char digest[sizeof(XXH128_hash_t)];
+
+} XXH128_canonical_t;
+
+XXH_PUBLIC_API void          XXH128_canonicalFromHash(XXH128_canonical_t *dst, XXH128_hash_t hash);
+XXH_PUBLIC_API XXH128_hash_t XXH128_hashFromCanonical(const XXH128_canonical_t *src);
+
+/* ===   Experimental API   === */
+/* Symbols defined below must be considered tied to a specific library version.
+ */
+
+/*
+ * XXH3_generateSecret():
+ *
+ * Derive a secret for use with `*_withSecret()` prototypes of XXH3.
+ * Use this if you need a higher level of security than the one provided by
+ * 64bit seed.
+ *
+ * Take as input a custom seed of any length and any content,
+ * generate from it a high-entropy secret of length XXH3_SECRET_DEFAULT_SIZE
+ * into already allocated buffer secretBuffer.
+ * The generated secret ALWAYS is XXH_SECRET_DEFAULT_SIZE bytes long.
+ *
+ * The generated secret can then be used with any `*_withSecret()` variant.
+ * The functions `XXH3_128bits_withSecret()`, `XXH3_64bits_withSecret()`,
+ * `XXH3_128bits_reset_withSecret()` and `XXH3_64bits_reset_withSecret()`
+ * are part of this list. They all accept a `secret` parameter
+ * which must be very long for implementation reasons (>= XXH3_SECRET_SIZE_MIN)
+ * _and_ feature very high entropy (consist of random-looking bytes).
+ * These conditions can be a high bar to meet, so
+ * this function can be used to generate a secret of proper quality.
+ *
+ * customSeed can be anything. It can have any size, even small ones,
+ * and its content can be anything, even some "low entropy" source such as a
+ * bunch of zeroes. The resulting `secret` will nonetheless respect all expected
+ * qualities.
+ *
+ * Supplying NULL as the customSeed copies the default secret into
+ * `secretBuffer`. When customSeedSize > 0, supplying NULL as customSeed is
+ * undefined behavior.
+ */
+XXH_PUBLIC_API void XXH3_generateSecret(void *secretBuffer, const void *customSeed, size_t customSeedSize);
+
+  #endif                                                                                        /* XXH_NO_LONG_LONG */
+
+  #if defined(XXH_INLINE_ALL) || defined(XXH_PRIVATE_API)
+    #define XXH_IMPLEMENTATION
+  #endif
+
+#endif /* defined(XXH_STATIC_LINKING_ONLY) && \
+          !defined(XXHASH_H_STATIC_13879238742) */
+
+/* ======================================================================== */
+/* ======================================================================== */
+/* ======================================================================== */
+
+/*-**********************************************************************
+ * xxHash implementation
+ *-**********************************************************************
+ * xxHash's implementation used to be found in xxhash.c.
+ *
+ * However, code inlining requires the implementation to be visible to the
+ * compiler, usually within the header.
+ *
+ * As a workaround, xxhash.c used to be included within xxhash.h. This caused
+ * some issues with some build systems, especially ones which treat .c files
+ * as source files.
+ *
+ * Therefore, the implementation is now directly integrated within xxhash.h.
+ * Another small advantage is that xxhash.c is no longer needed in /include.
+ ************************************************************************/
+
+#if (defined(XXH_INLINE_ALL) || defined(XXH_PRIVATE_API) || defined(XXH_IMPLEMENTATION)) && \
+    !defined(XXH_IMPLEM_13a8737387)
+  #define XXH_IMPLEM_13a8737387
+
+  /* *************************************
+   *  Tuning parameters
+   ***************************************/
+  /*!
+   * XXH_FORCE_MEMORY_ACCESS:
+   * By default, access to unaligned memory is controlled by `memcpy()`, which
+   * is safe and portable.
+   *
+   * Unfortunately, on some target/compiler combinations, the generated assembly
+   * is sub-optimal.
+   *
+   * The below switch allow to select a different access method for improved
+   * performance.
+   * Method 0 (default):
+   *     Use `memcpy()`. Safe and portable.
+   * Method 1:
+   *     `__attribute__((packed))` statement. It depends on compiler extensions
+   *     and is therefore not portable.
+   *     This method is safe if your compiler supports it, and *generally* as
+   *     fast or faster than `memcpy`.
+   * Method 2:
+   *     Direct access via cast. This method doesn't depend on the compiler but
+   *     violates the C standard.
+   *     It can generate buggy code on targets which do not support unaligned
+   *     memory accesses.
+   *     But in some circumstances, it's the only known way to get the most
+   *     performance (ie GCC + ARMv6)
+   * Method 3:
+   *     Byteshift. This can generate the best code on old compilers which don't
+   *     inline small `memcpy()` calls, and it might also be faster on
+   * big-endian systems which lack a native byteswap instruction. See
+   * https://stackoverflow.com/a/32095106/646947 for details. Prefer these
+   * methods in priority order (0 > 1 > 2 > 3)
+   */
+  #ifndef XXH_FORCE_MEMORY_ACCESS /* can be defined externally, on command \
+                                     line for example */
+    #if !defined(__clang__) && defined(__GNUC__) && defined(__ARM_FEATURE_UNALIGNED) && defined(__ARM_ARCH) && \
+        (__ARM_ARCH == 6)
+      #define XXH_FORCE_MEMORY_ACCESS 2
+    #elif !defined(__clang__) && ((defined(__INTEL_COMPILER) && !defined(_WIN32)) || \
+                                  (defined(__GNUC__) && (defined(__ARM_ARCH) && __ARM_ARCH >= 7)))
+      #define XXH_FORCE_MEMORY_ACCESS 1
+    #endif
+  #endif
+
+  /*!
+   * XXH_ACCEPT_NULL_INPUT_POINTER:
+   * If the input pointer is NULL, xxHash's default behavior is to dereference
+   * it, triggering a segfault. When this macro is enabled, xxHash actively
+   * checks the input for a null pointer. If it is, the result for null input
+   * pointers is the same as a zero-length input.
+   */
+  #ifndef XXH_ACCEPT_NULL_INPUT_POINTER                                                /* can be defined externally */
+    #define XXH_ACCEPT_NULL_INPUT_POINTER 0
+  #endif
+
+  /*!
+   * XXH_FORCE_ALIGN_CHECK:
+   * This is an important performance trick
+   * for architectures without decent unaligned memory access performance.
+   * It checks for input alignment, and when conditions are met,
+   * uses a "fast path" employing direct 32-bit/64-bit read,
+   * resulting in _dramatically faster_ read speed.
+   *
+   * The check costs one initial branch per hash, which is generally negligible,
+   * but not zero. Moreover, it's not useful to generate binary for an
+   * additional code path if memory access uses same instruction for both
+   * aligned and unaligned adresses.
+   *
+   * In these cases, the alignment check can be removed by setting this macro to
+   * 0. Then the code will always use unaligned memory access. Align check is
+   * automatically disabled on x86, x64 & arm64, which are platforms known to
+   * offer good unaligned memory accesses performance.
+   *
+   * This option does not affect XXH3 (only XXH32 and XXH64).
+   */
+  #ifndef XXH_FORCE_ALIGN_CHECK                                                        /* can be defined externally */
+    #if defined(__i386) || defined(__x86_64__) || defined(__aarch64__) || defined(_M_IX86) || defined(_M_X64) || \
+        defined(_M_ARM64)                                                                                 /* visual */
+      #define XXH_FORCE_ALIGN_CHECK 0
+    #else
+      #define XXH_FORCE_ALIGN_CHECK 1
+    #endif
+  #endif
+
+  /*!
+   * XXH_NO_INLINE_HINTS:
+   *
+   * By default, xxHash tries to force the compiler to inline almost all
+   * internal functions.
+   *
+   * This can usually improve performance due to reduced jumping and improved
+   * constant folding, but significantly increases the size of the binary which
+   * might not be favorable.
+   *
+   * Additionally, sometimes the forced inlining can be detrimental to
+   * performance, depending on the architecture.
+   *
+   * XXH_NO_INLINE_HINTS marks all internal functions as static, giving the
+   * compiler full control on whether to inline or not.
+   *
+   * When not optimizing (-O0), optimizing for size (-Os, -Oz), or using
+   * -fno-inline with GCC or Clang, this will automatically be defined.
+   */
+  #ifndef XXH_NO_INLINE_HINTS
+    #if defined(__OPTIMIZE_SIZE__) || defined(__NO_INLINE__)
+      #define XXH_NO_INLINE_HINTS 1
+    #else
+      #define XXH_NO_INLINE_HINTS 0
+    #endif
+  #endif
+
+  /*!
+   * XXH_REROLL:
+   * Whether to reroll XXH32_finalize, and XXH64_finalize,
+   * instead of using an unrolled jump table/if statement loop.
+   *
+   * This is automatically defined on -Os/-Oz on GCC and Clang.
+   */
+  #ifndef XXH_REROLL
+    #if defined(__OPTIMIZE_SIZE__)
+      #define XXH_REROLL 1
+    #else
+      #define XXH_REROLL 0
+    #endif
+  #endif
+
+  /* *************************************
+   *  Includes & Memory related functions
+   ***************************************/
+  /*!
+   * Modify the local functions below should you wish to use some other memory
+   * routines for malloc() and free()
+   */
+  #include <stdlib.h>
+
+static void *XXH_malloc(size_t s) {
+
+  return malloc(s);
+
+}
+
+static void XXH_free(void *p) {
+
+  free(p);
+
+}
+
+  /*! and for memcpy() */
+  #include <string.h>
+static void *XXH_memcpy(void *dest, const void *src, size_t size) {
+
+  return memcpy(dest, src, size);
+
+}
+
+  #include <limits.h>                                                                                 /* ULLONG_MAX */
+
+  /* *************************************
+   *  Compiler Specific Options
+   ***************************************/
+  #ifdef _MSC_VER                                                                      /* Visual Studio warning fix */
+    #pragma warning(disable : 4127) /* disable: C4127: conditional expression \
+                                       is constant */
+  #endif
+
+  #if XXH_NO_INLINE_HINTS                                                                 /* disable inlining hints */
+    #if defined(__GNUC__)
+      #define XXH_FORCE_INLINE static __attribute__((unused))
+    #else
+      #define XXH_FORCE_INLINE static
+    #endif
+    #define XXH_NO_INLINE static
+  /* enable inlining hints */
+  #elif defined(_MSC_VER)                                                                          /* Visual Studio */
+    #define XXH_FORCE_INLINE static __forceinline
+    #define XXH_NO_INLINE static __declspec(noinline)
+  #elif defined(__GNUC__)
+    #define XXH_FORCE_INLINE static __inline__ __attribute__((always_inline, unused))
+    #define XXH_NO_INLINE static __attribute__((noinline))
+  #elif defined(__cplusplus) || (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L))                 /* C99 */
+    #define XXH_FORCE_INLINE static inline
+    #define XXH_NO_INLINE static
+  #else
+    #define XXH_FORCE_INLINE static
+    #define XXH_NO_INLINE static
+  #endif
+
+  /* *************************************
+   *  Debug
+   ***************************************/
+  /*
+   * XXH_DEBUGLEVEL is expected to be defined externally, typically via the
+   * compiler's command line options. The value must be a number.
+   */
+  #ifndef XXH_DEBUGLEVEL
+    #ifdef DEBUGLEVEL                                                                           /* backwards compat */
+      #define XXH_DEBUGLEVEL DEBUGLEVEL
+    #else
+      #define XXH_DEBUGLEVEL 0
+    #endif
+  #endif
+
+  #if (XXH_DEBUGLEVEL >= 1)
+    #include <assert.h>                                                  /* note: can still be disabled with NDEBUG */
+    #define XXH_ASSERT(c) assert(c)
+  #else
+    #define XXH_ASSERT(c) ((void)0)
+  #endif
+
+  /* note: use after variable declarations */
+  #define XXH_STATIC_ASSERT(c)            \
+    do {                                  \
+                                          \
+      enum { XXH_sa = 1 / (int)(!!(c)) }; \
+                                          \
+    } while (0)
+
+  /* *************************************
+   *  Basic Types
+   ***************************************/
+  #if !defined(__VMS) && \
+      (defined(__cplusplus) || (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */))
+    #include <stdint.h>
+typedef uint8_t xxh_u8;
+  #else
+typedef unsigned char xxh_u8;
+  #endif
+typedef XXH32_hash_t xxh_u32;
+
+  #ifdef XXH_OLD_NAMES
+    #define BYTE xxh_u8
+    #define U8 xxh_u8
+    #define U32 xxh_u32
+  #endif
+
+/* ***   Memory access   *** */
+
+  #if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 3))
+  /*
+   * Manual byteshift. Best for old compilers which don't inline memcpy.
+   * We actually directly use XXH_readLE32 and XXH_readBE32.
+   */
+  #elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 2))
+
+/*
+ * Force direct memory access. Only works on CPU which support unaligned memory
+ * access in hardware.
+ */
+static xxh_u32 XXH_read32(const void *memPtr) {
+
+  return *(const xxh_u32 *)memPtr;
+
+}
+
+  #elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 1))
+
+    /*
+     * __pack instructions are safer but compiler specific, hence potentially
+     * problematic for some compilers.
+     *
+     * Currently only defined for GCC and ICC.
+     */
+    #ifdef XXH_OLD_NAMES
+typedef union {
+
+  xxh_u32 u32;
+
+} __attribute__((packed)) unalign;
+
+    #endif
+static xxh_u32 XXH_read32(const void *ptr) {
+
+  typedef union {
+
+    xxh_u32 u32;
+
+  } __attribute__((packed)) xxh_unalign;
+
+  return ((const xxh_unalign *)ptr)->u32;
+
+}
+
+  #else
+
+/*
+ * Portable and safe solution. Generally efficient.
+ * see: https://stackoverflow.com/a/32095106/646947
+ */
+static xxh_u32 XXH_read32(const void *memPtr) {
+
+  xxh_u32 val;
+  memcpy(&val, memPtr, sizeof(val));
+  return val;
+
+}
+
+  #endif                                                                          /* XXH_FORCE_DIRECT_MEMORY_ACCESS */
+
+/* ***   Endianess   *** */
+typedef enum { XXH_bigEndian = 0, XXH_littleEndian = 1 } XXH_endianess;
+
+  /*!
+   * XXH_CPU_LITTLE_ENDIAN:
+   * Defined to 1 if the target is little endian, or 0 if it is big endian.
+   * It can be defined externally, for example on the compiler command line.
+   *
+   * If it is not defined, a runtime check (which is usually constant folded)
+   * is used instead.
+   */
+  #ifndef XXH_CPU_LITTLE_ENDIAN
+    /*
+     * Try to detect endianness automatically, to avoid the nonstandard behavior
+     * in `XXH_isLittleEndian()`
+     */
+    #if defined(_WIN32) || defined(__LITTLE_ENDIAN__) || \
+        (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
+      #define XXH_CPU_LITTLE_ENDIAN 1
+    #elif defined(__BIG_ENDIAN__) || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)
+      #define XXH_CPU_LITTLE_ENDIAN 0
+    #else
+/*
+ * runtime test, presumed to simplify to a constant by compiler
+ */
+static int XXH_isLittleEndian(void) {
+
+  /*
+   * Portable and well-defined behavior.
+   * Don't use static: it is detrimental to performance.
+   */
+  const union {
+
+    xxh_u32 u;
+    xxh_u8  c[4];
+
+  } one = {1};
+
+  return one.c[0];
+
+}
+
+      #define XXH_CPU_LITTLE_ENDIAN XXH_isLittleEndian()
+    #endif
+  #endif
+
+  /* ****************************************
+   *  Compiler-specific Functions and Macros
+   ******************************************/
+  #define XXH_GCC_VERSION (__GNUC__ * 100 + __GNUC_MINOR__)
+
+  #ifdef __has_builtin
+    #define XXH_HAS_BUILTIN(x) __has_builtin(x)
+  #else
+    #define XXH_HAS_BUILTIN(x) 0
+  #endif
+
+  #if !defined(NO_CLANG_BUILTIN) && XXH_HAS_BUILTIN(__builtin_rotateleft32) && XXH_HAS_BUILTIN(__builtin_rotateleft64)
+    #define XXH_rotl32 __builtin_rotateleft32
+    #define XXH_rotl64 __builtin_rotateleft64
+  /* Note: although _rotl exists for minGW (GCC under windows), performance
+   * seems poor */
+  #elif defined(_MSC_VER)
+    #define XXH_rotl32(x, r) _rotl(x, r)
+    #define XXH_rotl64(x, r) _rotl64(x, r)
+  #else
+    #define XXH_rotl32(x, r) (((x) << (r)) | ((x) >> (32 - (r))))
+    #define XXH_rotl64(x, r) (((x) << (r)) | ((x) >> (64 - (r))))
+  #endif
+
+  #if defined(_MSC_VER)                                                                            /* Visual Studio */
+    #define XXH_swap32 _byteswap_ulong
+  #elif XXH_GCC_VERSION >= 403
+    #define XXH_swap32 __builtin_bswap32
+  #else
+static xxh_u32 XXH_swap32(xxh_u32 x) {
+
+  return ((x << 24) & 0xff000000) | ((x << 8) & 0x00ff0000) | ((x >> 8) & 0x0000ff00) | ((x >> 24) & 0x000000ff);
+
+}
+
+  #endif
+
+/* ***************************
+ *  Memory reads
+ *****************************/
+typedef enum { XXH_aligned, XXH_unaligned } XXH_alignment;
+
+  /*
+   * XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load.
+   *
+   * This is ideal for older compilers which don't inline memcpy.
+   */
+  #if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 3))
+
+XXH_FORCE_INLINE xxh_u32 XXH_readLE32(const void *memPtr) {
+
+  const xxh_u8 *bytePtr = (const xxh_u8 *)memPtr;
+  return bytePtr[0] | ((xxh_u32)bytePtr[1] << 8) | ((xxh_u32)bytePtr[2] << 16) | ((xxh_u32)bytePtr[3] << 24);
+
+}
+
+XXH_FORCE_INLINE xxh_u32 XXH_readBE32(const void *memPtr) {
+
+  const xxh_u8 *bytePtr = (const xxh_u8 *)memPtr;
+  return bytePtr[3] | ((xxh_u32)bytePtr[2] << 8) | ((xxh_u32)bytePtr[1] << 16) | ((xxh_u32)bytePtr[0] << 24);
+
+}
+
+  #else
+XXH_FORCE_INLINE xxh_u32 XXH_readLE32(const void *ptr) {
+
+  return XXH_CPU_LITTLE_ENDIAN ? XXH_read32(ptr) : XXH_swap32(XXH_read32(ptr));
+
+}
+
+static xxh_u32 XXH_readBE32(const void *ptr) {
+
+  return XXH_CPU_LITTLE_ENDIAN ? XXH_swap32(XXH_read32(ptr)) : XXH_read32(ptr);
+
+}
+
+  #endif
+
+XXH_FORCE_INLINE xxh_u32 XXH_readLE32_align(const void *ptr, XXH_alignment align) {
+
+  if (align == XXH_unaligned) {
+
+    return XXH_readLE32(ptr);
+
+  } else {
+
+    return XXH_CPU_LITTLE_ENDIAN ? *(const xxh_u32 *)ptr : XXH_swap32(*(const xxh_u32 *)ptr);
+
+  }
+
+}
+
+/* *************************************
+ *  Misc
+ ***************************************/
+XXH_PUBLIC_API unsigned XXH_versionNumber(void) {
+
+  return XXH_VERSION_NUMBER;
+
+}
+
+/* *******************************************************************
+ *  32-bit hash functions
+ *********************************************************************/
+static const xxh_u32 XXH_PRIME32_1 = 0x9E3779B1U;                             /* 0b10011110001101110111100110110001 */
+static const xxh_u32 XXH_PRIME32_2 = 0x85EBCA77U;                             /* 0b10000101111010111100101001110111 */
+static const xxh_u32 XXH_PRIME32_3 = 0xC2B2AE3DU;                             /* 0b11000010101100101010111000111101 */
+static const xxh_u32 XXH_PRIME32_4 = 0x27D4EB2FU;                             /* 0b00100111110101001110101100101111 */
+static const xxh_u32 XXH_PRIME32_5 = 0x165667B1U;                             /* 0b00010110010101100110011110110001 */
+
+  #ifdef XXH_OLD_NAMES
+    #define PRIME32_1 XXH_PRIME32_1
+    #define PRIME32_2 XXH_PRIME32_2
+    #define PRIME32_3 XXH_PRIME32_3
+    #define PRIME32_4 XXH_PRIME32_4
+    #define PRIME32_5 XXH_PRIME32_5
+  #endif
+
+static xxh_u32 XXH32_round(xxh_u32 acc, xxh_u32 input) {
+
+  acc += input * XXH_PRIME32_2;
+  acc = XXH_rotl32(acc, 13);
+  acc *= XXH_PRIME32_1;
+  #if defined(__GNUC__) && defined(__SSE4_1__) && !defined(XXH_ENABLE_AUTOVECTORIZE)
+  /*
+   * UGLY HACK:
+   * This inline assembly hack forces acc into a normal register. This is the
+   * only thing that prevents GCC and Clang from autovectorizing the XXH32
+   * loop (pragmas and attributes don't work for some resason) without globally
+   * disabling SSE4.1.
+   *
+   * The reason we want to avoid vectorization is because despite working on
+   * 4 integers at a time, there are multiple factors slowing XXH32 down on
+   * SSE4:
+   * - There's a ridiculous amount of lag from pmulld (10 cycles of latency on
+   *   newer chips!) making it slightly slower to multiply four integers at
+   *   once compared to four integers independently. Even when pmulld was
+   *   fastest, Sandy/Ivy Bridge, it is still not worth it to go into SSE
+   *   just to multiply unless doing a long operation.
+   *
+   * - Four instructions are required to rotate,
+   *      movqda tmp,  v // not required with VEX encoding
+   *      pslld  tmp, 13 // tmp <<= 13
+   *      psrld  v,   19 // x >>= 19
+   *      por    v,  tmp // x |= tmp
+   *   compared to one for scalar:
+   *      roll   v, 13    // reliably fast across the board
+   *      shldl  v, v, 13 // Sandy Bridge and later prefer this for some reason
+   *
+   * - Instruction level parallelism is actually more beneficial here because
+   *   the SIMD actually serializes this operation: While v1 is rotating, v2
+   *   can load data, while v3 can multiply. SSE forces them to operate
+   *   together.
+   *
+   * How this hack works:
+   * __asm__(""       // Declare an assembly block but don't declare any
+   * instructions :       // However, as an Input/Output Operand,
+   *          "+r"    // constrain a read/write operand (+) as a general purpose
+   * register (r). (acc)   // and set acc as the operand
+   * );
+   *
+   * Because of the 'r', the compiler has promised that seed will be in a
+   * general purpose register and the '+' says that it will be 'read/write',
+   * so it has to assume it has changed. It is like volatile without all the
+   * loads and stores.
+   *
+   * Since the argument has to be in a normal register (not an SSE register),
+   * each time XXH32_round is called, it is impossible to vectorize.
+   */
+  __asm__("" : "+r"(acc));
+  #endif
+  return acc;
+
+}
+
+/* mix all bits */
+static xxh_u32 XXH32_avalanche(xxh_u32 h32) {
+
+  h32 ^= h32 >> 15;
+  h32 *= XXH_PRIME32_2;
+  h32 ^= h32 >> 13;
+  h32 *= XXH_PRIME32_3;
+  h32 ^= h32 >> 16;
+  return (h32);
+
+}
+
+  #define XXH_get32bits(p) XXH_readLE32_align(p, align)
+
+static xxh_u32 XXH32_finalize(xxh_u32 h32, const xxh_u8 *ptr, size_t len, XXH_alignment align) {
+
+  /* dummy comment */
+
+  #define XXH_PROCESS1                           \
+    do {                                         \
+                                                 \
+      h32 += (*ptr++) * XXH_PRIME32_5;           \
+      h32 = XXH_rotl32(h32, 11) * XXH_PRIME32_1; \
+                                                 \
+    } while (0)
+
+  #define XXH_PROCESS4                           \
+    do {                                         \
+                                                 \
+      h32 += XXH_get32bits(ptr) * XXH_PRIME32_3; \
+      ptr += 4;                                  \
+      h32 = XXH_rotl32(h32, 17) * XXH_PRIME32_4; \
+                                                 \
+    } while (0)
+
+  /* Compact rerolled version */
+  if (XXH_REROLL) {
+
+    len &= 15;
+    while (len >= 4) {
+
+      XXH_PROCESS4;
+      len -= 4;
+
+    }
+
+    while (len > 0) {
+
+      XXH_PROCESS1;
+      --len;
+
+    }
+
+    return XXH32_avalanche(h32);
+
+  } else {
+
+    switch (len & 15) /* or switch(bEnd - p) */ {
+
+      case 12:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 8:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 4:
+        XXH_PROCESS4;
+        return XXH32_avalanche(h32);
+
+      case 13:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 9:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 5:
+        XXH_PROCESS4;
+        XXH_PROCESS1;
+        return XXH32_avalanche(h32);
+
+      case 14:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 10:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 6:
+        XXH_PROCESS4;
+        XXH_PROCESS1;
+        XXH_PROCESS1;
+        return XXH32_avalanche(h32);
+
+      case 15:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 11:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 7:
+        XXH_PROCESS4;
+        /* fallthrough */
+      case 3:
+        XXH_PROCESS1;
+        /* fallthrough */
+      case 2:
+        XXH_PROCESS1;
+        /* fallthrough */
+      case 1:
+        XXH_PROCESS1;
+        /* fallthrough */
+      case 0:
+        return XXH32_avalanche(h32);
+
+    }
+
+    XXH_ASSERT(0);
+    return h32;                                                         /* reaching this point is deemed impossible */
+
+  }
+
+}
+
+  #ifdef XXH_OLD_NAMES
+    #define PROCESS1 XXH_PROCESS1
+    #define PROCESS4 XXH_PROCESS4
+  #else
+    #undef XXH_PROCESS1
+    #undef XXH_PROCESS4
+  #endif
+
+XXH_FORCE_INLINE xxh_u32 XXH32_endian_align(const xxh_u8 *input, size_t len, xxh_u32 seed, XXH_alignment align) {
+
+  const xxh_u8 *bEnd = input + len;
+  xxh_u32       h32;
+
+  #if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER >= 1)
+  if (input == NULL) {
+
+    len = 0;
+    bEnd = input = (const xxh_u8 *)(size_t)16;
+
+  }
+
+  #endif
+
+  if (len >= 16) {
+
+    const xxh_u8 *const limit = bEnd - 15;
+    xxh_u32             v1 = seed + XXH_PRIME32_1 + XXH_PRIME32_2;
+    xxh_u32             v2 = seed + XXH_PRIME32_2;
+    xxh_u32             v3 = seed + 0;
+    xxh_u32             v4 = seed - XXH_PRIME32_1;
+
+    do {
+
+      v1 = XXH32_round(v1, XXH_get32bits(input));
+      input += 4;
+      v2 = XXH32_round(v2, XXH_get32bits(input));
+      input += 4;
+      v3 = XXH32_round(v3, XXH_get32bits(input));
+      input += 4;
+      v4 = XXH32_round(v4, XXH_get32bits(input));
+      input += 4;
+
+    } while (input < limit);
+
+    h32 = XXH_rotl32(v1, 1) + XXH_rotl32(v2, 7) + XXH_rotl32(v3, 12) + XXH_rotl32(v4, 18);
+
+  } else {
+
+    h32 = seed + XXH_PRIME32_5;
+
+  }
+
+  h32 += (xxh_u32)len;
+
+  return XXH32_finalize(h32, input, len & 15, align);
+
+}
+
+XXH_PUBLIC_API XXH32_hash_t XXH32(const void *input, size_t len, XXH32_hash_t seed) {
+
+  #if 0
+    /* Simple version, good for code maintenance, but unfortunately slow for small inputs */
+    XXH32_state_t state;
+    XXH32_reset(&state, seed);
+    XXH32_update(&state, (const xxh_u8*)input, len);
+    return XXH32_digest(&state);
+
+  #else
+
+  if (XXH_FORCE_ALIGN_CHECK) {
+
+    if ((((size_t)input) & 3) == 0) {                       /* Input is 4-bytes aligned, leverage the speed benefit */
+      return XXH32_endian_align((const xxh_u8 *)input, len, seed, XXH_aligned);
+
+    }
+
+  }
+
+  return XXH32_endian_align((const xxh_u8 *)input, len, seed, XXH_unaligned);
+  #endif
+
+}
+
+/*******   Hash streaming   *******/
+
+XXH_PUBLIC_API XXH32_state_t *XXH32_createState(void) {
+
+  return (XXH32_state_t *)XXH_malloc(sizeof(XXH32_state_t));
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH32_freeState(XXH32_state_t *statePtr) {
+
+  XXH_free(statePtr);
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API void XXH32_copyState(XXH32_state_t *dstState, const XXH32_state_t *srcState) {
+
+  memcpy(dstState, srcState, sizeof(*dstState));
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH32_reset(XXH32_state_t *statePtr, XXH32_hash_t seed) {
+
+  XXH32_state_t state; /* using a local state to memcpy() in order to avoid
+                          strict-aliasing warnings */
+  memset(&state, 0, sizeof(state));
+  state.v1 = seed + XXH_PRIME32_1 + XXH_PRIME32_2;
+  state.v2 = seed + XXH_PRIME32_2;
+  state.v3 = seed + 0;
+  state.v4 = seed - XXH_PRIME32_1;
+  /* do not write into reserved, planned to be removed in a future version */
+  memcpy(statePtr, &state, sizeof(state) - sizeof(state.reserved));
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH32_update(XXH32_state_t *state, const void *input, size_t len) {
+
+  if (input == NULL)
+  #if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER >= 1)
+    return XXH_OK;
+  #else
+    return XXH_ERROR;
+  #endif
+
+  {
+
+    const xxh_u8 *      p = (const xxh_u8 *)input;
+    const xxh_u8 *const bEnd = p + len;
+
+    state->total_len_32 += (XXH32_hash_t)len;
+    state->large_len |= (XXH32_hash_t)((len >= 16) | (state->total_len_32 >= 16));
+
+    if (state->memsize + len < 16) {                                                          /* fill in tmp buffer */
+      XXH_memcpy((xxh_u8 *)(state->mem32) + state->memsize, input, len);
+      state->memsize += (XXH32_hash_t)len;
+      return XXH_OK;
+
+    }
+
+    if (state->memsize) {                                                    /* some data left from previous update */
+      XXH_memcpy((xxh_u8 *)(state->mem32) + state->memsize, input, 16 - state->memsize);
+      {
+
+        const xxh_u32 *p32 = state->mem32;
+        state->v1 = XXH32_round(state->v1, XXH_readLE32(p32));
+        p32++;
+        state->v2 = XXH32_round(state->v2, XXH_readLE32(p32));
+        p32++;
+        state->v3 = XXH32_round(state->v3, XXH_readLE32(p32));
+        p32++;
+        state->v4 = XXH32_round(state->v4, XXH_readLE32(p32));
+
+      }
+
+      p += 16 - state->memsize;
+      state->memsize = 0;
+
+    }
+
+    if (p <= bEnd - 16) {
+
+      const xxh_u8 *const limit = bEnd - 16;
+      xxh_u32             v1 = state->v1;
+      xxh_u32             v2 = state->v2;
+      xxh_u32             v3 = state->v3;
+      xxh_u32             v4 = state->v4;
+
+      do {
+
+        v1 = XXH32_round(v1, XXH_readLE32(p));
+        p += 4;
+        v2 = XXH32_round(v2, XXH_readLE32(p));
+        p += 4;
+        v3 = XXH32_round(v3, XXH_readLE32(p));
+        p += 4;
+        v4 = XXH32_round(v4, XXH_readLE32(p));
+        p += 4;
+
+      } while (p <= limit);
+
+      state->v1 = v1;
+      state->v2 = v2;
+      state->v3 = v3;
+      state->v4 = v4;
+
+    }
+
+    if (p < bEnd) {
+
+      XXH_memcpy(state->mem32, p, (size_t)(bEnd - p));
+      state->memsize = (unsigned)(bEnd - p);
+
+    }
+
+  }
+
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH32_hash_t XXH32_digest(const XXH32_state_t *state) {
+
+  xxh_u32 h32;
+
+  if (state->large_len) {
+
+    h32 = XXH_rotl32(state->v1, 1) + XXH_rotl32(state->v2, 7) + XXH_rotl32(state->v3, 12) + XXH_rotl32(state->v4, 18);
+
+  } else {
+
+    h32 = state->v3 /* == seed */ + XXH_PRIME32_5;
+
+  }
+
+  h32 += state->total_len_32;
+
+  return XXH32_finalize(h32, (const xxh_u8 *)state->mem32, state->memsize, XXH_aligned);
+
+}
+
+/*******   Canonical representation   *******/
+
+/*
+ * The default return values from XXH functions are unsigned 32 and 64 bit
+ * integers.
+ *
+ * The canonical representation uses big endian convention, the same convention
+ * as human-readable numbers (large digits first).
+ *
+ * This way, hash values can be written into a file or buffer, remaining
+ * comparable across different systems.
+ *
+ * The following functions allow transformation of hash values to and from their
+ * canonical format.
+ */
+XXH_PUBLIC_API void XXH32_canonicalFromHash(XXH32_canonical_t *dst, XXH32_hash_t hash) {
+
+  XXH_STATIC_ASSERT(sizeof(XXH32_canonical_t) == sizeof(XXH32_hash_t));
+  if (XXH_CPU_LITTLE_ENDIAN) hash = XXH_swap32(hash);
+  memcpy(dst, &hash, sizeof(*dst));
+
+}
+
+XXH_PUBLIC_API XXH32_hash_t XXH32_hashFromCanonical(const XXH32_canonical_t *src) {
+
+  return XXH_readBE32(src);
+
+}
+
+  #ifndef XXH_NO_LONG_LONG
+
+/* *******************************************************************
+ *  64-bit hash functions
+ *********************************************************************/
+
+/*******   Memory access   *******/
+
+typedef XXH64_hash_t xxh_u64;
+
+    #ifdef XXH_OLD_NAMES
+      #define U64 xxh_u64
+    #endif
+
+    /*!
+     * XXH_REROLL_XXH64:
+     * Whether to reroll the XXH64_finalize() loop.
+     *
+     * Just like XXH32, we can unroll the XXH64_finalize() loop. This can be a
+     * performance gain on 64-bit hosts, as only one jump is required.
+     *
+     * However, on 32-bit hosts, because arithmetic needs to be done with two
+     * 32-bit registers, and 64-bit arithmetic needs to be simulated, it isn't
+     * beneficial to unroll. The code becomes ridiculously large (the largest
+     * function in the binary on i386!), and rerolling it saves anywhere from
+     * 3kB to 20kB. It is also slightly faster because it fits into cache better
+     * and is more likely to be inlined by the compiler.
+     *
+     * If XXH_REROLL is defined, this is ignored and the loop is always
+     * rerolled.
+     */
+    #ifndef XXH_REROLL_XXH64
+      #if (defined(__ILP32__) || defined(_ILP32)) ||                                                     \
+          !(defined(__x86_64__) || defined(_M_X64) || defined(_M_AMD64) || defined(_M_ARM64) ||          \
+            defined(__aarch64__) || defined(__arm64__) || defined(__PPC64__) || defined(__PPC64LE__) ||  \
+            defined(__ppc64__) || defined(__powerpc64__) || defined(__mips64__) || defined(__mips64)) || \
+          (!defined(SIZE_MAX) || SIZE_MAX < ULLONG_MAX)
+        #define XXH_REROLL_XXH64 1
+      #else
+        #define XXH_REROLL_XXH64 0
+      #endif
+    #endif                                                                            /* !defined(XXH_REROLL_XXH64) */
+
+    #if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 3))
+    /*
+     * Manual byteshift. Best for old compilers which don't inline memcpy.
+     * We actually directly use XXH_readLE64 and XXH_readBE64.
+     */
+    #elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 2))
+
+/* Force direct memory access. Only works on CPU which support unaligned memory
+ * access in hardware */
+static xxh_u64 XXH_read64(const void *memPtr) {
+
+  return *(const xxh_u64 *)memPtr;
+
+}
+
+    #elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 1))
+
+      /*
+       * __pack instructions are safer, but compiler specific, hence potentially
+       * problematic for some compilers.
+       *
+       * Currently only defined for GCC and ICC.
+       */
+      #ifdef XXH_OLD_NAMES
+typedef union {
+
+  xxh_u32 u32;
+  xxh_u64 u64;
+
+} __attribute__((packed)) unalign64;
+
+      #endif
+static xxh_u64 XXH_read64(const void *ptr) {
+
+  typedef union {
+
+    xxh_u32 u32;
+    xxh_u64 u64;
+
+  } __attribute__((packed)) xxh_unalign64;
+
+  return ((const xxh_unalign64 *)ptr)->u64;
+
+}
+
+    #else
+
+/*
+ * Portable and safe solution. Generally efficient.
+ * see: https://stackoverflow.com/a/32095106/646947
+ */
+static xxh_u64 XXH_read64(const void *memPtr) {
+
+  xxh_u64 val;
+  memcpy(&val, memPtr, sizeof(val));
+  return val;
+
+}
+
+    #endif                                                                        /* XXH_FORCE_DIRECT_MEMORY_ACCESS */
+
+    #if defined(_MSC_VER)                                                                          /* Visual Studio */
+      #define XXH_swap64 _byteswap_uint64
+    #elif XXH_GCC_VERSION >= 403
+      #define XXH_swap64 __builtin_bswap64
+    #else
+static xxh_u64 XXH_swap64(xxh_u64 x) {
+
+  return ((x << 56) & 0xff00000000000000ULL) | ((x << 40) & 0x00ff000000000000ULL) |
+         ((x << 24) & 0x0000ff0000000000ULL) | ((x << 8) & 0x000000ff00000000ULL) | ((x >> 8) & 0x00000000ff000000ULL) |
+         ((x >> 24) & 0x0000000000ff0000ULL) | ((x >> 40) & 0x000000000000ff00ULL) |
+         ((x >> 56) & 0x00000000000000ffULL);
+
+}
+
+    #endif
+
+    /* XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load. */
+    #if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS == 3))
+
+XXH_FORCE_INLINE xxh_u64 XXH_readLE64(const void *memPtr) {
+
+  const xxh_u8 *bytePtr = (const xxh_u8 *)memPtr;
+  return bytePtr[0] | ((xxh_u64)bytePtr[1] << 8) | ((xxh_u64)bytePtr[2] << 16) | ((xxh_u64)bytePtr[3] << 24) |
+         ((xxh_u64)bytePtr[4] << 32) | ((xxh_u64)bytePtr[5] << 40) | ((xxh_u64)bytePtr[6] << 48) |
+         ((xxh_u64)bytePtr[7] << 56);
+
+}
+
+XXH_FORCE_INLINE xxh_u64 XXH_readBE64(const void *memPtr) {
+
+  const xxh_u8 *bytePtr = (const xxh_u8 *)memPtr;
+  return bytePtr[7] | ((xxh_u64)bytePtr[6] << 8) | ((xxh_u64)bytePtr[5] << 16) | ((xxh_u64)bytePtr[4] << 24) |
+         ((xxh_u64)bytePtr[3] << 32) | ((xxh_u64)bytePtr[2] << 40) | ((xxh_u64)bytePtr[1] << 48) |
+         ((xxh_u64)bytePtr[0] << 56);
+
+}
+
+    #else
+XXH_FORCE_INLINE xxh_u64 XXH_readLE64(const void *ptr) {
+
+  return XXH_CPU_LITTLE_ENDIAN ? XXH_read64(ptr) : XXH_swap64(XXH_read64(ptr));
+
+}
+
+static xxh_u64 XXH_readBE64(const void *ptr) {
+
+  return XXH_CPU_LITTLE_ENDIAN ? XXH_swap64(XXH_read64(ptr)) : XXH_read64(ptr);
+
+}
+
+    #endif
+
+XXH_FORCE_INLINE xxh_u64 XXH_readLE64_align(const void *ptr, XXH_alignment align) {
+
+  if (align == XXH_unaligned)
+    return XXH_readLE64(ptr);
+  else
+    return XXH_CPU_LITTLE_ENDIAN ? *(const xxh_u64 *)ptr : XXH_swap64(*(const xxh_u64 *)ptr);
+
+}
+
+/*******   xxh64   *******/
+
+static const xxh_u64 XXH_PRIME64_1 =
+    0x9E3779B185EBCA87ULL; /* 0b1001111000110111011110011011000110000101111010111100101010000111
+                            */
+static const xxh_u64 XXH_PRIME64_2 =
+    0xC2B2AE3D27D4EB4FULL; /* 0b1100001010110010101011100011110100100111110101001110101101001111
+                            */
+static const xxh_u64 XXH_PRIME64_3 =
+    0x165667B19E3779F9ULL; /* 0b0001011001010110011001111011000110011110001101110111100111111001
+                            */
+static const xxh_u64 XXH_PRIME64_4 =
+    0x85EBCA77C2B2AE63ULL; /* 0b1000010111101011110010100111011111000010101100101010111001100011
+                            */
+static const xxh_u64 XXH_PRIME64_5 =
+    0x27D4EB2F165667C5ULL; /* 0b0010011111010100111010110010111100010110010101100110011111000101
+                            */
+
+    #ifdef XXH_OLD_NAMES
+      #define PRIME64_1 XXH_PRIME64_1
+      #define PRIME64_2 XXH_PRIME64_2
+      #define PRIME64_3 XXH_PRIME64_3
+      #define PRIME64_4 XXH_PRIME64_4
+      #define PRIME64_5 XXH_PRIME64_5
+    #endif
+
+static xxh_u64 XXH64_round(xxh_u64 acc, xxh_u64 input) {
+
+  acc += input * XXH_PRIME64_2;
+  acc = XXH_rotl64(acc, 31);
+  acc *= XXH_PRIME64_1;
+  return acc;
+
+}
+
+static xxh_u64 XXH64_mergeRound(xxh_u64 acc, xxh_u64 val) {
+
+  val = XXH64_round(0, val);
+  acc ^= val;
+  acc = acc * XXH_PRIME64_1 + XXH_PRIME64_4;
+  return acc;
+
+}
+
+static xxh_u64 XXH64_avalanche(xxh_u64 h64) {
+
+  h64 ^= h64 >> 33;
+  h64 *= XXH_PRIME64_2;
+  h64 ^= h64 >> 29;
+  h64 *= XXH_PRIME64_3;
+  h64 ^= h64 >> 32;
+  return h64;
+
+}
+
+    #define XXH_get64bits(p) XXH_readLE64_align(p, align)
+
+static xxh_u64 XXH64_finalize(xxh_u64 h64, const xxh_u8 *ptr, size_t len, XXH_alignment align) {
+
+    /* dummy comment */
+
+    #define XXH_PROCESS1_64                        \
+      do {                                         \
+                                                   \
+        h64 ^= (*ptr++) * XXH_PRIME64_5;           \
+        h64 = XXH_rotl64(h64, 11) * XXH_PRIME64_1; \
+                                                   \
+      } while (0)
+
+    #define XXH_PROCESS4_64                                        \
+      do {                                                         \
+                                                                   \
+        h64 ^= (xxh_u64)(XXH_get32bits(ptr)) * XXH_PRIME64_1;      \
+        ptr += 4;                                                  \
+        h64 = XXH_rotl64(h64, 23) * XXH_PRIME64_2 + XXH_PRIME64_3; \
+                                                                   \
+      } while (0)
+
+    #define XXH_PROCESS8_64                                        \
+      do {                                                         \
+                                                                   \
+        xxh_u64 const k1 = XXH64_round(0, XXH_get64bits(ptr));     \
+        ptr += 8;                                                  \
+        h64 ^= k1;                                                 \
+        h64 = XXH_rotl64(h64, 27) * XXH_PRIME64_1 + XXH_PRIME64_4; \
+                                                                   \
+      } while (0)
+
+  /* Rerolled version for 32-bit targets is faster and much smaller. */
+  if (XXH_REROLL || XXH_REROLL_XXH64) {
+
+    len &= 31;
+    while (len >= 8) {
+
+      XXH_PROCESS8_64;
+      len -= 8;
+
+    }
+
+    if (len >= 4) {
+
+      XXH_PROCESS4_64;
+      len -= 4;
+
+    }
+
+    while (len > 0) {
+
+      XXH_PROCESS1_64;
+      --len;
+
+    }
+
+    return XXH64_avalanche(h64);
+
+  } else {
+
+    switch (len & 31) {
+
+      case 24:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 16:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 8:
+        XXH_PROCESS8_64;
+        return XXH64_avalanche(h64);
+
+      case 28:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 20:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 12:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 4:
+        XXH_PROCESS4_64;
+        return XXH64_avalanche(h64);
+
+      case 25:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 17:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 9:
+        XXH_PROCESS8_64;
+        XXH_PROCESS1_64;
+        return XXH64_avalanche(h64);
+
+      case 29:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 21:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 13:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 5:
+        XXH_PROCESS4_64;
+        XXH_PROCESS1_64;
+        return XXH64_avalanche(h64);
+
+      case 26:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 18:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 10:
+        XXH_PROCESS8_64;
+        XXH_PROCESS1_64;
+        XXH_PROCESS1_64;
+        return XXH64_avalanche(h64);
+
+      case 30:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 22:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 14:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 6:
+        XXH_PROCESS4_64;
+        XXH_PROCESS1_64;
+        XXH_PROCESS1_64;
+        return XXH64_avalanche(h64);
+
+      case 27:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 19:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 11:
+        XXH_PROCESS8_64;
+        XXH_PROCESS1_64;
+        XXH_PROCESS1_64;
+        XXH_PROCESS1_64;
+        return XXH64_avalanche(h64);
+
+      case 31:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 23:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 15:
+        XXH_PROCESS8_64;
+        /* fallthrough */
+      case 7:
+        XXH_PROCESS4_64;
+        /* fallthrough */
+      case 3:
+        XXH_PROCESS1_64;
+        /* fallthrough */
+      case 2:
+        XXH_PROCESS1_64;
+        /* fallthrough */
+      case 1:
+        XXH_PROCESS1_64;
+        /* fallthrough */
+      case 0:
+        return XXH64_avalanche(h64);
+
+    }
+
+  }
+
+  /* impossible to reach */
+  XXH_ASSERT(0);
+  return 0;                                                  /* unreachable, but some compilers complain without it */
+
+}
+
+    #ifdef XXH_OLD_NAMES
+      #define PROCESS1_64 XXH_PROCESS1_64
+      #define PROCESS4_64 XXH_PROCESS4_64
+      #define PROCESS8_64 XXH_PROCESS8_64
+    #else
+      #undef XXH_PROCESS1_64
+      #undef XXH_PROCESS4_64
+      #undef XXH_PROCESS8_64
+    #endif
+
+XXH_FORCE_INLINE xxh_u64 XXH64_endian_align(const xxh_u8 *input, size_t len, xxh_u64 seed, XXH_alignment align) {
+
+  const xxh_u8 *bEnd = input + len;
+  xxh_u64       h64;
+
+    #if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER >= 1)
+  if (input == NULL) {
+
+    len = 0;
+    bEnd = input = (const xxh_u8 *)(size_t)32;
+
+  }
+
+    #endif
+
+  if (len >= 32) {
+
+    const xxh_u8 *const limit = bEnd - 32;
+    xxh_u64             v1 = seed + XXH_PRIME64_1 + XXH_PRIME64_2;
+    xxh_u64             v2 = seed + XXH_PRIME64_2;
+    xxh_u64             v3 = seed + 0;
+    xxh_u64             v4 = seed - XXH_PRIME64_1;
+
+    do {
+
+      v1 = XXH64_round(v1, XXH_get64bits(input));
+      input += 8;
+      v2 = XXH64_round(v2, XXH_get64bits(input));
+      input += 8;
+      v3 = XXH64_round(v3, XXH_get64bits(input));
+      input += 8;
+      v4 = XXH64_round(v4, XXH_get64bits(input));
+      input += 8;
+
+    } while (input <= limit);
+
+    h64 = XXH_rotl64(v1, 1) + XXH_rotl64(v2, 7) + XXH_rotl64(v3, 12) + XXH_rotl64(v4, 18);
+    h64 = XXH64_mergeRound(h64, v1);
+    h64 = XXH64_mergeRound(h64, v2);
+    h64 = XXH64_mergeRound(h64, v3);
+    h64 = XXH64_mergeRound(h64, v4);
+
+  } else {
+
+    h64 = seed + XXH_PRIME64_5;
+
+  }
+
+  h64 += (xxh_u64)len;
+
+  return XXH64_finalize(h64, input, len, align);
+
+}
+
+XXH_PUBLIC_API XXH64_hash_t XXH64(const void *input, size_t len, XXH64_hash_t seed) {
+
+    #if 0
+    /* Simple version, good for code maintenance, but unfortunately slow for small inputs */
+    XXH64_state_t state;
+    XXH64_reset(&state, seed);
+    XXH64_update(&state, (const xxh_u8*)input, len);
+    return XXH64_digest(&state);
+
+    #else
+
+  if (XXH_FORCE_ALIGN_CHECK) {
+
+    if ((((size_t)input) & 7) == 0) {                       /* Input is aligned, let's leverage the speed advantage */
+      return XXH64_endian_align((const xxh_u8 *)input, len, seed, XXH_aligned);
+
+    }
+
+  }
+
+  return XXH64_endian_align((const xxh_u8 *)input, len, seed, XXH_unaligned);
+
+    #endif
+
+}
+
+/*******   Hash Streaming   *******/
+
+XXH_PUBLIC_API XXH64_state_t *XXH64_createState(void) {
+
+  return (XXH64_state_t *)XXH_malloc(sizeof(XXH64_state_t));
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH64_freeState(XXH64_state_t *statePtr) {
+
+  XXH_free(statePtr);
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API void XXH64_copyState(XXH64_state_t *dstState, const XXH64_state_t *srcState) {
+
+  memcpy(dstState, srcState, sizeof(*dstState));
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH64_reset(XXH64_state_t *statePtr, XXH64_hash_t seed) {
+
+  XXH64_state_t state; /* use a local state to memcpy() in order to avoid
+                          strict-aliasing warnings */
+  memset(&state, 0, sizeof(state));
+  state.v1 = seed + XXH_PRIME64_1 + XXH_PRIME64_2;
+  state.v2 = seed + XXH_PRIME64_2;
+  state.v3 = seed + 0;
+  state.v4 = seed - XXH_PRIME64_1;
+  /* do not write into reserved64, might be removed in a future version */
+  memcpy(statePtr, &state, sizeof(state) - sizeof(state.reserved64));
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH_errorcode XXH64_update(XXH64_state_t *state, const void *input, size_t len) {
+
+  if (input == NULL)
+    #if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER >= 1)
+    return XXH_OK;
+    #else
+    return XXH_ERROR;
+    #endif
+
+  {
+
+    const xxh_u8 *      p = (const xxh_u8 *)input;
+    const xxh_u8 *const bEnd = p + len;
+
+    state->total_len += len;
+
+    if (state->memsize + len < 32) {                                                          /* fill in tmp buffer */
+      XXH_memcpy(((xxh_u8 *)state->mem64) + state->memsize, input, len);
+      state->memsize += (xxh_u32)len;
+      return XXH_OK;
+
+    }
+
+    if (state->memsize) {                                                                     /* tmp buffer is full */
+      XXH_memcpy(((xxh_u8 *)state->mem64) + state->memsize, input, 32 - state->memsize);
+      state->v1 = XXH64_round(state->v1, XXH_readLE64(state->mem64 + 0));
+      state->v2 = XXH64_round(state->v2, XXH_readLE64(state->mem64 + 1));
+      state->v3 = XXH64_round(state->v3, XXH_readLE64(state->mem64 + 2));
+      state->v4 = XXH64_round(state->v4, XXH_readLE64(state->mem64 + 3));
+      p += 32 - state->memsize;
+      state->memsize = 0;
+
+    }
+
+    if (p + 32 <= bEnd) {
+
+      const xxh_u8 *const limit = bEnd - 32;
+      xxh_u64             v1 = state->v1;
+      xxh_u64             v2 = state->v2;
+      xxh_u64             v3 = state->v3;
+      xxh_u64             v4 = state->v4;
+
+      do {
+
+        v1 = XXH64_round(v1, XXH_readLE64(p));
+        p += 8;
+        v2 = XXH64_round(v2, XXH_readLE64(p));
+        p += 8;
+        v3 = XXH64_round(v3, XXH_readLE64(p));
+        p += 8;
+        v4 = XXH64_round(v4, XXH_readLE64(p));
+        p += 8;
+
+      } while (p <= limit);
+
+      state->v1 = v1;
+      state->v2 = v2;
+      state->v3 = v3;
+      state->v4 = v4;
+
+    }
+
+    if (p < bEnd) {
+
+      XXH_memcpy(state->mem64, p, (size_t)(bEnd - p));
+      state->memsize = (unsigned)(bEnd - p);
+
+    }
+
+  }
+
+  return XXH_OK;
+
+}
+
+XXH_PUBLIC_API XXH64_hash_t XXH64_digest(const XXH64_state_t *state) {
+
+  xxh_u64 h64;
+
+  if (state->total_len >= 32) {
+
+    xxh_u64 const v1 = state->v1;
+    xxh_u64 const v2 = state->v2;
+    xxh_u64 const v3 = state->v3;
+    xxh_u64 const v4 = state->v4;
+
+    h64 = XXH_rotl64(v1, 1) + XXH_rotl64(v2, 7) + XXH_rotl64(v3, 12) + XXH_rotl64(v4, 18);
+    h64 = XXH64_mergeRound(h64, v1);
+    h64 = XXH64_mergeRound(h64, v2);
+    h64 = XXH64_mergeRound(h64, v3);
+    h64 = XXH64_mergeRound(h64, v4);
+
+  } else {
+
+    h64 = state->v3 /*seed*/ + XXH_PRIME64_5;
+
+  }
+
+  h64 += (xxh_u64)state->total_len;
+
+  return XXH64_finalize(h64, (const xxh_u8 *)state->mem64, (size_t)state->total_len, XXH_aligned);
+
+}
+
+/******* Canonical representation   *******/
+
+XXH_PUBLIC_API void XXH64_canonicalFromHash(XXH64_canonical_t *dst, XXH64_hash_t hash) {
+
+  XXH_STATIC_ASSERT(sizeof(XXH64_canonical_t) == sizeof(XXH64_hash_t));
+  if (XXH_CPU_LITTLE_ENDIAN) hash = XXH_swap64(hash);
+  memcpy(dst, &hash, sizeof(*dst));
+
+}
+
+XXH_PUBLIC_API XXH64_hash_t XXH64_hashFromCanonical(const XXH64_canonical_t *src) {
+
+  return XXH_readBE64(src);
+
+}
+
+  /* *********************************************************************
+   *  XXH3
+   *  New generation hash designed for speed on small keys and vectorization
+   ************************************************************************ */
+
+    #include "xxh3.h"
+
+  #endif                                                                                        /* XXH_NO_LONG_LONG */
+
+#endif                                                                                        /* XXH_IMPLEMENTATION */
+
+#if defined(__cplusplus)
+
+}
+
+#endif
+
diff -ruN qemu/include/qemu/qemu-plugin.h qemu_patched/include/qemu/qemu-plugin.h
--- qemu/include/qemu/qemu-plugin.h	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/include/qemu/qemu-plugin.h	2023-11-03 16:20:18.185438021 +0100
@@ -624,4 +624,64 @@
  */
 uint64_t qemu_plugin_entry_code(void);
 
+
+/**
+ * qemu_plugin_get_cpu_register() - reads guest's 32/64 bit cpu registers
+ *
+ * @vcpu_index: vcpu index
+ * @offset1
+ * @offset2
+ * @reg
+ */
+uint32_t qemu_plugin_get_cpu_register(unsigned int vcpu_index, 
+                                      unsigned int offset1, 
+                                      unsigned int offset2, 
+                                      unsigned int reg);
+
+uint64_t qemu_plugin_get_cpu_register_64(unsigned int vcpu_index, 
+                                      unsigned int offset1, 
+                                      unsigned int offset2, 
+                                      unsigned int reg);
+
+
+void qemu_plugin_set_cpu_register(unsigned int cpu_index,
+                                     unsigned int offset1,
+                                     unsigned int offset2,
+                                     unsigned int reg,
+                                     uint64_t value);
+
+
+/**
+ * qemu_plugin_vcpu_read_phys_mem() - reads guest's memory content
+ *
+ * @vcpu_index: vcpu index
+ * @addr: guest's virtual address
+ * @buf: destination buffer to read data to
+ * @len: number of bytes to read
+ *
+ * Adjusts address according to internal memory mapping and reads
+ * content of guest memory.
+ */
+void qemu_plugin_vcpu_read_phys_mem(unsigned int vcpu_index,
+                                   uint64_t addr,
+                                   void *buf,
+                                   uint64_t len);
+
+/**
+ * qemu_plugin_vcpu_write_phys_mem() - writes guest's memory content
+ *
+ * @vcpu_index: vcpu index
+ * @addr: guest's virtual address
+ * @buf: destination buffer to write data to
+ * @len: number of bytes to write
+ *
+ * Adjusts address according to internal memory mapping and write
+ * content of guest memory.
+ */                               
+void qemu_plugin_vcpu_write_phys_mem(unsigned int vcpu_index,
+                                    uint64_t addr,
+                                    void *buf,
+                                    uint64_t len);
+
+
 #endif /* QEMU_PLUGIN_API_H */
diff -ruN qemu/include/sysemu/sysemu.h qemu_patched/include/sysemu/sysemu.h
--- qemu/include/sysemu/sysemu.h	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/include/sysemu/sysemu.h	2023-12-04 23:43:36.192626407 +0100
@@ -6,6 +6,29 @@
 #include "qemu/notify.h"
 #include "qemu/uuid.h"
 
+#include <sys/shm.h>
+#include <sys/sem.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
+
+#define SEM_RD_FUZZER_MODE "/fuzzer_mode_rd_sem_buffer"
+#define SEM_WR_FUZZER_MODE "/fuzzer_mode_wr_sem_buffer"
+#define SHM_FUZZER_MODE "/fuzzer_mode_shm_buffer"
+#define SEM_RD_END_TEST_CASE_MODE "/fuzzer_end_test_case"
+
+#define SEM_RD_FUZZING_INPUT "/fuzzing_input_rd_sem_buffer"
+#define SEM_WR_FUZZING_INPUT "/fuzzing_input_wr_sem_buffer"
+#define SHM_FUZZING_INPUT "/fuzzing_input_shm_buffer"
+
+
+#define DEFAULT_MODE 0
+#define SNAPSHOT_MODE 1
+#define FUZZING_MODE 2
+#define RECOVER_MODE 3
+
 /* vl.c */
 
 extern int only_migratable;
@@ -103,6 +126,8 @@
 
 bool defaults_enabled(void);
 
+void *qemu_fuzzer_thread(void* arg);
+void qemu_fuzzing_loop(void);
 void qemu_init(int argc, char **argv, char **envp);
 void qemu_main_loop(void);
 void qemu_cleanup(void);
diff -ruN qemu/libAFL/aflpp.c qemu_patched/libAFL/aflpp.c
--- qemu/libAFL/aflpp.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/aflpp.c	2023-11-26 21:14:40.482391465 +0100
@@ -0,0 +1,476 @@
+/*
+   american fuzzy lop++ - queue relates routines
+   ---------------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                        Heiko EiÃfeldt <heiko.eissfeldt@hexco.de> and
+                        Andrea Fioraldi <andreafioraldi@gmail.com>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the actual code for the library framework.
+
+ */
+#include <sys/types.h>
+#include <sys/wait.h>
+#include <signal.h>
+
+#include "libAFL/aflpp.h"
+#include "stdbool.h"
+#include "libAFL/afl-returns.h"
+
+afl_ret_t afl_executor_init(afl_executor_t *executor) {
+
+  memset(executor, 0, sizeof(afl_executor_t));
+  executor->current_input = NULL;
+  executor->observors = NULL;
+  executor->observors_count = 0;
+
+  // Default implementations of the functions
+  executor->funcs.init_cb = NULL;
+  executor->funcs.destroy_cb = NULL;
+  executor->funcs.place_input_cb = NULL;
+  executor->funcs.run_target_cb = NULL;
+  executor->funcs.observer_add = afl_executor_add_observer;
+  executor->funcs.observers_reset = afl_observers_reset;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+// Default implementations for executor vtable
+void afl_executor_deinit(afl_executor_t *executor) {
+
+  size_t i;
+  executor->current_input = NULL;
+
+  for (i = 0; i < executor->observors_count; i++) {
+
+    afl_observer_deinit(executor->observors[i]);
+
+  }
+
+  afl_free(executor->observors);
+  executor->observors = NULL;
+
+  executor->observors_count = 0;
+
+}
+
+afl_ret_t afl_executor_add_observer(afl_executor_t *executor, afl_observer_t *obs_channel) {
+
+  executor->observors_count++;
+
+  executor->observors = afl_realloc(executor->observors, executor->observors_count * sizeof(afl_observer_t *));
+  if (!executor->observors) { return AFL_RET_ALLOC; }
+  executor->observors[executor->observors_count - 1] = obs_channel;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+afl_input_t *afl_executor_get_current_input(afl_executor_t *executor) {
+
+  return executor->current_input;
+
+}
+
+void afl_observers_reset(afl_executor_t *executor) {
+
+  size_t i;
+  for (i = 0; i < executor->observors_count; ++i) {
+
+    afl_observer_t *obs_channel = executor->observors[i];
+    if (obs_channel->funcs.reset) { obs_channel->funcs.reset(obs_channel); }
+
+  }
+
+}
+
+/* Function to simple initialize the forkserver */
+/*
+afl_forkserver_t *fsrv_init(char *target_path, char **target_args) {
+
+  afl_forkserver_t *fsrv = calloc(1, sizeof(afl_forkserver_t));
+  if (!fsrv) { return NULL; }
+
+  if (afl_executor_init(&(fsrv->base))) {
+
+    free(fsrv);
+    return NULL;
+
+  }
+
+  // defining standard functions for the forkserver vtable 
+  fsrv->base.funcs.init_cb = fsrv_start;
+  fsrv->base.funcs.place_input_cb = fsrv_place_input;
+  fsrv->base.funcs.run_target_cb = fsrv_run_target;
+  fsrv->use_stdin = 1;
+
+  fsrv->target_path = target_path;
+  fsrv->target_args = target_args;
+  fsrv->out_file = calloc(1, 50);
+  snprintf(fsrv->out_file, 50, "out-%d", rand());
+
+  char **target_args_copy = target_args;
+  while (*target_args_copy != NULL) {
+
+    if (!strcmp(*target_args_copy, "@@")) {
+
+      fsrv->use_stdin = 0;
+      *target_args_copy = fsrv->out_file;  // Replace @@ with the output file name
+      break;
+
+    }
+
+    target_args_copy++;
+
+  }
+
+  // FD for the stdin of the child process 
+  if (fsrv->use_stdin) {
+
+    if (!fsrv->out_file) {
+
+      fsrv->out_fd = -1;
+
+    } else {
+
+      fsrv->out_fd = open((char *)fsrv->out_file, O_WRONLY | O_CREAT, 0600);
+      if (!fsrv->out_fd) {
+
+        afl_executor_deinit(&fsrv->base);
+        free(fsrv);
+        return NULL;
+
+      }
+
+    }
+
+  }
+
+  fsrv->out_dir_fd = -1;
+
+  fsrv->dev_null_fd = open("/dev/null", O_WRONLY);
+  if (!fsrv->dev_null_fd) {
+
+    close(fsrv->out_fd);
+    afl_executor_deinit(&fsrv->base);
+    free(fsrv);
+    return NULL;
+
+  }
+
+  // exec related stuff 
+  fsrv->child_pid = -1;
+  fsrv->exec_tmout = 0;   //Default exec time in ms 
+
+  return fsrv;
+
+}*/
+
+/* This function starts up the forkserver for further process requests */
+afl_ret_t fsrv_start(afl_executor_t *fsrv_executor) {
+
+  afl_forkserver_t *fsrv = (afl_forkserver_t *)fsrv_executor;
+
+  int st_pipe[2], ctl_pipe[2];
+  s32 status;
+  s32 rlen;
+
+  ACTF("Spinning up the fork server...");
+
+  if (pipe(st_pipe) || pipe(ctl_pipe)) { return AFL_RET_ERRNO; }
+
+  fsrv->last_run_timed_out = 0;
+  fsrv->fsrv_pid = fork();
+
+  if (fsrv->fsrv_pid < 0) { return AFL_RET_ERRNO; }
+
+  if (!fsrv->fsrv_pid) {
+
+    /* CHILD PROCESS */
+
+    setsid();
+
+    if (fsrv->use_stdin) {
+
+      fsrv->out_fd = open((char *)fsrv->out_file, O_RDONLY | O_CREAT, 0600);
+      if (!fsrv->out_fd) { PFATAL("Could not open outfile in child"); }
+
+      dup2(fsrv->out_fd, 0);
+      close(fsrv->out_fd);
+
+    }
+
+    dup2(fsrv->dev_null_fd, 1);
+    dup2(fsrv->dev_null_fd, 2);
+
+    /* Set up control and status pipes, close the unneeded original fds. */
+
+    if (dup2(ctl_pipe[0], FORKSRV_FD) < 0) { PFATAL("dup2() failed"); }
+    if (dup2(st_pipe[1], FORKSRV_FD + 1) < 0) { PFATAL("dup2() failed"); }
+
+    close(ctl_pipe[0]);
+    close(ctl_pipe[1]);
+    close(st_pipe[0]);
+    close(st_pipe[1]);
+
+    execv(fsrv->target_path, fsrv->target_args);
+
+    /* Use a distinctive bitmap signature to tell the parent about execv()
+       falling through. */
+
+    fsrv->trace_bits = (u8 *)0xdeadbeef;
+    fprintf(stderr, "Error: execv to target failed\n");
+    exit(0);
+
+  }
+
+  /* PARENT PROCESS */
+
+  char pid_buf[16];
+  sprintf(pid_buf, "%d", fsrv->fsrv_pid);
+  /* Close the unneeded endpoints. */
+
+  close(ctl_pipe[0]);
+  close(st_pipe[1]);
+
+  fsrv->fsrv_ctl_fd = ctl_pipe[1];
+  fsrv->fsrv_st_fd = st_pipe[0];
+
+  /* Wait for the fork server to come up, but don't wait too long. */
+
+  rlen = 0;
+  if (fsrv->exec_tmout) {
+
+    u32 time_ms = afl_read_s32_timed(fsrv->fsrv_st_fd, &status, fsrv->exec_tmout * FORK_WAIT_MULT);
+
+    if (!time_ms) {
+
+      kill(fsrv->fsrv_pid, SIGKILL);
+
+    } else if (time_ms > fsrv->exec_tmout * FORK_WAIT_MULT) {
+
+      fsrv->last_run_timed_out = 1;
+      kill(fsrv->fsrv_pid, SIGKILL);
+
+    } else {
+
+      rlen = 4;
+
+    }
+
+  } else {
+
+    rlen = read(fsrv->fsrv_st_fd, &status, 4);
+
+  }
+
+  /* If we have a four-byte "hello" message from the server, we're all set.
+     Otherwise, try to figure out what went wrong. */
+
+  if (rlen == 4) {
+
+    OKF("All right - fork server is up.");
+    return AFL_RET_SUCCESS;
+
+  }
+
+  if (fsrv->trace_bits == (u8 *)0xdeadbeef) {
+
+    WARNF("Unable to execute target application ('%s')", fsrv->target_args[0]);
+    return AFL_RET_EXEC_ERROR;
+
+  }
+
+  WARNF("Fork server handshake failed");
+  return AFL_RET_BROKEN_TARGET;
+
+}
+
+/* Places input in the executor for the target */
+u8 fsrv_place_input(afl_executor_t *fsrv_executor, afl_input_t *input) {
+
+  afl_forkserver_t *fsrv = (afl_forkserver_t *)fsrv_executor;
+
+  if (!fsrv->use_stdin) { fsrv->out_fd = open(fsrv->out_file, O_RDWR | O_CREAT | O_EXCL, 00600); }
+
+  ssize_t write_len = write(fsrv->out_fd, input->bytes, input->len);
+
+  if (write_len < 0 || (size_t)write_len != input->len) { FATAL("Short Write"); }
+
+  fsrv->base.current_input = input;
+
+  if (!fsrv->use_stdin) { close(fsrv->out_fd); }
+
+  return write_len;
+
+}
+
+/* Execute target application. Return status
+   information.*/
+afl_exit_t fsrv_run_target(afl_executor_t *fsrv_executor) {
+
+  afl_forkserver_t *fsrv = (afl_forkserver_t *)fsrv_executor;
+
+  s32 res;
+  u32 exec_ms;
+  u32 write_value = fsrv->last_run_timed_out;
+
+  /* After this memset, fsrv->trace_bits[] are effectively volatile, so we
+     must prevent any earlier operations from venturing into that
+     territory. */
+
+  // memset(fsrv->trace_bits, 0, fsrv->map_size);
+
+  MEM_BARRIER();
+
+  /* we have the fork server (or faux server) up and running
+  First, tell it if the previous run timed out. */
+
+  if ((res = write(fsrv->fsrv_ctl_fd, &write_value, 4)) != 4) {
+
+    RPFATAL(res, "Unable to request new process from fork server (OOM?)");
+
+  }
+
+  fsrv->last_run_timed_out = 0;
+
+  if ((res = read(fsrv->fsrv_st_fd, &fsrv->child_pid, 4)) != 4) {
+
+    RPFATAL(res, "Unable to request new process from fork server (OOM?)");
+
+  }
+
+  if (fsrv->child_pid <= 0) { FATAL("Fork server is misbehaving (OOM?)"); }
+
+  exec_ms = afl_read_s32_timed(fsrv->fsrv_st_fd, &fsrv->child_status, fsrv->exec_tmout);
+
+  if (exec_ms > fsrv->exec_tmout) {
+
+    /* If there was no response from forkserver after timeout seconds,
+    we kill the child. The forkserver should inform us afterwards */
+
+    kill(fsrv->child_pid, SIGKILL);
+    fsrv->last_run_timed_out = 1;
+    if (read(fsrv->fsrv_st_fd, &fsrv->child_status, 4) < 4) { exec_ms = 0; }
+
+  }
+
+  if (!exec_ms) {}
+
+  if (!WIFSTOPPED(fsrv->child_status)) { fsrv->child_pid = 0; }
+
+  fsrv->total_execs++;
+  if (!fsrv->use_stdin) { unlink(fsrv->out_file); }
+
+  /* Any subsequent operations on fsrv->trace_bits must not be moved by the
+     compiler below this point. Past this location, fsrv->trace_bits[]
+     behave very normally and do not have to be treated as volatile. */
+
+  MEM_BARRIER();
+
+  /* Report outcome to caller. */
+
+  if (WIFSIGNALED(fsrv->child_status)) {
+
+    fsrv->last_kill_signal = WTERMSIG(fsrv->child_status);
+
+    if (fsrv->last_run_timed_out && fsrv->last_kill_signal == SIGKILL) { return AFL_EXIT_TIMEOUT; }
+
+    return AFL_EXIT_CRASH;
+
+  }
+
+  return AFL_EXIT_OK;
+
+}
+
+/* An in-mem executor we have */
+/*
+void in_memory_executor_init(in_memory_executor_t *in_memory_executor, harness_function_type harness) {
+
+  afl_executor_init(&in_memory_executor->base);
+  in_memory_executor->harness = harness;
+  in_memory_executor->argv = NULL;
+  in_memory_executor->argc = 0;
+
+  in_memory_executor->base.funcs.run_target_cb = in_memory_run_target;
+  in_memory_executor->base.funcs.place_input_cb = in_mem_executor_place_input;
+
+}
+
+void in_memory_executor_deinit(in_memory_executor_t *in_memory_executor) {
+
+  afl_executor_deinit(&in_memory_executor->base);
+  in_memory_executor->harness = NULL;
+  in_memory_executor->argv = NULL;
+  in_memory_executor->argc = 0;
+
+  in_memory_executor->base.funcs.run_target_cb = in_memory_run_target;
+  in_memory_executor->base.funcs.place_input_cb = in_mem_executor_place_input;
+
+}*/
+
+u8 in_mem_executor_place_input(afl_executor_t *executor, afl_input_t *input) {
+
+  executor->current_input = input;
+  return 0;
+
+}
+
+/* Function to simple initialize the mils_executor */
+void mils_executor_init(mils_executor_t *mils_executor, harness_function_type harness) 
+{
+  afl_executor_init(&mils_executor->base);
+  mils_executor->harness = harness;
+
+  mils_executor->base.funcs.run_target_cb = mils_run_target;
+  mils_executor->base.funcs.place_input_cb = mils_executor_place_input;
+}
+
+
+/* Function to simple deinitialize the mils_executor */
+void mils_executor_deinit(mils_executor_t *mils_executor) 
+{
+  afl_executor_deinit(&mils_executor->base);
+  mils_executor->harness = NULL;
+
+  mils_executor->base.funcs.run_target_cb = mils_run_target;
+  mils_executor->base.funcs.place_input_cb = mils_executor_place_input;
+}
+
+
+
+
+/* Places input in the executor for the target */
+void mils_executor_place_input(afl_executor_t *executor, afl_input_t *input) 
+{
+	mils_executor_t *mils_executor = (mils_executor_t *)executor;
+	//printf("\nMILS EXECUTOR (aflpp_place_input)  ---  INPUT: %s, LEN: %d\n", input->bytes,input->len);
+	mils_executor->base.current_input = input;
+}
+
+
+
+afl_exit_t mils_run_target(afl_executor_t *executor) 
+{
+  mils_executor_t *mils_executor = (mils_executor_t *)executor;
+
+  afl_input_t *input = mils_executor->base.current_input;
+  //printf("\ncurrent input is %s\n",input->bytes);
+  u8 *data = (input->funcs.serialize) ? (input->funcs.serialize(input)) : input->bytes;
+
+  afl_exit_t run_result = mils_executor->harness(&mils_executor->base, data, input->len);
+
+  return run_result;
+}
diff -ruN qemu/libAFL/common.c qemu_patched/libAFL/common.c
--- qemu/libAFL/common.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/common.c	2023-10-31 00:21:48.343096275 +0100
@@ -0,0 +1,116 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#include <dirent.h>
+
+#include "libAFL/common.h"
+
+/* Get unix time in microseconds */
+inline u64 afl_get_cur_time_us(void) {
+
+  struct timeval  tv;
+  struct timezone tz;
+
+  gettimeofday(&tv, &tz);
+
+  return (tv.tv_sec * 1000000ULL) + tv.tv_usec;
+
+}
+
+bool afl_dir_exists(char *dirpath) {
+
+  DIR *dir_in = NULL;
+
+  size_t dir_name_size = strlen(dirpath);
+
+  if (dirpath[dir_name_size - 1] == '/') { dirpath[dir_name_size - 1] = '\0'; }
+
+  if (!(dir_in = opendir(dirpath))) { return false; }
+  closedir(dir_in);
+  return true;
+
+}
+
+/* Get unix time in seconds */
+inline u64 afl_get_cur_time(void) {
+
+  return afl_get_cur_time_us() / 1000;
+
+}
+
+/* Get unix time in microseconds */
+inline u64 afl_get_cur_time_s(void) {
+
+  struct timeval  tv;
+  struct timezone tz;
+
+  gettimeofday(&tv, &tz);
+
+  return tv.tv_sec;
+
+}
+
+/* Few helper functions */
+
+void *afl_insert_substring(u8 *src_buf, u8 *dest_buf, size_t len, void *token, size_t token_len, size_t offset) {
+
+  // void *new_buf = calloc(len + token_len + 1, 1);
+  memmove(dest_buf, src_buf, offset);
+
+  memmove(dest_buf + offset, token, token_len);
+
+  memmove(dest_buf + offset + token_len, src_buf + offset, len - offset);
+
+  return dest_buf;
+
+}
+
+/* This function inserts given number of bytes at a certain offset in a string
+  and returns a ptr to the newly allocated memory. NOTE: You have to free the
+  original memory(if malloced) yourself*/
+u8 *afl_insert_bytes(u8 *src_buf, u8 *dest_buf, size_t len, u8 byte, size_t insert_len, size_t offset) {
+
+  memmove(dest_buf, src_buf, offset);
+
+  memset(dest_buf + offset, byte, insert_len);
+
+  memmove(dest_buf + offset + insert_len, src_buf + offset, len - offset);
+
+  return dest_buf;
+
+}
+
+size_t afl_erase_bytes(u8 *buf, size_t len, size_t offset, size_t remove_len) {
+
+  memmove(buf + offset, buf + offset + remove_len, len - offset - remove_len);
+  memset(buf + len - remove_len, 0x0, remove_len);
+
+  size_t new_size = len - remove_len;
+
+  return new_size;
+
+}
+
diff -ruN qemu/libAFL/engine.c qemu_patched/libAFL/engine.c
--- qemu/libAFL/engine.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/engine.c	2024-01-13 16:31:19.607746511 +0100
@@ -0,0 +1,750 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+#include <stdlib.h>
+#include <unistd.h>
+#include <fcntl.h>
+#include <dirent.h>
+#include <time.h>
+#include <limits.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+#include "libAFL/engine.h"
+#include "libAFL/aflpp.h"
+#include "libAFL/afl-returns.h"
+#include "libAFL/fuzzone.h"
+#include "libAFL/os.h"
+#include "libAFL/queue.h"
+#include "libAFL/input.h"
+
+afl_ret_t afl_engine_init(afl_engine_t *engine, afl_executor_t *executor, afl_fuzz_one_t *fuzz_one,
+                          afl_queue_global_t *global_queue)
+{
+
+  engine->executor = executor;
+  engine->fuzz_one = fuzz_one;
+  engine->global_queue = global_queue;
+  engine->feedbacks = NULL;
+  engine->feedbacks_count = 0;
+  engine->executions = 0;
+  // engine->cpu_bound = -1; // Initialize bound cpu to -1 (0xffffffff) bit mask for non affinity
+
+  if (global_queue)
+  {
+    global_queue->base.funcs.set_engine(&global_queue->base, engine);
+  }
+
+  engine->funcs.get_queue = afl_engine_get_queue;
+  engine->funcs.get_execs = afl_get_execs;
+  engine->funcs.get_fuzz_one = afl_engine_get_fuzz_one;
+  engine->funcs.get_start_time = afl_engine_get_start_time;
+
+  engine->funcs.set_fuzz_one = afl_set_fuzz_one;
+  engine->funcs.add_feedback = afl_engine_add_feedback;
+  engine->funcs.set_global_queue = afl_set_global_queue;
+
+  engine->funcs.execute = afl_engine_execute;
+  engine->funcs.load_testcases_from_dir = afl_engine_load_testcases_from_dir;
+  engine->funcs.loop = afl_engine_loop;
+  // engine->funcs.handle_new_message = afl_engine_handle_new_message;
+  afl_ret_t ret = afl_rand_init(&engine->rand);
+
+  engine->buf = NULL;
+
+  if (ret != AFL_RET_SUCCESS)
+  {
+    return ret;
+  }
+
+  engine->id = afl_rand_next(&engine->rand);
+
+  return AFL_RET_SUCCESS;
+}
+
+void afl_engine_deinit(afl_engine_t *engine)
+{
+
+  size_t i;
+  /* Let's free everything associated with the engine here, except the queues,
+   * should we leave anything else? */
+
+  afl_rand_deinit(&engine->rand);
+
+  engine->fuzz_one = NULL;
+  engine->executor = NULL;
+  engine->global_queue = NULL;
+
+  for (i = 0; i < engine->feedbacks_count; ++i)
+  {
+
+    engine->feedbacks[i] = NULL;
+  }
+
+  afl_free(engine->feedbacks);
+  engine->feedbacks = NULL;
+
+  engine->start_time = 0;
+  engine->current_feedback_queue = NULL;
+  engine->feedbacks_count = 0;
+  engine->executions = 0;
+}
+
+afl_queue_global_t *afl_engine_get_queue(afl_engine_t *engine)
+{
+
+  return engine->global_queue;
+}
+
+afl_fuzz_one_t *afl_engine_get_fuzz_one(afl_engine_t *engine)
+{
+
+  return engine->fuzz_one;
+}
+
+u64 afl_get_execs(afl_engine_t *engine)
+{
+
+  return engine->executions;
+}
+
+u64 afl_engine_get_start_time(afl_engine_t *engine)
+{
+
+  return engine->start_time;
+}
+
+void afl_set_fuzz_one(afl_engine_t *engine, afl_fuzz_one_t *fuzz_one)
+{
+
+  engine->fuzz_one = fuzz_one;
+
+  if (fuzz_one)
+  {
+    fuzz_one->funcs.set_engine(engine->fuzz_one, engine);
+  }
+}
+
+void afl_set_global_queue(afl_engine_t *engine, afl_queue_global_t *global_queue)
+{
+
+  engine->global_queue = global_queue;
+
+  if (global_queue)
+  {
+    global_queue->base.funcs.set_engine(&global_queue->base, engine);
+  }
+}
+
+afl_ret_t afl_engine_add_feedback(afl_engine_t *engine, afl_feedback_t *feedback)
+{
+
+  engine->feedbacks_count++;
+  engine->feedbacks = afl_realloc(engine->feedbacks, engine->feedbacks_count * sizeof(afl_feedback_t *));
+  if (!engine->feedbacks)
+  {
+    return AFL_RET_ALLOC;
+  }
+
+  engine->feedbacks[engine->feedbacks_count - 1] = feedback;
+
+  return AFL_RET_SUCCESS;
+}
+
+static bool afl_engine_handle_single_testcase_load(char *infile, void *data)
+{
+  afl_engine_t *engine = (afl_engine_t *)data;
+
+  afl_input_t *input = afl_input_new();
+
+  if (!input)
+  {
+
+    DBG("Error allocating input %s", infile);
+    return false;
+  }
+
+  AFL_TRY(input->funcs.load_from_file(input, infile), {
+    WARNF("Error loading seed %s: %s", infile, afl_ret_stringify(err));
+    free(input);
+    return false;
+  });
+
+  /*
+    afl_ret_t run_result = engine->funcs.execute(engine, input);
+
+    if (run_result == AFL_RET_SUCCESS) {
+
+      if (engine->verbose) OKF("Loaded seed %s", infile);
+
+    } else {
+
+      WARNF("Error loading seed %s", infile);
+      // free(input); // should we?
+      return false;
+
+    }
+
+    // We add the corpus to the queue initially for all the feedback queues
+
+    size_t i;
+    for (i = 0; i < engine->feedbacks_count; ++i) {
+
+      afl_entry_t *entry = afl_entry_new(input);
+      if (!entry) {
+
+        DBG("Error allocating entry.");
+        return false;
+
+      }
+
+      engine->feedbacks[i]->queue->base.funcs.insert(&engine->feedbacks[i]->queue->base, entry);
+
+    }
+
+    //if (run_result == AFL_RET_WRITE_TO_CRASH) { if (engine->verbose) WARNF("Crashing input found in initial corpus,
+    this is usually not a good idea.\n"); }
+  */
+  /* We add the corpus to the global queue */
+  afl_entry_t *entry = afl_entry_new(input, NULL);
+  if (!entry)
+  {
+
+    DBG("Error allocating entry.");
+    return false;
+  }
+
+  engine->global_queue->base.funcs.insert(&engine->global_queue->base, entry);
+  if (engine->verbose)
+    OKF("Loaded seed %s", infile);
+
+  return true;
+}
+
+/*afl_ret_t afl_engine_load_testcases_from_dir(afl_engine_t *engine, char *dirpath) {
+
+  return afl_for_each_file(dirpath, afl_engine_handle_single_testcase_load, (void *)engine);
+
+}*/
+
+u8 *datahex(char *string)
+{
+
+  if (string == NULL)
+    return NULL;
+
+  size_t slength = strlen(string);
+  if ((slength % 2) != 0) /* must be even */
+    return NULL;
+
+  size_t dlength = slength / 2;
+
+  uint8_t *data = malloc(dlength);
+  memset(data, 0, dlength);
+
+  size_t index = 0;
+  while (index < slength)
+  {
+    char c = string[index];
+    int value = 0;
+    if (c >= '0' && c <= '9')
+      value = (c - '0');
+    else if (c >= 'A' && c <= 'F')
+      value = (10 + (c - 'A'));
+    else if (c >= 'a' && c <= 'f')
+      value = (10 + (c - 'a'));
+    else
+    {
+      free(data);
+      return NULL;
+    }
+
+    data[(index / 2)] += value << (((index + 1) % 2) * 4);
+
+    index++;
+  }
+
+  return data;
+}
+
+afl_ret_t afl_engine_load_testcases_from_dir(afl_engine_t *engine, char *dirpath) {
+
+  return afl_for_each_file(dirpath, afl_engine_handle_single_testcase_load, (void *)engine);
+}
+
+afl_ret_t afl_engine_load_testcases_from_dir_2(afl_engine_t *engine)
+{
+  if (RESUME_CAMPAIGN)
+  {
+    FILE *file = fopen(COV_FILE_READ, "r");
+    if (file == NULL)
+    {
+      perror("Error opening file");
+      return;
+    }
+
+    char hex_string[30000];
+    while (fgets(hex_string, 30000, file) != NULL)
+    {
+      // Process or print each line (here, we'll just print it)
+      int hex_length = strlen(hex_string); // printf("%s", line);
+
+      if (hex_length % 2 != 0)
+      {
+        printf("Invalid hexadecimal string length.\n");
+        return 1;
+      }
+
+      const size_t char_length = hex_length / 2;
+      char *char_ptr = (char *)malloc(char_length + 1); // +1 for the null terminator
+      if (char_ptr == NULL)
+      {
+        printf("Memory allocation failed.\n");
+        return 1;
+      }
+
+      for (size_t i = 0, j = 0; i < hex_length; i += 2, ++j)
+      {
+        char byte[3] = {hex_string[i], hex_string[i + 1], '\0'};
+        char_ptr[j] = (char)strtol(byte, NULL, 16); // Convert to char
+      }
+      char_ptr[char_length] = '\0'; // Null-terminate the char* string
+
+      printf("Converted string: %s\n", char_ptr);
+
+      free(char_ptr);
+      printf("string is %s\n", hex_string);
+      printf("string length is %d\n", hex_length);
+
+      afl_input_t *inputx = afl_input_new();
+      inputx->bytes = malloc(30000 * sizeof(char));
+
+      for (int i = 0; i < hex_length; i++)
+      {
+        inputx->bytes[i] = char_ptr[i];
+      }
+      inputx->len = hex_length;
+      afl_entry_t *entryx = afl_entry_new(inputx, NULL);
+      if (!entryx)
+      {
+        DBG("Error allocating entry");
+        return false;
+      }
+      engine->global_queue->base.funcs.insert(&engine->global_queue->base, entryx);
+    }
+
+    fclose(file);
+  }
+  char *inputHex0 = "75f63b505698d1f5fa7bc90ac1331ad40fde16ca5b95f988f84d2f2c14add94443aa079026eea0e4bdd65d74388ac3e259c08648493fbceb3635b885";
+  int len0 = strlen(inputHex0) / 2;
+  // char * inputHex0 = "75f63b505698d1f5fa7bc90ac1331ad40fde16ca5b95f988f84d2f2c14add94443aa079026eea0e4bdd65d74388ac3e259c08648493fbceb3635b88575f63b505698d1f5fa7bc90ac1331ad40fde16ca5b95f988f84d2f2c14add94443aa079026eea0e4bdd65d74388ac3e259c08648493fbceb3635b88575f63b505698d1f5fa7bc90ac1331ad40fde16ca5b95f988f84d2f2c14add94443aa079026eea0e4bdd65d74388ac3e259c08648493fbceb3635b88575f63b505698d1f5fa7bc90ac1331ad40fde16ca5b95f988f84d2f2c14add94443aa079026eea0e4bdd65d74388ac3e259c08648493fbceb3635b885";
+  // int len0 = 480;
+
+  afl_input_t *input0 = afl_input_new();
+  if (!input0)
+  {
+    DBG("Error allocating input");
+    return false;
+  }
+  /*
+    input0->bytes = datahex(inputHex0);
+    input0->len = len0;
+  */
+
+  // JSONPARSER ESPERIMENTO 1
+  // input0->bytes = "AA";
+  // JSONPARSER ESPERIMENTO 2
+  input0->bytes = "AAAAA";
+
+  // SMTP ESPERIMENTO 1
+
+  // tinyExprB2
+  // input0->bytes = "e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e";
+  // input0->bytes = "e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e+e";
+
+  input0->len = strlen(input0->bytes);
+
+  // json-parser
+
+  char *inputJson_esempio = "{ \
+    \"nome\": \"Mario\", \
+    \"cognome\": \"Rossi\", \
+    \"eta\": 30, \
+    \"email\": \"mario.rossi@email.com\", \
+    \"indirizzo\": { \
+        \"via\": \"Via Roma\", \
+        \"citta\": \"Roma\", \
+        \"CAP\": \"00100\" \
+    }, \
+    \"interessi\": [ \
+        \"musica\", \
+        \"viaggi\", \
+        \"tecnologia\" \
+    ] \
+}";
+
+  char *inputJson = "{\"name\":\"test\"}";
+  // input0->bytes = inputJson_esempio;
+  // input0->len = strlen(inputJson_esempio);
+  printf("seed is %s\n", input0->bytes);
+
+  afl_entry_t *entry0 = afl_entry_new(input0, NULL);
+  if (!entry0)
+  {
+    DBG("Error allocating entry");
+    return false;
+  }
+  engine->global_queue->base.funcs.insert(&engine->global_queue->base, entry0);
+
+  /*afl_input_t *input1 = afl_input_new();
+  if (!input1) {
+    DBG("Error allocating input");
+    return false;
+  }
+   input1->bytes = "1+2+3+4+5";
+   input1->len = strlen(input1->bytes);
+  afl_entry_t *entry1 = afl_entry_new(input1, NULL);
+  if (!entry1) {
+    DBG("Error allocating entry");
+    return false;
+  }
+  engine->global_queue->base.funcs.insert(&engine->global_queue->base, entry1);
+   */
+
+  /*
+ char * inputHex2 = "6c38f4a92fc2c9056b1c985482140368d7a19fdbbe2e58e5f995eda0491590c8881799bbdf";
+ int len2 = 37;
+ afl_input_t *input2 = afl_input_new();
+ if (!input2) {
+   DBG("Error allocating input");
+   return false;
+ }
+ input2->bytes = datahex(inputHex2);
+ input2->len = len2;
+ afl_entry_t *entry2 = afl_entry_new(input2, NULL);
+ if (!entry2) {
+   DBG("Error allocating entry");
+   return false;
+ }
+ engine->global_queue->base.funcs.insert(&engine->global_queue->base, entry2);
+ char * inputHex3 = "6e892c3bbed909cd749ef0359abe02cc82980e0c073bb71f7d6de5d29a9b8842da2f66fea22c48cf166ad5";
+ int len3 = 43;
+ afl_input_t *input3 = afl_input_new();
+ if (!input3) {
+   DBG("Error allocating input");
+   return false;
+ }
+ input3->bytes = datahex(inputHex3);
+ input3->len = len3;
+ afl_entry_t *entry3 = afl_entry_new(input3, NULL);
+ if (!entry3) {
+   DBG("Error allocating entry");
+   return false;
+ }
+ engine->global_queue->base.funcs.insert(&engine->global_queue->base, entry3);
+ char * inputHex4 = "c9661551703d809f2678477a4a5d8671fb9d1b0edb8fe7a81acb7756d6cc78163c4f10e9ea3bfda6f7bb54d898935513cea7fb1b1bf969";
+ int len4 = 55;
+ afl_input_t *input4 = afl_input_new();
+ if (!input4) {
+   DBG("Error allocating input");
+   return false;
+ }
+ input4->bytes = datahex(inputHex4);
+ input4->len = len4;
+ afl_entry_t *entry4 = afl_entry_new(input4, NULL);
+ if (!entry4) {
+   DBG("Error allocating entry");
+   return false;
+ }
+ engine->global_queue->base.funcs.insert(&engine->global_queue->base, entry4);
+ */
+  return AFL_RET_SUCCESS;
+}
+/*
+afl_ret_t afl_engine_handle_new_message(afl_engine_t *engine, llmp_message_t *msg) {
+
+  // Default implementation, handles only new queue entry messages. Users have
+  // liberty with this function
+
+  if (msg->tag == LLMP_TAG_NEW_QUEUE_ENTRY_V1) {
+
+    afl_input_t *input = afl_input_new();
+    if (!input) { return AFL_RET_ALLOC; }
+
+    // the msg will stick around forever, so this is safe.
+    input->bytes = msg->buf;
+    input->len = msg->buf_len;
+
+    afl_entry_info_t *info_ptr = (afl_entry_info_t *)((u8 *)(msg->buf + msg->buf_len));
+
+    afl_entry_t *new_entry = afl_entry_new(input, info_ptr);
+
+    // Users can experiment here, adding entries to different queues based on
+    // the message tag. Right now, let's just add it to all queues
+    size_t i = 0;
+    engine->global_queue->base.funcs.insert(&engine->global_queue->base, new_entry);
+    afl_queue_feedback_t **feedback_queues = engine->global_queue->feedback_queues;
+    for (i = 0; i < engine->global_queue->feedback_queues_count; ++i) {
+
+      feedback_queues[i]->base.funcs.insert(&feedback_queues[i]->base, new_entry);
+
+    }
+
+  }
+
+  return AFL_RET_SUCCESS;
+
+}*/
+
+u8 afl_engine_execute(afl_engine_t *engine, afl_input_t *input)
+{
+
+  size_t i;
+  afl_executor_t *executor = engine->executor;
+
+  executor->funcs.observers_reset(executor);
+
+  executor->funcs.place_input_cb(executor, input);
+
+  if (engine->start_time == 0)
+  {
+    engine->start_time = time(NULL);
+  }
+
+  afl_exit_t run_result = executor->funcs.run_target_cb(executor);
+
+  engine->executions++;
+  /* We've run the target with the executor, we can now simply postExec call the
+   * observation channels*/
+
+  for (i = 0; i < executor->observors_count; ++i)
+  {
+
+    afl_observer_t *obs_channel = executor->observors[i];
+    if (obs_channel->funcs.post_exec)
+    {
+      obs_channel->funcs.post_exec(executor->observors[i], engine);
+    }
+  }
+
+  // Now based on the return of executor's run target, we basically return an
+  // afl_ret_t type to the callee
+
+  switch (run_result)
+  {
+
+  case AFL_EXIT_OK:
+    // printf("test\n\n");
+  case AFL_EXIT_TIMEOUT:
+    return AFL_RET_SUCCESS;
+  default:
+  {
+
+    afl_queue_global_t *global_queue = afl_engine_get_queue(engine);
+    if (afl_input_dump_to_crashfile(executor->current_input, global_queue->base.dirpath) == AFL_RET_SUCCESS)
+      engine->crashes++;
+    return AFL_RET_WRITE_TO_CRASH;
+  }
+  }
+}
+/*
+afl_ret_t afl_engine_loop(afl_engine_t *engine) {
+
+  while (true) {
+
+    afl_ret_t fuzz_one_ret = engine->fuzz_one->funcs.perform(engine->fuzz_one);
+
+    //let's call this engine's message handler
+
+    if (engine->funcs.handle_new_message) {
+
+      // Let's read the broadcasted messages now
+      llmp_message_t *msg = NULL;
+
+      while ((msg = llmp_client_recv(engine->llmp_client))) {
+
+        AFL_TRY(engine->funcs.handle_new_message(engine, msg), { return err; });
+
+      }
+
+    }
+
+    switch (fuzz_one_ret) {
+
+        // case AFL_RET_WRITE_TO_CRASH:
+
+        //   // crash_write_return =
+        //   // afl_input_dump_to_crashfile(engine->executor->current_input);
+
+        //   return AFL_RET_WRITE_TO_CRASH;
+
+        //   break;
+
+      case AFL_RET_NULL_QUEUE_ENTRY:
+        SAYF("NULL QUEUE\n");
+        return fuzz_one_ret;
+      case AFL_RET_ERROR_INPUT_COPY:
+        return fuzz_one_ret;
+      default:
+        continue;
+
+    }
+
+  }
+
+}
+*/
+
+afl_ret_t afl_engine_loop(afl_engine_t *engine)
+{
+  while (1)
+  {
+    if (VERBOSE_LOG > 0)
+      printf("\nSONO IN: AFL_ENGINE_LOOP\n\n");
+
+    afl_ret_t fuzz_one_ret = engine->fuzz_one->funcs.perform(engine->fuzz_one);
+
+    switch (fuzz_one_ret)
+    {
+
+    case AFL_RET_NULL_QUEUE_ENTRY:
+      printf("NULL QUEUE\n");
+      return fuzz_one_ret;
+    case AFL_RET_ERROR_INPUT_COPY:
+      printf("AFL_RET_ERROR_INPUT_COPY");
+      return fuzz_one_ret;
+    default:
+      continue;
+    }
+  }
+}
+
+/* A function which can be run just before starting the fuzzing process. This checks if the engine(and all it's
+ * components) is initialized or not */
+
+afl_ret_t afl_engine_check_configuration(afl_engine_t *engine)
+{
+
+  bool has_warning = false;
+
+#define AFL_WARN_ENGINE(str)                              \
+  do                                                      \
+  {                                                       \
+                                                          \
+    WARNF("No " str " present in engine-%u", engine->id); \
+    has_warning = true;                                   \
+                                                          \
+  } while (0);
+
+  if (!engine)
+  {
+
+    WARNF("Engine is null");
+    return AFL_RET_NULL_PTR;
+  }
+
+  /* Let's start by checking the essential parts of engine, executor, feedback(if available) */
+
+  if (!engine->executor)
+  {
+
+    /* WARNF("No executor present in engine-%u", engine->id);
+    // goto error;  */
+    AFL_WARN_ENGINE("executor");
+  }
+
+  /* afl_executor_t *executor = engine->executor; */
+
+  if (!engine->global_queue)
+  {
+    AFL_WARN_ENGINE("global_queue")
+  }
+  afl_queue_global_t *global_queue = engine->global_queue;
+
+  if (!engine->fuzz_one)
+  {
+    AFL_WARN_ENGINE("fuzzone")
+  }
+  afl_fuzz_one_t *fuzz_one = engine->fuzz_one;
+
+  size_t i = 0;
+  for (i = 0; i < engine->feedbacks_count; ++i)
+  {
+
+    if (!engine->feedbacks[i])
+    {
+
+      WARNF("Feedback is NULL at %zu idx but feedback count is greater (%llu).", i, engine->feedbacks_count);
+      has_warning = true;
+      break;
+    }
+  }
+
+  /*  if (!engine->llmp_client) { AFL_WARN_ENGINE("llmp client") } */
+  /*
+    if (executor) {
+
+      for (size_t i = 0; i < executor->observors_count; ++i) {
+
+        if (!executor->observors[i]) { AFL_WARN_ENGINE("observation channel") }
+
+      }
+
+    }
+  */
+
+  if (global_queue)
+  {
+    size_t i = 0;
+    for (i = 0; i < global_queue->feedback_queues_count; ++i)
+    {
+
+      if (!global_queue->feedback_queues[i])
+      {
+        AFL_WARN_ENGINE("Feedback queue")
+      }
+    }
+  }
+
+  if (fuzz_one)
+  {
+    size_t i = 0;
+    for (i = 0; i < fuzz_one->stages_count; ++i)
+    {
+
+      if (!fuzz_one->stages[i])
+      {
+        AFL_WARN_ENGINE("Stage")
+      }
+      /* Stage needs to be checked properly */
+    }
+  }
+
+  if (has_warning)
+  {
+    return AFL_RET_ERROR_INITIALIZE;
+  }
+
+  return AFL_RET_SUCCESS;
+
+#undef AFL_WARN_ENGINE
+}
diff -ruN qemu/libAFL/feedback.c qemu_patched/libAFL/feedback.c
--- qemu/libAFL/feedback.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/feedback.c	2023-12-30 14:01:08.780261373 +0100
@@ -0,0 +1,376 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#include "libAFL/feedback.h"
+#include "libAFL/observer.h"
+#include "libAFL/aflpp.h"
+#include "libAFL/config.h"
+#include <stdio.h>
+
+afl_ret_t afl_feedback_init(afl_feedback_t *feedback, afl_queue_feedback_t *queue)
+{
+
+  feedback->queue = queue;
+
+  feedback->funcs.set_feedback_queue = afl_feedback_set_queue;
+  feedback->funcs.get_feedback_queue = afl_feedback_get_queue;
+  feedback->funcs.is_interesting = NULL;
+
+  feedback->tag = AFL_FEEDBACK_TAG_BASE;
+
+  return AFL_RET_SUCCESS;
+}
+
+void afl_feedback_deinit(afl_feedback_t *feedback)
+{
+
+  feedback->tag = AFL_DEINITIALIZED;
+
+  /* Since feedback is deinitialized, we remove it's ptr from the feedback_queue
+   */
+  feedback->queue = NULL;
+}
+
+void afl_feedback_set_queue(afl_feedback_t *feedback, afl_queue_feedback_t *queue)
+{
+
+  feedback->queue = queue;
+
+  if (queue)
+  {
+    queue->feedback = feedback;
+  }
+}
+
+afl_queue_feedback_t *afl_feedback_get_queue(afl_feedback_t *feedback)
+{
+
+  return feedback->queue;
+}
+
+/* Map feedback. Can be easily used with a tracebits map similar to AFL++ */
+
+afl_ret_t afl_feedback_cov_init(afl_feedback_cov_t *feedback, afl_queue_feedback_t *queue,
+                                afl_observer_covmap_t *observer_cov)
+{
+  if (SAVE_METRICS)
+    initFile(STATS_FILE);
+  size_t size = observer_cov->shared_map.map_size;
+
+  feedback->observer_cov = observer_cov;
+
+  feedback->virgin_bits = calloc(1, size);
+  if (!feedback->virgin_bits)
+  {
+    return AFL_RET_ALLOC;
+  }
+  memset(feedback->virgin_bits, 0xff, size);
+
+  AFL_TRY(afl_feedback_init(&feedback->base, queue), {
+    free(feedback->virgin_bits);
+    return err;
+  });
+
+  feedback->size = size;
+  feedback->base.funcs.is_interesting = afl_feedback_cov_is_interesting;
+
+  feedback->base.tag = AFL_FEEDBACK_TAG_COV;
+
+  return AFL_RET_SUCCESS;
+}
+
+/* Set virgin bits according to the map passed into the func */
+afl_ret_t afl_feedback_cov_set_virgin_bits(afl_feedback_cov_t *feedback, u8 *virgin_bits_copy_from, size_t size)
+{
+
+  if (size != feedback->observer_cov->shared_map.map_size)
+  {
+
+    FATAL("Virgin bitmap size may never differs from observer_covmap size");
+  }
+
+  feedback->virgin_bits = realloc(feedback->virgin_bits, size);
+  if (!feedback->virgin_bits)
+  {
+
+    DBG("Failed to alloc %ld bytes for virgin_bitmap", size);
+    feedback->size = 0;
+    return AFL_RET_ALLOC;
+  }
+
+  memcpy(feedback->virgin_bits, virgin_bits_copy_from, size);
+  feedback->size = size;
+  return AFL_RET_SUCCESS;
+}
+
+void afl_feedback_cov_deinit(afl_feedback_cov_t *feedback)
+{
+
+  free(feedback->virgin_bits);
+  feedback->virgin_bits = NULL;
+  feedback->size = 0;
+  afl_feedback_deinit(&feedback->base);
+}
+
+// Function to calculate coverage (how many blocks?)
+int calculateCoverageIndex(int length, uint8_t *coverageMap, int compare)
+{
+  int setBits = 0;
+  // printf("length is %d\n",length);
+  for (int i = 0; i < length; i++)
+  {
+    // printf("%x ",coverageMap[i]);
+    if (coverageMap[i] == compare)
+    {
+      setBits += 1;
+    }
+  }
+
+  int CoverageIndex = length - setBits;
+  return CoverageIndex;
+}
+
+void initFile(const char *filePath)
+{
+  FILE *file = fopen(filePath, "w+");
+
+  if (file == NULL)
+  {
+    fprintf(stderr, "Error opening file\n");
+    return;
+  }
+
+  // Append the data to the file
+  fprintf(file, "TIME,ITERATIONS,EDGE_COVERAGE,BLOCK_COVERAGE,INPUT");
+
+  // Close the file
+  fclose(file);
+}
+
+void appendToFile(const char *filePath, int iterations, float edge_coverage, float block_coverage, time_t currentTime, afl_input_t *input)
+{
+  FILE *file = fopen(filePath, "a+");
+
+  if (file == NULL)
+  {
+    fprintf(stderr, "Error opening file\n");
+    return;
+  }
+  if (iterations > SEED_COUNT) // 2 is NUM OF SEEDS
+  {
+    fprintf(file, "\n%ld,%d,%.2f,%.2f,", currentTime, iterations, edge_coverage, block_coverage);
+    printf("beforeee\n");
+    printf("before %s\n", input->bytes);
+    for (int i = 0; i < input->len; i++)
+    {
+      fprintf(file, "%02X", input->bytes[i]);
+    }
+    printf("after\n");
+  }
+  else
+  {
+    fprintf(file, "\n%ld,%d,%.2f,%.2f,null", currentTime, iterations, edge_coverage, block_coverage);
+  }
+  // Append the data to the file
+
+  /**/
+  if (VERBOSE_LOG > 0)
+    printf("Data appended to the file.\n");
+
+  // Close the file
+  fclose(file);
+
+  if (VERBOSE_LOG > 0)
+    printf("Data appended to the file successfully.\n");
+}
+
+static int iterations = 0;
+float old_edge_coverage_index = 0;
+float old_block_coverage_index = 0;
+float __attribute__((hot)) afl_feedback_cov_is_interesting(afl_feedback_t *feedback, afl_executor_t *fsrv)
+{
+  iterations++;
+  (void)fsrv;
+  /*if (!FEEDBACK_MODE)
+    return 0; // puÃ² esser implementato in maniera piÃ¹ modulare "staccando" il modulo feedback dal main ma mi dava problemi quindi nessun input Ã¨ interessante e amen
+  */
+  // inoltre non viene calcolata la bitmap. sarebbe interessante fare confronto sui tempi con e senza feedback
+
+  // if(iterations==1131)return 1;
+  // else return 0;
+  //  return 0;
+#ifdef AFL_DEBUG
+  if (feedback->tag != AFL_FEEDBACK_TAG_COV)
+  {
+    FATAL("Called cov_is_interesting with non-cov feeback");
+  }
+#endif
+
+  afl_feedback_cov_t *map_feedback = (afl_feedback_cov_t *)feedback;
+  afl_observer_covmap_t *obs_channel = map_feedback->observer_cov;
+
+#ifdef WORD_SIZE_64
+
+  u64 *current = (u64 *)obs_channel->shared_map.map;
+  u64 *virgin = (u64 *)map_feedback->virgin_bits;
+
+  u32 i = (obs_channel->shared_map.map_size >> 3);
+
+#else
+
+  u32 *current = (u32 *)obs_channel->shared_map.map;
+  u32 *virgin = (u32 *)map_feedback->virgin_bits;
+
+  u32 i = (obs_channel->shared_map.map_size >> 2);
+
+#endif /* ^WORD_SIZE_64 */
+  // the map size must be a minimum of 8 bytes.
+  // for variable/dynamic map sizes this is ensured in the forkserver
+
+  float ret = 0.0;
+
+  while (i--)
+  {
+
+    /* Optimize for (*current & *virgin) == 0 - i.e., no bits in current bitmap
+       that have not been already cleared from the virgin map - since this will
+       almost always be the case. */
+
+    // the (*current) is unnecessary but speeds up the overall comparison
+    if (unlikely(*current) && unlikely(*current & *virgin))
+    {
+
+      if (likely(ret < 2))
+      {
+
+        u8 *cur = (u8 *)current;
+        u8 *vir = (u8 *)virgin;
+
+        /* Looks like we have not found any new bytes yet; see if any non-zero
+           bytes in current[] are pristine in virgin[]. */
+
+#ifdef WORD_SIZE_64
+
+        if (*virgin == 0xffffffffffffffff || (cur[0] && vir[0] == 0xff) || (cur[1] && vir[1] == 0xff) ||
+            (cur[2] && vir[2] == 0xff) || (cur[3] && vir[3] == 0xff) || (cur[4] && vir[4] == 0xff) ||
+            (cur[5] && vir[5] == 0xff) || (cur[6] && vir[6] == 0xff) || (cur[7] && vir[7] == 0xff))
+        {
+
+          ret = 1.0;
+        }
+        else
+        {
+
+          ret = 0.5;
+        }
+
+#else
+
+        if (*virgin == 0xffffffff || (cur[0] && vir[0] == 0xff) || (cur[1] && vir[1] == 0xff) ||
+            (cur[2] && vir[2] == 0xff) || (cur[3] && vir[3] == 0xff))
+          ret = 1.0;
+        else
+          ret = 0.5;
+
+#endif /* ^WORD_SIZE_64 */
+      }
+
+      *virgin &= ~*current;
+    }
+
+    ++current;
+    ++virgin;
+  }
+
+#ifdef DEBUG
+  DBG("MAP: %p %lu", obs_channel->shared_map.map, obs_channel->shared_map.map_size);
+  for (u32 j = 0; j < obs_channel->shared_map.map_size; j++)
+  {
+
+    if (obs_channel->shared_map.map[j])
+    {
+      printf(" %04x=%02x", j, obs_channel->shared_map.map[j]);
+    }
+  }
+
+  printf(" ret=%f\n", ret);
+#endif
+  int cont = 0;
+
+  if (VERBOSE_LOG > 2)
+  {
+    DBG("MAP: %p %lu", obs_channel->shared_map.map, obs_channel->shared_map.map_size);
+    for (u32 j = 0; j < obs_channel->shared_map.map_size; j++)
+    {
+
+      if (obs_channel->shared_map.map[j] > 0)
+      {
+        cont += 1;
+        printf("%04x=%02x ", j, obs_channel->shared_map.map[j]);
+      }
+    }
+  }
+
+  // printf("num of tb blocks visitati almeno una volta %d\n",cont);
+  //  printf("virgin map length is %d\n",map_feedback->size);
+
+  float edge_coverage_index = calculateCoverageIndex(map_feedback->size, map_feedback->virgin_bits, 0xff); // obs_channel->shared_map.map);
+
+  if (VERBOSE_LOG > 0)
+  {
+    printf("\nCOVERAGE: %d\n", edge_coverage_index);
+    printf("interestingness = %f\n", ret);
+  }
+
+  if (SAVE_METRICS)
+  {
+    u64 *current_blk_cov = (u64 *)obs_channel->shared_map_block_coverage.map;
+    //*current_blk_cov = ~*current_blk_cov; // complemento
+    float block_coverage_index;
+    block_coverage_index = calculateCoverageIndex(obs_channel->shared_map_block_coverage.map_size, current_blk_cov, 0x00);
+
+    if (edge_coverage_index > old_edge_coverage_index || block_coverage_index > old_block_coverage_index)
+    {
+      // we should log into a file
+
+      time_t currentTime = time(NULL);
+
+      if (BLOCKCOV_MODE)
+      {
+        appendToFile(STATS_FILE, iterations, block_coverage_index, edge_coverage_index, currentTime, fsrv->current_input);
+      }
+      else
+      {
+        appendToFile(STATS_FILE, iterations, edge_coverage_index, block_coverage_index, currentTime, fsrv->current_input);
+      }
+    }
+    old_block_coverage_index = block_coverage_index;
+    old_edge_coverage_index = edge_coverage_index;
+  }
+  if (!FEEDBACK_MODE)
+    return 0;
+  return ret;
+}
diff -ruN qemu/libAFL/fuzzone.c qemu_patched/libAFL/fuzzone.c
--- qemu/libAFL/fuzzone.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/fuzzone.c	2023-11-26 16:17:12.568000657 +0100
@@ -0,0 +1,193 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+#include "libAFL/queue.h"
+#include "libAFL/fuzzone.h"
+#include "libAFL/engine.h"
+#include "libAFL/stage.h"
+
+afl_ret_t afl_fuzz_one_init(afl_fuzz_one_t *fuzz_one, afl_engine_t *engine)
+{
+
+  fuzz_one->engine = engine;
+
+  if (engine)
+  {
+    engine->fuzz_one = fuzz_one;
+  }
+
+  fuzz_one->funcs.add_stage = afl_fuzz_one_add_stage;
+  fuzz_one->funcs.perform = afl_fuzz_one_perform;
+  fuzz_one->funcs.set_engine = afl_fuzz_one_set_engine;
+
+  return AFL_RET_SUCCESS;
+}
+
+void afl_fuzz_one_deinit(afl_fuzz_one_t *fuzz_one)
+{
+
+  size_t i;
+  /* Also remove the fuzz one from engine */
+  fuzz_one->engine = NULL;
+
+  /* TODO: Should we deinitialize the stages or just remove the reference of
+   * fuzzone from them? */
+  for (i = 0; i < fuzz_one->stages_count; ++i)
+  {
+
+    fuzz_one->stages[i] = NULL;
+  }
+
+  afl_free(fuzz_one->stages);
+  fuzz_one->stages = NULL;
+  fuzz_one->stages_count = 0;
+}
+
+afl_ret_t afl_fuzz_one_perform_new(afl_fuzz_one_t *fuzz_one)
+{
+
+  /* Fuzzone grabs the current queue entry from the global queue and
+  // sends it to stage. */
+  // printf("\nSONO IN: afl_fuzz_one_perform\n\n");
+
+  int i;
+
+  afl_queue_global_t *global_queue = fuzz_one->engine->global_queue;
+  // printf("after global queue\n\n");
+  afl_entry_t *queue_entry =
+      global_queue->base.funcs.get_next_in_queue((afl_queue_t *)global_queue, fuzz_one->engine->id);
+  // printf("after base.funcs.get_next_in_queue((afl_queue_t *)globa\n\n");
+  if (!queue_entry)
+  {
+    printf("\nnull queue entry\n");
+
+    return AFL_RET_NULL_QUEUE_ENTRY;
+  }
+  // printf("size is %ld\n\n",fuzz_one->stages_count);
+  /* Fuzz the entry with every stage */
+  for (i = 0; i < fuzz_one->stages_count; ++i)
+  {
+
+    afl_stage_t *current_stage = fuzz_one->stages[i];
+    printf("\nCURRENT STAGE: %d\n", i + 1);
+    afl_ret_t stage_ret = current_stage->funcs.perform(current_stage, queue_entry);
+    printf("\nPOST-PERFORM. STAGE: %d ------ RET: %d\n", i + 1, (int)stage_ret);
+
+    switch (stage_ret)
+    {
+
+    case AFL_RET_SUCCESS:
+      continue;
+    default:
+      return stage_ret;
+    }
+  }
+
+  return AFL_RET_SUCCESS;
+}
+
+afl_ret_t afl_fuzz_one_perform(afl_fuzz_one_t *fuzz_one)
+{
+
+  // Fuzzone grabs the current queue entry from the global queue and
+  // sends it to stage.
+
+  // printf("\nSONO IN: afl_fuzz_one_perform\n\n");
+
+  size_t i;
+
+  afl_queue_global_t *global_queue = fuzz_one->engine->global_queue;
+
+  afl_entry_t *queue_entry =
+      global_queue->base.funcs.get_next_in_queue((afl_queue_t *)global_queue, fuzz_one->engine->id);
+
+  if (!queue_entry)
+  {
+    return AFL_RET_NULL_QUEUE_ENTRY;
+  }
+
+  /* Fuzz the entry with every stage */
+  for (i = 0; i < fuzz_one->stages_count; ++i)
+  {
+
+    /*afl_stage_t *current_stage = fuzz_one->stages[i];
+    afl_ret_t    stage_ret = current_stage->funcs.perform(current_stage, queue_entry->input);
+    */
+    afl_stage_t *current_stage = fuzz_one->stages[i];
+    printf("\nCURRENT STAGE: %d\n", i + 1);
+    afl_ret_t stage_ret = current_stage->funcs.perform(current_stage, queue_entry);
+    printf("\nPOST-PERFORM. STAGE: %d ------ RET: %d\n", i + 1, (int)stage_ret);
+
+    switch (stage_ret)
+    {
+
+    case AFL_RET_SUCCESS:
+      continue;
+    default:
+      return stage_ret;
+    }
+  }
+
+  return AFL_RET_SUCCESS;
+}
+
+afl_ret_t afl_fuzz_one_add_stage(afl_fuzz_one_t *fuzz_one, afl_stage_t *stage)
+{
+
+  if (!stage || !fuzz_one)
+  {
+    return AFL_RET_NULL_PTR;
+  }
+
+  fuzz_one->stages_count++;
+  fuzz_one->stages = afl_realloc(fuzz_one->stages, fuzz_one->stages_count * sizeof(afl_stage_t *));
+  if (!fuzz_one->stages)
+  {
+    return AFL_RET_ALLOC;
+  }
+
+  fuzz_one->stages[fuzz_one->stages_count - 1] = stage;
+
+  stage->engine = fuzz_one->engine;
+
+  return AFL_RET_SUCCESS;
+}
+
+afl_ret_t afl_fuzz_one_set_engine(afl_fuzz_one_t *fuzz_one, afl_engine_t *engine)
+{
+
+  size_t i;
+  fuzz_one->engine = engine;
+
+  if (engine)
+  {
+    engine->fuzz_one = fuzz_one;
+  }
+
+  for (i = 0; i < fuzz_one->stages_count; ++i)
+  {
+
+    fuzz_one->stages[i]->engine = engine;
+  }
+
+  return AFL_RET_SUCCESS;
+}
diff -ruN qemu/libAFL/input.c qemu_patched/libAFL/input.c
--- qemu/libAFL/input.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/input.c	2023-11-28 19:17:31.497042439 +0100
@@ -0,0 +1,243 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#include <fcntl.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <unistd.h>
+
+#include "libAFL/input.h"
+#include "libAFL/afl-returns.h"
+#include "libAFL/xxh3.h"
+#include "libAFL/xxhash.h"
+#include "libAFL/alloc-inl.h"
+
+afl_ret_t afl_input_init(afl_input_t *input)
+{
+
+  input->funcs.clear = afl_input_clear;
+  input->funcs.copy = afl_input_copy;
+  input->funcs.deserialize = afl_input_deserialize;
+  input->funcs.get_bytes = afl_input_get_bytes;
+  input->funcs.load_from_file = afl_input_load_from_file;
+  input->funcs.restore = afl_input_restore;
+  input->funcs.save_to_file = afl_input_write_to_file;
+  input->funcs.serialize = afl_input_serialize;
+  input->funcs.delete = afl_input_delete;
+
+  input->copy_buf = NULL;
+
+  input->bytes = NULL;
+  input->len = 0;
+
+  return AFL_RET_SUCCESS;
+}
+
+void afl_input_deinit(afl_input_t *input)
+{
+
+  /* Deiniting requires a little hack. We free the byte ONLY if copy buf is not NULL. Because then we can assume that
+   * the input is in the queue*/
+  if (input->bytes && input->copy_buf)
+  {
+
+    free(input->bytes);
+    afl_free(input->copy_buf);
+  }
+
+  input->bytes = NULL;
+  input->len = 0;
+
+  return;
+}
+
+// default implemenatations for the vtable functions for the raw_input type
+
+void afl_input_clear(afl_input_t *input)
+{
+
+  memset(input->bytes, 0x0, input->len);
+  input->len = 0;
+
+  return;
+}
+
+afl_input_t *afl_input_copy(afl_input_t *orig_inp)
+{
+
+  afl_input_t *copy_inp = afl_input_new();
+  if (!copy_inp)
+  {
+    return NULL;
+  }
+  copy_inp->bytes = afl_realloc(orig_inp->copy_buf, (orig_inp->len) * sizeof(u8));
+  orig_inp->copy_buf = copy_inp->bytes;
+  if (!copy_inp->bytes)
+  {
+
+    afl_input_delete(copy_inp);
+    return NULL;
+  }
+
+  memcpy(copy_inp->bytes, orig_inp->bytes, orig_inp->len);
+  copy_inp->len = orig_inp->len;
+  return copy_inp;
+}
+
+void afl_input_deserialize(afl_input_t *input, u8 *bytes, size_t len)
+{
+
+  if (input->bytes)
+    free(input->bytes);
+  input->bytes = bytes;
+  input->len = len;
+
+  return;
+}
+
+u8 *afl_input_get_bytes(afl_input_t *input)
+{
+
+  return input->bytes;
+}
+
+afl_ret_t afl_input_load_from_file(afl_input_t *input, char *fname)
+{
+
+  struct stat st;
+  s32 fd = open(fname, O_RDONLY);
+
+  if (fd < 0)
+  {
+    return AFL_RET_FILE_OPEN_ERROR;
+  }
+
+  if (fstat(fd, &st) || !st.st_size)
+  {
+
+    close(fd);
+    return AFL_RET_FILE_SIZE;
+  }
+
+  input->len = st.st_size;
+  input->bytes = calloc(input->len + 1, 1);
+  if (!input->bytes)
+  {
+
+    close(fd);
+    return AFL_RET_ALLOC;
+  }
+
+  ssize_t ret = read(fd, input->bytes, input->len);
+  close(fd);
+
+  if (ret < 0 || (size_t)ret != input->len)
+  {
+
+    free(input->bytes);
+    input->bytes = NULL;
+    return AFL_RET_SHORT_READ;
+  }
+
+  return AFL_RET_SUCCESS;
+}
+
+afl_ret_t afl_input_write_to_file(afl_input_t *input, char *fname)
+{
+  /*printf("i want to save input %s\n",input->bytes);
+  // if it already exists we will not overwrite it
+  //if (access(fname, W_OK) == 0) return AFL_RET_FILE_DUPLICATE;
+  printf("ok1\n");
+  s32 fd = open(fname, O_RDWR | O_CREAT | O_EXCL, 0600);
+  printf("ok2\n");
+  if (fd < 0) { return AFL_RET_FILE_OPEN_ERROR; }
+  printf("ok3\n");
+  ssize_t write_len = write(fd, input->bytes, input->len);
+  close(fd);
+  printf("ok4\n");
+  if (write_len < (ssize_t)input->len) { return AFL_RET_SHORT_WRITE; }
+  printf("ok5\n");
+  return AFL_RET_SUCCESS;*/
+
+  FILE *file = fopen(fname, "a+");
+  for (int i = 0; i < input->len; i++)
+  {
+    fprintf(file, "%02X", input->bytes[i]);
+  }
+  fprintf(file, "\n");
+  fclose(file);
+}
+
+void afl_input_restore(afl_input_t *input, afl_input_t *new_inp)
+{
+
+  input->bytes = new_inp->bytes;
+
+  return;
+}
+
+u8 *afl_input_serialize(afl_input_t *input)
+{
+
+  // Very stripped down implementation, actually depends on user alot.
+  return input->bytes;
+}
+
+afl_ret_t afl_input_dump_to_file(char *filetag, afl_input_t *data, char *directory)
+{
+
+  char filename[PATH_MAX];
+
+  /* TODO: This filename should be replaced by "crashes-SHA_OF_BYTES" later */
+
+  u64 input_data_checksum = XXH64(data->bytes, data->len, HASH_CONST);
+  if (directory)
+  {
+
+    snprintf(filename, sizeof(filename), "%s/%s-%016llx", directory, filetag, input_data_checksum);
+  }
+  else
+  {
+
+    snprintf(filename, sizeof(filename), "%s-%016llx", filetag, input_data_checksum);
+  }
+
+  return afl_input_write_to_file(data, filename);
+}
+
+// Timeout related functions
+afl_ret_t afl_input_dump_to_timeoutfile(afl_input_t *data, char *directory)
+{
+
+  return afl_input_dump_to_file("timeout", data, directory);
+}
+
+// Crash related functions
+afl_ret_t afl_input_dump_to_crashfile(afl_input_t *data, char *directory)
+{
+
+  return afl_input_dump_to_file("crash", data, directory);
+}
diff -ruN qemu/libAFL/meson.build qemu_patched/libAFL/meson.build
--- qemu/libAFL/meson.build	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/meson.build	2023-11-12 03:28:45.630255966 +0100
@@ -0,0 +1,17 @@
+
+softmmu_ss.add(files(
+  'aflpp.c',
+  #'alloc-inl.c',
+  'common.c',
+  'engine.c',
+  'feedback.c',
+  'fuzzone.c',
+  'input.c',
+  'mutator.c',
+  'observer.c',
+  'os.c',
+  'queue.c',
+  'rand.c',
+  'shmem.c',
+  'stage.c',
+))
\ Manca newline alla fine del file
diff -ruN qemu/libAFL/mutator.c qemu_patched/libAFL/mutator.c
--- qemu/libAFL/mutator.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/mutator.c	2023-12-09 18:53:57.409531583 +0100
@@ -0,0 +1,678 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#include <stdlib.h>
+
+#include "libAFL/mutator.h"
+#include "libAFL/engine.h"
+#include "libAFL/stage.h"
+#include "libAFL/alloc-inl.h"
+#include "libAFL/config.h"
+#include "libAFL/debug.h"
+
+afl_ret_t afl_mutator_init(afl_mutator_t *mutator, afl_engine_t *engine)
+{
+  mutator->engine = engine;
+  mutator->mutate_buf = NULL;
+
+  return AFL_RET_SUCCESS;
+}
+
+void afl_mutator_deinit(afl_mutator_t *mutator)
+{
+  mutator->engine = NULL;
+}
+
+afl_ret_t afl_mutator_scheduled_init(afl_mutator_scheduled_t *sched_mut, afl_engine_t *engine, size_t max_iterations)
+{
+  AFL_TRY(afl_mutator_init(&(sched_mut->base), engine), { return err; });
+
+  sched_mut->base.funcs.mutate = afl_mutate_scheduled_mutator;
+  sched_mut->funcs.add_func = afl_mutator_add_func;
+  sched_mut->funcs.get_iters = afl_iterations;
+  sched_mut->funcs.schedule = afl_schedule;
+
+  sched_mut->max_iterations = (max_iterations > 0) ? max_iterations : 7;
+  return AFL_RET_SUCCESS;
+}
+
+void afl_mutator_scheduled_deinit(afl_mutator_scheduled_t *sched_mut)
+{
+
+  size_t i;
+  afl_mutator_deinit(&(sched_mut->base));
+  sched_mut->max_iterations = 0;
+
+  for (i = 0; i < sched_mut->mutators_count; ++i)
+  {
+
+    sched_mut->mutations[i] = NULL;
+  }
+
+  afl_free(sched_mut->mutations);
+  sched_mut->mutations = NULL;
+
+  sched_mut->mutators_count = 0;
+}
+
+afl_ret_t afl_mutator_add_func(afl_mutator_scheduled_t *mutator, afl_mutator_func mutator_func)
+{
+
+  mutator->mutators_count++;
+  mutator->mutations = afl_realloc(mutator->mutations, mutator->mutators_count * sizeof(afl_mutator_func));
+  if (!mutator->mutations)
+  {
+
+    mutator->mutators_count = 0;
+    return AFL_RET_ALLOC;
+  }
+
+  mutator->mutations[mutator->mutators_count - 1] = mutator_func;
+  return AFL_RET_SUCCESS;
+}
+
+size_t afl_iterations(afl_mutator_scheduled_t *mutator)
+{
+
+  return 1 << (1 + afl_rand_below(&mutator->base.engine->rand, mutator->max_iterations));
+}
+
+size_t afl_schedule(afl_mutator_scheduled_t *mutator)
+{
+
+  return afl_rand_below(&mutator->base.engine->rand, mutator->mutators_count);
+}
+
+size_t afl_mutate_scheduled_mutator(afl_mutator_t *mutator, afl_input_t *input)
+{
+  /* This is to stop from compiler complaining about the incompatible pointer
+  // type for the function ptrs. We need a better solution for this to pass the
+  // scheduled_mutator rather than the mutator as an argument. */
+  afl_mutator_scheduled_t *scheduled_mutator = (afl_mutator_scheduled_t *)mutator;
+  size_t i;
+  /* printf("\nNUMBER OF ITERATIONS: %d\n", scheduled_mutator->funcs.get_iters(scheduled_mutator)); */
+
+  for (i = 0; i < scheduled_mutator->funcs.get_iters(scheduled_mutator); ++i)
+  {
+    scheduled_mutator->mutations[scheduled_mutator->funcs.schedule(scheduled_mutator)](&scheduled_mutator->base, input);
+  }
+  return 0;
+}
+
+/* A few simple mutators that we use over in AFL++ in the havoc and
+ * deterministic modes*/
+static size_t choose_block_len(afl_rand_t *rand, size_t limit)
+{
+  size_t min_value, max_value;
+  switch (afl_rand_below(rand, 3))
+  {
+  case 0:
+    min_value = 1;
+    max_value = HAVOC_BLK_SMALL;
+    break;
+  case 1:
+    min_value = HAVOC_BLK_SMALL;
+    max_value = HAVOC_BLK_MEDIUM;
+    break;
+  default:
+    if (afl_rand_below(rand, 10))
+    {
+
+      min_value = HAVOC_BLK_MEDIUM;
+      max_value = HAVOC_BLK_LARGE;
+    }
+    else
+    {
+
+      min_value = HAVOC_BLK_LARGE;
+      max_value = HAVOC_BLK_XL;
+    }
+  }
+
+  if (min_value >= limit)
+  {
+    min_value = 1;
+  }
+
+  return afl_rand_between(rand, min_value, MIN(max_value, limit));
+}
+
+void afl_mutfunc_flip_bit(afl_mutator_t *mutator, afl_input_t *input)
+{
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+  int bit = afl_rand_below(rand, input->len * 8 - 1) + 1;
+
+  input->bytes[(bit >> 3)] ^= (1 << ((bit - 1) % 8));
+}
+
+void afl_mutfunc_flip_2_bits(afl_mutator_t *mutator, afl_input_t *input)
+{
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+  size_t size = input->len;
+
+  int bit = afl_rand_below(rand, (size * 8) - 1) + 1;
+
+  if ((size << 3) - bit < 2)
+  {
+    return;
+  }
+
+  input->bytes[bit >> 3] ^= (1 << ((bit - 1) % 8));
+  bit++;
+  input->bytes[bit >> 3] ^= (1 << ((bit - 1) % 8));
+}
+
+void afl_mutfunc_flip_4_bits(afl_mutator_t *mutator, afl_input_t *input)
+{
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+
+  if (size <= 0)
+  {
+    return;
+  }
+
+  int bit = afl_rand_below(rand, (size << 3) - 1) + 1;
+
+  if ((size << 3) - bit < 4)
+  {
+    return;
+  }
+
+  input->bytes[bit >> 3] ^= (1 << ((bit - 1) % 8));
+  bit++;
+  input->bytes[bit >> 3] ^= (1 << ((bit - 1) % 8));
+  bit++;
+  input->bytes[bit >> 3] ^= (1 << ((bit - 1) % 8));
+  bit++;
+  input->bytes[bit >> 3] ^= (1 << ((bit - 1) % 8));
+}
+
+void afl_mutfunc_flip_byte(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+
+  if (size <= 0)
+  {
+    return;
+  }
+
+  int byte = afl_rand_below(rand, size);
+
+  input->bytes[byte] ^= 0xff;
+
+  return;
+}
+
+void afl_mutfunc_flip_2_bytes(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+
+  if (size < 2)
+  {
+    return;
+  }
+
+  int byte = afl_rand_below(rand, size - 1);
+
+  input->bytes[byte] ^= 0xff;
+  input->bytes[byte + 1] ^= 0xff;
+}
+
+void afl_mutfunc_flip_4_bytes(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+
+  if (size < 4)
+  {
+    return;
+  }
+
+  int byte = afl_rand_below(rand, size - 3);
+
+  if (byte == -1)
+  {
+    return;
+  }
+
+  input->bytes[byte] ^= 0xff;
+  input->bytes[byte + 1] ^= 0xff;
+  input->bytes[byte + 2] ^= 0xff;
+  input->bytes[byte + 3] ^= 0xff;
+}
+
+void afl_mutfunc_random_byte_add_sub(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+
+  if (size <= 0)
+  {
+    return;
+  }
+
+  size_t idx = afl_rand_below(rand, size);
+
+  input->bytes[idx] -= 1 + (u8)afl_rand_below(rand, ARITH_MAX);
+  input->bytes[idx] += 1 + (u8)afl_rand_below(rand, ARITH_MAX);
+}
+
+void afl_mutfunc_random_byte(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+  if (size <= 0)
+  {
+    return;
+  }
+
+  int idx = afl_rand_below(rand, size);
+  input->bytes[idx] ^= 1 + (u8)afl_rand_below(rand, 255);
+}
+
+void afl_mutfunc_delete_bytes(afl_mutator_t *mutator, afl_input_t *input)
+{
+  if (unlikely(!input->len))
+    return;
+
+  afl_rand_t *rand = &mutator->engine->rand;
+  size_t size = input->len;
+
+  if (size < 2)
+    return;
+
+  size_t del_len = choose_block_len(rand, size - 1);
+  size_t del_from = afl_rand_below(rand, size - del_len + 1);
+
+  /* We delete the bytes and then update the new input length*/
+  input->len = afl_erase_bytes(input->bytes, size, del_from, del_len);
+}
+
+void afl_mutfunc_clone_bytes(afl_mutator_t *mutator, afl_input_t *input)
+{
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+
+  afl_rand_t *rand = &mutator->engine->rand;
+
+  size_t size = input->len;
+
+  /* printf("\nafl_mutfunc_clone_bytes  ---  input_len: %d ;  size: %d;  bytes: %s\n", input->len, size, (char*)input->bytes); */
+
+  if (!size)
+  {
+    return;
+  }
+
+  int actually_clone = afl_rand_below(rand, 4);
+
+  size_t clone_from, clone_to, clone_len;
+
+  clone_to = afl_rand_below(rand, size);
+
+  /* printf("\nafl_mutfunc_clone_bytes  ---  ACTUALLY CLONE: %d;  CLONE TO: %d\n", actually_clone, clone_to); */
+
+  if (actually_clone)
+  {
+    // printf("\ntestX\n");
+    clone_len = choose_block_len(rand, size);
+     /*if(clone_len>25){//realloc bug workaround
+       clone_len=25;
+     }*/
+
+    clone_from = afl_rand_below(rand, size - clone_len + 1);
+
+    mutator->mutate_buf = afl_realloc(mutator->mutate_buf, clone_len + size);
+
+    input->bytes = afl_insert_substring(input->bytes, mutator->mutate_buf, size, input->bytes + clone_from, clone_len, clone_to);
+    input->len += clone_len;
+  }
+  else
+  {
+    
+    clone_len = choose_block_len(rand, HAVOC_BLK_XL);
+    printf("\nafl_mutfunc_clone_bytes  --- ELSE ---  CLONE LEN: %d\n", clone_len);
+     if(clone_len>20){//realloc bug workaround // se input len seed troppo piccolo questo fa crashare! vorrei fare debug :(
+       clone_len=20;
+     }
+
+    input->bytes = afl_insert_bytes(input->bytes, mutator->mutate_buf, size, afl_rand_below(rand, 255), clone_len, clone_to); // was afl_rand_below(rand, 255)
+    printf("\nafl_mutfunc_clone_bytes  --- ELSE ---  BYTES: %s;  EFF. LEN: %d\n", input->bytes, strlen((char *)input->bytes));
+
+    input->len += clone_len;
+    printf("\nafl_mutfunc_clone_bytes  --- ELSE --- NEW INPUT LEN: %d\n", input->len);
+  }
+}
+
+static void locate_diffs(u8 *ptr1, u8 *ptr2, u32 len, s32 *first, s32 *last)
+{
+
+  s32 f_loc = -1;
+  s32 l_loc = -1;
+  u32 pos;
+
+  for (pos = 0; pos < len; ++pos)
+  {
+
+    if (*(ptr1++) != *(ptr2++))
+    {
+
+      if (f_loc == -1)
+      {
+        f_loc = pos;
+      }
+      l_loc = pos;
+    }
+  }
+
+  *first = f_loc;
+  *last = l_loc;
+
+  return;
+}
+
+void afl_mutfunc_splice(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  /* Let's grab the engine for random num generation and queue */
+
+  if (unlikely(!input->len))
+  {
+    return;
+  }
+  afl_engine_t *engine = mutator->engine;
+  afl_queue_global_t *global_queue = engine->global_queue;
+
+  afl_input_t *splice_input = NULL;
+
+  s32 f_diff = 0;
+  s32 l_diff = 0;
+
+  int counter = 0;
+
+  do
+  {
+
+    size_t random_queue_idx =
+        afl_rand_below(&engine->rand, global_queue->feedback_queues_count + 1); /* +1 so that we can also grab a queue */
+                                                                                /* entry from the global_queue */
+
+    if (random_queue_idx < global_queue->feedback_queues_count)
+    {
+
+      /* Grab a random entry from the random feedback queue */
+      afl_queue_feedback_t *random_fbck_queue = global_queue->feedback_queues[random_queue_idx];
+      splice_input =
+          (random_fbck_queue->base.entries_count > 0)
+              ? random_fbck_queue->base.entries[afl_rand_below(&engine->rand, random_fbck_queue->base.entries_count)]
+                    ->input
+              : NULL;
+
+      if (splice_input && !splice_input->bytes)
+      {
+        splice_input = NULL;
+      }
+    }
+    else
+    {
+
+      /* Grab a random entry from the global queue */
+      splice_input =
+          (global_queue->base.entries_count > 0)
+              ? global_queue->base.entries[afl_rand_below(&engine->rand, global_queue->base.entries_count)]->input
+              : NULL;
+      if (splice_input && !splice_input->bytes)
+      {
+        splice_input = NULL;
+      }
+    }
+
+    /* Counter basically stops it from infinite loop in case of empty queue */
+    if (counter++ > 20)
+    {
+      return;
+    }
+
+    if (!splice_input)
+    {
+      continue;
+    }
+
+    locate_diffs(input->bytes, splice_input->bytes, MIN((s64)input->len, (s64)splice_input->len), &f_diff, &l_diff);
+
+  } while (f_diff < 0 || l_diff < 2 || f_diff == l_diff);
+
+  /* Split somewhere between the first and last differing byte. */
+
+  u32 split_at = f_diff + afl_rand_below(&engine->rand, l_diff - f_diff);
+
+  /* Do the thing. */
+
+  input->len = splice_input->len;
+
+  /* Let's use the mutate_buf for splicing */
+  mutator->mutate_buf = afl_realloc(mutator->mutate_buf, input->len);
+  memcpy(mutator->mutate_buf, input->bytes, split_at);
+  input->bytes = mutator->mutate_buf;
+  memcpy(input->bytes + split_at, splice_input->bytes + split_at, splice_input->len - split_at);
+}
+
+afl_ret_t afl_mutator_scheduled_add_havoc_funcs(afl_mutator_scheduled_t *mutator)
+{
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_flip_byte), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_flip_2_bytes), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_flip_4_bytes), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_delete_bytes), { return err; });
+  //AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_clone_bytes), { return err; }); //  fa timeouttare mils se configurato con dimensione max piccola
+  // contiene bug di realloc. workaround dentro la funzione ma da controllare.
+  //  bug scomparso a quanto pare... ? 
+  // update: in realtÃ  sembra dipendere adlla lunghezza del seed
+  // se il seed Ã¨ troppo corto, realloc
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_flip_bit), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_flip_2_bits), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_flip_4_bits), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_random_byte_add_sub), { return err; });
+  AFL_TRY(mutator->funcs.add_func(mutator, afl_mutfunc_random_byte), { return err; });
+
+  return AFL_RET_SUCCESS;
+}
+
+afl_ret_t afl_mutator_deterministic_init(afl_mutator_deterministic_t *det_mut, det_mutate_func mutate_func,
+                                         size_t (*get_iters)(afl_mutator_deterministic_t *, afl_input_t *))
+{
+
+  det_mut->base.funcs.mutate = mutate_func;
+  det_mut->funcs.get_iters = get_iters;
+
+  det_mut->stage_cur = 0;
+  det_mut->stage_max = 0;
+
+  return AFL_RET_SUCCESS;
+}
+
+void afl_mutator_deterministic_deinit(afl_mutator_deterministic_t *det_stage)
+{
+
+  afl_mutator_deinit(&det_stage->base);
+}
+
+#define FLIP_BIT(token, bit)                              \
+  {                                                       \
+                                                          \
+    u8 *token_case = (u8 *)(token);                       \
+    u32 bit_cast = (u32)(bit);                            \
+    token_case[bit_cast >> 3] ^= (128 >> (bit_cast & 7)); \
+  }
+
+/* All the above mutators, but as deterministic form. */
+
+size_t afl_mutate_bitflip_det(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  afl_mutator_deterministic_t *det_mutator = (afl_mutator_deterministic_t *)mutator;
+
+  FLIP_BIT(input->bytes, det_mutator->stage_cur);
+  return input->len;
+}
+
+size_t afl_get_iters_bitflip_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input)
+{
+
+  (void)det_mut;
+  return (input->len << 3);
+}
+
+size_t afl_mutate_det_flip_two(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  afl_mutator_deterministic_t *det_mutator = (afl_mutator_deterministic_t *)mutator;
+
+  FLIP_BIT(input->bytes, det_mutator->stage_cur);
+  FLIP_BIT(input->bytes, det_mutator->stage_cur + 1);
+  return input->len;
+}
+
+size_t afl_get_iters_flip_two_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input)
+{
+
+  (void)det_mut;
+  return ((input->len << 3) - 1);
+}
+
+size_t afl_mutate_det_flip_four(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  afl_mutator_deterministic_t *det_mutator = (afl_mutator_deterministic_t *)mutator;
+
+  FLIP_BIT(input->bytes, det_mutator->stage_cur);
+  FLIP_BIT(input->bytes, det_mutator->stage_cur + 1);
+  FLIP_BIT(input->bytes, det_mutator->stage_cur + 2);
+  FLIP_BIT(input->bytes, det_mutator->stage_cur + 3);
+  return input->len;
+}
+
+size_t afl_get_iters_flip_four_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input)
+{
+
+  (void)det_mut;
+  return ((input->len << 3) - 3);
+}
+
+size_t afl_mutate_det_flip_byte(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  afl_mutator_deterministic_t *det_mutator = (afl_mutator_deterministic_t *)mutator;
+
+  input->bytes[det_mutator->stage_cur] ^= 0xff;
+  return input->len;
+}
+
+size_t afl_get_iters_flip_byte_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input)
+{
+
+  (void)det_mut;
+  return (input->len);
+}
+
+size_t afl_mutate_det_flip_two_byte(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  afl_mutator_deterministic_t *det_mutator = (afl_mutator_deterministic_t *)mutator;
+
+  *(u16 *)(input->bytes + det_mutator->stage_cur) ^= 0xffff;
+  return input->len;
+}
+
+size_t afl_get_iters_flip_two_byte_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input)
+{
+
+  (void)det_mut;
+  return (input->len - 1);
+}
+
+size_t afl_mutate_det_flip_four_byte(afl_mutator_t *mutator, afl_input_t *input)
+{
+
+  afl_mutator_deterministic_t *det_mutator = (afl_mutator_deterministic_t *)mutator;
+
+  *(u32 *)(input->bytes + det_mutator->stage_cur) ^= 0xffffffff;
+  return input->len;
+}
+
+size_t afl_get_iters_flip_four_byte_det(afl_mutator_deterministic_t *det_mut, afl_input_t *input)
+{
+
+  (void)det_mut;
+  return (input->len - 3);
+}
diff -ruN qemu/libAFL/observer.c qemu_patched/libAFL/observer.c
--- qemu/libAFL/observer.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/observer.c	2023-11-12 18:19:17.585082649 +0100
@@ -0,0 +1,116 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#include "libAFL/observer.h"
+#include "libAFL/afl-returns.h"
+#include "libAFL/common.h"
+
+afl_ret_t afl_observer_init(afl_observer_t *channel) {
+
+  (void)channel;
+
+  channel->tag = AFL_OBSERVER_TAG_BASE;
+  return AFL_RET_SUCCESS;
+
+}
+
+void afl_observer_deinit(afl_observer_t *channel) {
+
+  channel->tag = AFL_DEINITIALIZED;
+  (void)channel;
+
+}
+
+void afl_observer_flush(afl_observer_t *channel) {
+
+  (void)channel;
+
+  /* TODO: Implementation */
+  return;
+
+}
+
+void afl_observer_reset(afl_observer_t *channel) {
+
+  (void)channel;
+
+  /* TODO: Implementation */
+  return;
+
+}
+
+void afl_observer_post_exec(afl_observer_t *channel) {
+
+  (void)channel;
+  /* TODO: Implementation */
+  return;
+
+}
+
+afl_ret_t afl_observer_covmap_init(afl_observer_covmap_t *map_channel, size_t map_size) {
+
+  afl_observer_init(&map_channel->base);
+  map_channel->base.tag = AFL_OBSERVER_TAG_COVMAP;
+
+  if (!afl_shmem_init(&map_channel->shared_map, map_size)) { return AFL_RET_ERROR_INITIALIZE; }
+
+  map_channel->base.funcs.reset = afl_observer_covmap_reset;
+
+  map_channel->funcs.get_map_size = afl_observer_covmap_get_map_size;
+  map_channel->funcs.get_trace_bits = afl_observer_covmap_get_trace_bits;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+void afl_observer_covmap_deinit(afl_observer_covmap_t *map_channel) {
+
+  afl_shmem_deinit(&map_channel->shared_map);
+
+  afl_observer_deinit(&map_channel->base);
+
+}
+
+void afl_observer_covmap_reset(afl_observer_t *channel) {
+
+  afl_observer_covmap_t *map_channel = (afl_observer_covmap_t *)channel;
+
+  memset(map_channel->shared_map.map, 0, map_channel->shared_map.map_size);
+
+}
+
+u8 *afl_observer_covmap_get_trace_bits(afl_observer_covmap_t *obs_channel) {
+
+  return obs_channel->shared_map.map;
+
+}
+
+size_t afl_observer_covmap_get_map_size(afl_observer_covmap_t *obs_channel) {
+
+  return obs_channel->shared_map.map_size;
+
+}
+
diff -ruN qemu/libAFL/os.c qemu_patched/libAFL/os.c
--- qemu/libAFL/os.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/os.c	2023-10-31 01:31:46.453897433 +0100
@@ -0,0 +1,655 @@
+#ifndef _GNU_SOURCE
+  #define _GNU_SOURCE 1
+#endif
+
+#include <signal.h>
+#include <assert.h>
+#include "libAFL/types.h" // <types.h>
+#include <unistd.h>
+#include <stdbool.h>
+#include <dirent.h>
+#include <sys/wait.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <limits.h>
+#include <sched.h>
+#include <ctype.h>
+#include "libAFL/os.h"
+#include "libAFL/engine.h"
+#include "libAFL/xxh3.h"
+
+#if defined(__APPLE__) || defined(__FreeBSD__) || defined(__OpenBSD__) || \
+    defined(__NetBSD__) || defined(__DragonFly__)
+  #include <sys/sysctl.h>
+#endif                           /* __APPLE__ || __FreeBSD__ || __OpenBSD__ */
+
+#if defined(__linux__) || defined(__FreeBSD__) || defined(__NetBSD__) || \
+    defined(__DragonFly__) || defined(__sun)
+  #define HAVE_AFFINITY 1
+  #if defined(__FreeBSD__) || defined(__DragonFly__)
+    #include <sys/param.h>
+    #if defined(__FreeBSD__)
+      #include <sys/cpuset.h>
+    #endif
+    #include <sys/user.h>
+    #include <pthread.h>
+    #include <pthread_np.h>
+    #define cpu_set_t cpuset_t
+  #elif defined(__NetBSD__)
+    #include <pthread.h>
+  #elif defined(__sun)
+    #include <sys/types.h>
+    #include <kstat.h>
+    #include <sys/sysinfo.h>
+    #include <sys/pset.h>
+  #endif
+#endif                                                         /* __linux__ */
+
+// Process related functions
+
+void _afl_process_init_internal(afl_os_t *afl_os) {
+
+  afl_os->fork = afl_proc_fork;
+
+  afl_os->resume = afl_proc_resume;
+  afl_os->wait = afl_proc_wait;
+  afl_os->suspend = afl_proc_suspend;
+
+}
+
+afl_fork_result_t afl_proc_fork(afl_os_t *afl_os) {
+
+  pid_t child = fork();
+
+  if (child == 0)
+    return CHILD;
+  else if (child < 0)
+    return FORK_FAILED;
+
+  afl_os->handler_process = child;
+  return PARENT;
+
+}
+
+void afl_proc_suspend(afl_os_t *afl_os) {
+
+  kill(afl_os->handler_process, SIGSTOP);
+
+}
+
+void afl_proc_resume(afl_os_t *afl_os) {
+
+  kill(afl_os->handler_process, SIGCONT);
+
+}
+
+afl_exit_t afl_proc_wait(afl_os_t *afl_os, bool untraced) {
+
+  int status = 0;
+  if (waitpid((afl_os->handler_process), &status, untraced ? WUNTRACED : 0) < 0)
+    return -1;  // Waitpid fails here, how should we handle this?
+
+  if (WIFEXITED(status)) return AFL_EXIT_OK;
+
+  // If the afl_os was simply stopped , we return AFL_EXIT_STOP
+  if (WIFSTOPPED(status)) return AFL_EXIT_STOP;
+
+  // If the afl_os exited with a signal, we check the corresponsing signum of
+  // the afl_os and return values correspondingly
+  if (WIFSIGNALED(status)) {
+
+    int signal_num = WTERMSIG(status);  // signal number
+    switch (signal_num) {
+
+      case SIGKILL:
+        return AFL_EXIT_TIMEOUT;
+      case SIGSEGV:
+        return AFL_EXIT_SEGV;
+      case SIGABRT:
+        return AFL_EXIT_ABRT;
+      case SIGBUS:
+        return AFL_EXIT_BUS;
+      case SIGILL:
+        return AFL_EXIT_ILL;
+      default:
+        /* Any other SIGNAL we need to take care of? */
+        return AFL_EXIT_CRASH;
+
+    }
+
+  }
+
+  else {
+
+    FATAL("BUG: Currently Unhandled");
+
+  }
+
+}
+
+static afl_ret_t __afl_for_each_file(char *dirpath, bool (*handle_file)(char *filename, void *data), void *data) {
+
+  DIR *          dir_in = NULL;
+  struct dirent *dir_ent = NULL;
+  char           infile[PATH_MAX];
+  uint32_t       ok = 0;
+
+  if (!(dir_in = opendir(dirpath))) { return AFL_RET_FILE_OPEN_ERROR; }
+
+  while ((dir_ent = readdir(dir_in))) {
+
+    if (dir_ent->d_name[0] == '.') {
+
+      continue;  // skip anything that starts with '.'
+
+    }
+
+    snprintf((char *)infile, sizeof(infile), "%s/%s", dirpath, dir_ent->d_name);
+    infile[sizeof(infile) - 1] = '\0';
+
+    /* TODO: Error handling? */
+    struct stat st;
+    if (access(infile, R_OK) != 0 || stat(infile, &st) != 0) { continue; }
+    if (S_ISDIR(st.st_mode)) {
+
+      if (__afl_for_each_file(infile, handle_file, data) == AFL_RET_SUCCESS) { ok = 1; }
+      continue;
+
+    }
+
+    if (!S_ISREG(st.st_mode) || st.st_size == 0) { continue; }
+
+    if (handle_file(infile, data) == true) { ok = 1; }
+
+  }
+
+  closedir(dir_in);
+
+  if (ok) {
+
+    return AFL_RET_SUCCESS;
+
+  } else {
+
+    return AFL_RET_EMPTY;
+
+  }
+
+}
+
+/* Run `handle_file` for each file in the dirpath, recursively.
+void *data will be passed to handle_file as 2nd param.
+if handle_file returns false, further execution stops. */
+afl_ret_t afl_for_each_file(char *dirpath, bool (*handle_file)(char *filename, void *data), void *data) {
+
+  size_t dir_name_size = strlen(dirpath);
+  if (dirpath[dir_name_size - 1] == '/') { dirpath[dir_name_size - 1] = '\0'; }
+  if (access(dirpath, R_OK | X_OK) != 0) return AFL_RET_FILE_OPEN_ERROR;
+
+  return __afl_for_each_file(dirpath, handle_file, data);
+
+}
+
+
+/* WIP: Let's implement a simple function which binds the cpu to the current process
+   The code is very similar to how we do it in AFL++ */
+
+/* bind process to a specific cpu. Returns 0 on failure. */
+
+static u8 bind_cpu(s32 cpuid) {
+
+  #if defined(__linux__) || defined(__FreeBSD__) || defined(__DragonFly__)
+  cpu_set_t c;
+  #elif defined(__NetBSD__)
+  cpuset_t *c;
+  #elif defined(__sun)
+  psetid_t c;
+  #endif
+
+  #if defined(__linux__) || defined(__FreeBSD__) || defined(__DragonFly__)
+
+  CPU_ZERO(&c);
+  CPU_SET(cpuid, &c);
+
+  #elif defined(__NetBSD__)
+
+  c = cpuset_create();
+  if (c == NULL) { PFATAL("cpuset_create failed"); }
+  cpuset_set(cpuid, c);
+
+  #elif defined(__sun)
+
+  pset_create(&c);
+  if (pset_assign(c, cpuid, NULL)) { PFATAL("pset_assign failed"); }
+
+  #endif
+
+  #if defined(__linux__)
+
+  return (sched_setaffinity(0, sizeof(c), &c) == 0);
+
+  #elif defined(__FreeBSD__) || defined(__DragonFly__)
+
+  return (pthread_setaffinity_np(pthread_self(), sizeof(c), &c) == 0);
+
+  #elif defined(__NetBSD__)
+
+  if (pthread_setaffinity_np(pthread_self(), cpuset_size(c), c)) {
+
+    cpuset_destroy(c);
+    return 0;
+
+  }
+
+  cpuset_destroy(c);
+  return 1;
+
+  #elif defined(__sun)
+
+  if (pset_bind(c, P_PID, getpid(), NULL)) {
+
+    pset_destroy(c);
+    return 0;
+
+  }
+
+  pset_destroy(c);
+  return 1;
+
+  #else
+
+  (void) cpuid;
+  // this will need something for other platforms
+  // TODO: Solaris/Illumos has processor_bind ... might worth a try
+  WARNF("Cannot bind to CPU yet on this platform.");
+  return 1;
+
+  #endif
+
+}
+
+
+
+/* Get the number of runnable processes, with some simple smoothing. */
+
+double get_runnable_processes(void) {
+
+  double res = 0;
+
+#if defined(__APPLE__) || defined(__FreeBSD__) || defined(__OpenBSD__) || \
+    defined(__NetBSD__) || defined(__DragonFly__)
+
+  /* I don't see any portable sysctl or so that would quickly give us the
+     number of runnable processes; the 1-minute load average can be a
+     semi-decent approximation, though. */
+
+  if (getloadavg(&res, 1) != 1) return 0;
+
+#else
+
+  /* On Linux, /proc/stat is probably the best way; load averages are
+     computed in funny ways and sometimes don't reflect extremely short-lived
+     processes well. */
+
+  FILE *f = fopen("/proc/stat", "r");
+  char tmp[1024];
+  u32 val = 0;
+
+  if (!f) { return 0; }
+
+  while (fgets(tmp, sizeof(tmp), f)) {
+
+    if (!strncmp(tmp, "procs_running ", 14) ||
+        !strncmp(tmp, "procs_blocked ", 14)) {
+
+      val += atoi(tmp + 14);
+
+    }
+
+  }
+
+  fclose(f);
+
+  if (!res) {
+
+    res = val;
+
+  } else {
+
+    res = res * (1.0 - 1.0 / AVG_SMOOTHING) +
+          ((double)val) * (1.0 / AVG_SMOOTHING);
+
+  }
+
+#endif          /* ^(__APPLE__ || __FreeBSD__ || __OpenBSD__ || __NetBSD__) */
+
+  return res;
+
+}
+
+/* Count the number of logical CPU cores. */
+
+s32 get_core_count() {
+
+  s32 cpu_core_count = 0;
+
+#if defined(__APPLE__) || defined(__FreeBSD__) || defined(__OpenBSD__) || \
+    defined(__DragonFly__)
+
+  size_t s = sizeof(cpu_core_count);
+
+  /* On *BSD systems, we can just use a sysctl to get the number of CPUs. */
+
+  #ifdef __APPLE__
+
+  if (sysctlbyname("hw.logicalcpu", &cpu_core_count, &s, NULL, 0) < 0)
+    return 0;
+
+  #else
+
+  int s_name[2] = {CTL_HW, HW_NCPU};
+
+  if (sysctl(s_name, 2, &cpu_core_count, &s, NULL, 0) < 0) return 0;
+
+  #endif                                                      /* ^__APPLE__ */
+
+#else
+
+  #ifdef HAVE_AFFINITY
+
+  cpu_core_count = sysconf(_SC_NPROCESSORS_ONLN);
+
+  #else
+
+  FILE *f = fopen("/proc/stat", "r");
+  char    tmp[1024];
+
+  if (!f) return 0;
+
+  while (fgets(tmp, sizeof(tmp), f))
+    if (!strncmp(tmp, "cpu", 3) && isdigit(tmp[3])) ++cpu_core_count;
+
+  fclose(f);
+
+  #endif                                                  /* ^HAVE_AFFINITY */
+
+#endif                        /* ^(__APPLE__ || __FreeBSD__ || __OpenBSD__) */
+
+  if (cpu_core_count > 0) {
+
+    u32 cur_runnable = 0;
+
+    cur_runnable = (u32)get_runnable_processes();
+
+#if defined(__APPLE__) || defined(__FreeBSD__) || defined(__OpenBSD__) || \
+    defined(__DragonFly__)
+
+    /* Add ourselves, since the 1-minute average doesn't include that yet. */
+
+    ++cur_runnable;
+
+#endif                           /* __APPLE__ || __FreeBSD__ || __OpenBSD__ */
+
+    OKF("You have %d CPU core%s and %u runnable tasks (utilization: %0.0f%%).",
+        cpu_core_count, cpu_core_count > 1 ? "s" : "", cur_runnable,
+        cur_runnable * 100.0 / cpu_core_count);
+
+    if (cpu_core_count > 1) {
+
+      if (cur_runnable > cpu_core_count * 1.5) {
+
+        WARNF("System under apparent load, performance may be spotty.");
+
+      } else if ((s64)cur_runnable + 1 <= (s64)cpu_core_count) {
+
+        OKF("Try parallel jobs");
+
+      }
+
+    }
+
+  } else {
+
+    cpu_core_count = 0;
+    WARNF("Unable to figure out the number of CPU cores.");
+
+  }
+
+  return cpu_core_count;
+
+}
+
+
+afl_ret_t bind_to_cpu() {
+
+  u8 cpu_used[4096];
+
+  s32 i;
+
+  #if defined(__linux__)
+
+  // Let's open up /proc and check if there are any CPU cores available
+  DIR * proc;
+  struct dirent * dir_entry;
+
+  proc = opendir("/proc");
+
+  while ((dir_entry = readdir(proc))) {
+
+    if (!isdigit(dir_entry->d_name[0])) { continue; } // Leave files which aren't process files
+
+    char fn[PATH_MAX];
+    char tmp[MAX_LINE];
+
+    FILE *f;
+    u8 has_vmsize = 0;
+
+    snprintf(fn, PATH_MAX, "/proc/%s/status", dir_entry->d_name);
+
+    if (!(f = fopen(fn, "r"))) { continue; }
+
+    while (fgets(tmp, MAX_LINE, f)) {
+
+      u32 hval;
+
+      /* Processes without VmSize are probably kernel tasks. */
+
+      if (!strncmp(tmp, "VmSize:\t", 8)) { has_vmsize = 1; }
+
+      if (!strncmp(tmp, "Cpus_allowed_list:\t", 19) && !strchr(tmp, '-') &&
+          !strchr(tmp, ',') && sscanf(tmp + 19, "%u", &hval) == 1 &&
+          hval < sizeof(cpu_used) && has_vmsize) {
+
+        cpu_used[hval] = 1;
+        break;
+
+      }
+
+    }
+
+    fclose(f);
+ 
+
+  }
+
+  closedir(proc);
+
+
+  #elif defined(__FreeBSD__) || defined(__DragonFly__)
+
+  struct kinfo_proc *procs;
+  size_t             nprocs;
+  s32                proccount;
+  int                s_name[] = {CTL_KERN, KERN_PROC, KERN_PROC_ALL};
+  size_t             s_name_l = sizeof(s_name) / sizeof(s_name[0]);
+
+  if (sysctl(s_name, s_name_l, NULL, &nprocs, NULL, 0) != 0) {
+
+    return AFL_RET_UNKNOWN_ERROR;
+
+  }
+
+  proccount = nprocs / sizeof(*procs);
+  nprocs = nprocs * 4 / 3;
+
+  procs = ck_alloc(nprocs);
+  if (sysctl(s_name, s_name_l, procs, &nprocs, NULL, 0) != 0) {
+
+    ck_free(procs);
+    return AFL_RET_UNKNOWN_ERROR;
+
+  }
+
+  for (i = 0; i < proccount; i++) {
+
+    #if defined(__FreeBSD__)
+
+    if (!strcmp(procs[i].ki_comm, "idle")) continue;
+
+    // fix when ki_oncpu = -1
+    int oncpu;
+    oncpu = procs[i].ki_oncpu;
+    if (oncpu == -1) oncpu = procs[i].ki_lastcpu;
+
+    if (oncpu != -1 && (size_t)oncpu < sizeof(cpu_used) && procs[i].ki_pctcpu > 60)
+      cpu_used[oncpu] = 1;
+
+    #elif defined(__DragonFly__)
+
+    if (procs[i].kp_lwp.kl_cpuid < (s32)(sizeof(cpu_used)) &&
+        procs[i].kp_lwp.kl_pctcpu > 10)
+      cpu_used[procs[i].kp_lwp.kl_cpuid] = 1;
+
+    #endif
+
+  }
+
+  ck_free(procs);
+
+  #elif defined(__NetBSD__)
+
+  struct kinfo_proc2 *procs;
+  size_t              nprocs;
+  s32                 proccount;
+  int                 s_name[] = {
+
+      CTL_KERN, KERN_PROC2, KERN_PROC_ALL, 0, sizeof(struct kinfo_proc2), 0};
+  size_t s_name_l = sizeof(s_name) / sizeof(s_name[0]);
+
+  if (sysctl(s_name, s_name_l, NULL, &nprocs, NULL, 0) != 0) {
+
+    return AFL_RET_UNKNOWN_ERROR;
+
+  }
+
+  proccount = nprocs / sizeof(struct kinfo_proc2);
+  procs = ck_alloc(nprocs * sizeof(struct kinfo_proc2));
+  s_name[5] = proccount;
+
+  if (sysctl(s_name, s_name_l, procs, &nprocs, NULL, 0) != 0) {
+
+    ck_free(procs);
+    return AFL_RET_UNKNOWN_ERROR;
+
+  }
+
+  for (i = 0; i < proccount; i++) {
+
+    if (procs[i].p_cpuid < sizeof(cpu_used) && procs[i].p_pctcpu > 0)
+      cpu_used[procs[i].p_cpuid] = 1;
+
+  }
+
+  ck_free(procs);
+
+  #elif defined(__sun)
+
+  kstat_named_t *n;
+  kstat_ctl_t *  m;
+  kstat_t *      k;
+  cpu_stat_t     cs;
+  u32            ncpus;
+
+  m = kstat_open();
+
+  if (!m) FATAL("kstat_open failed");
+
+  k = kstat_lookup(m, "unix", 0, "system_misc");
+
+  if (!k) {
+
+    kstat_close(m);
+    return AFL_RET_UNKNOWN_ERROR;
+
+  }
+
+  if (kstat_read(m, k, NULL)) {
+
+    kstat_close(m);
+    return AFL_RET_UNKNOWN_ERROR;
+
+  }
+
+  n = kstat_data_lookup(k, "ncpus");
+  ncpus = n->value.i32;
+
+  if (ncpus > sizeof(cpu_used)) ncpus = sizeof(cpu_used);
+
+  for (i = 0; i < ncpus; i++) {
+
+    k = kstat_lookup(m, "cpu_stat", i, NULL);
+    if (kstat_read(m, k, &cs)) {
+
+      kstat_close(m);
+      return AFL_RET_UNKNOWN_ERROR;
+
+    }
+
+    if (cs.cpu_sysinfo.cpu[CPU_IDLE] > 0) continue;
+
+    if (cs.cpu_sysinfo.cpu[CPU_USER] > 0 || cs.cpu_sysinfo.cpu[CPU_KERNEL] > 0)
+      cpu_used[i] = 1;
+
+  }
+
+  kstat_close(m);
+
+  #else
+    #warning \
+        "For this platform we do not have free CPU binding code yet. If possible, please supply a PR to https://github.com/AFLplusplus/libAFL"
+  #endif
+  size_t cpu_start = 0;
+  s32 cpu_core_count = get_core_count();
+
+  #if !defined(__ANDROID__)
+
+  for (i = cpu_start; i < cpu_core_count; i++) {
+
+  #else
+
+  /* for some reason Android goes backwards */
+
+  for (i = cpu_core_count - 1; i > -1; i--) {
+
+  #endif
+
+    if (cpu_used[i]) { continue; }
+
+    OKF("Found a free CPU core, try binding to #%u.", i);
+
+    if (bind_cpu(i)) {
+
+      /* Success :) */
+      break;
+
+    }
+
+    WARNF("setaffinity failed to CPU %d, trying next CPU", i);
+    cpu_start++;
+
+  }
+
+  return AFL_RET_SUCCESS;
+
+
+}
diff -ruN qemu/libAFL/queue.c qemu_patched/libAFL/queue.c
--- qemu/libAFL/queue.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/queue.c	2023-11-29 19:14:55.096245532 +0100
@@ -0,0 +1,464 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+                     Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+                     Andrea Fioraldi <andreafioraldi@gmail.com>,
+                     Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   This is the Library based on AFL++ which can be used to build
+   customized fuzzers for a specific target while taking advantage of
+   a lot of features that AFL++ already provides.
+
+ */
+
+#include <sys/stat.h>
+#include <stdio.h>
+
+#include "libAFL/queue.h"
+#include "libAFL/feedback.h"
+#include "libAFL/engine.h"
+#include "libAFL/fuzzone.h"
+#include "libAFL/stage.h"
+#include "libAFL/mutator.h"
+#include "libAFL/config.h"
+#include "libAFL/alloc-inl.h"
+
+/* We start with the implementation of queue_entry functions here. */
+afl_ret_t afl_entry_init(afl_entry_t *entry, afl_input_t *input, afl_entry_info_t *info) {
+
+  entry->input = input;
+  if (!info) {
+
+    entry->info = calloc(1, sizeof(afl_entry_info_t));
+    if (!entry->info) { return AFL_RET_ALLOC; }
+    entry->info_calloc = 1;
+
+  } else {
+
+    entry->info = info;
+
+  }
+  
+  entry->info->det_done = 0;
+
+  entry->funcs.get_input = afl_entry_get_input;
+  entry->funcs.get_next = afl_entry_get_next;
+  entry->funcs.get_prev = afl_entry_get_prev;
+  entry->funcs.get_parent = afl_entry_get_parent;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+void afl_entry_deinit(afl_entry_t *entry) {
+
+	/* printf("\nSONO IN: afl_entry_deinit\n"); */
+	/* We remove the element from the linked-list */
+	
+	if (entry->next) { entry->next->prev = entry->prev; }
+	
+	if (entry->prev) { entry->prev->next = entry->next; }
+	
+	/* we also delete the input associated with it */
+	entry->input->funcs.delete(entry->input);
+	
+	/* and the info structure */
+	if (entry->info_calloc) { free(entry->info); }
+	
+	/*
+	// Unneeded as the structure is free'd via the macro
+	entry->next = NULL;
+	entry->prev = NULL;
+	entry->queue = NULL;
+	entry->parent = NULL;
+	entry->info = NULL;
+	entry->input = NULL;
+	*/
+
+}
+
+/* Default implementations for the queue entry vtable functions */
+afl_input_t *afl_entry_get_input(afl_entry_t *entry) {
+
+  return entry->input;
+
+}
+
+afl_entry_t *afl_entry_get_next(afl_entry_t *entry) {
+
+  return entry->next;
+
+}
+
+afl_entry_t *afl_entry_get_prev(afl_entry_t *entry) {
+
+  return entry->prev;
+
+}
+
+afl_entry_t *afl_entry_get_parent(afl_entry_t *entry) {
+
+  return entry->parent;
+
+}
+
+/* We implement the queue based functions now. */
+
+afl_ret_t afl_queue_init(afl_queue_t *queue) {
+
+  queue->entries = NULL;
+  queue->save_to_files = false;
+  queue->fuzz_started = false;
+  queue->entries_count = 0;
+  queue->base = NULL;
+  queue->current = 0;
+  memset(queue->dirpath, 0, PATH_MAX);
+
+  queue->funcs.insert = afl_queue_insert;
+  queue->funcs.get_size = afl_queue_get_size;
+  queue->funcs.get_dirpath = afl_queue_get_dirpath;
+  queue->funcs.get_names_id = afl_queue_get_names_id;
+  queue->funcs.get_save_to_files = afl_queue_should_save_to_file;
+ queue->funcs.set_dirpath = afl_queue_set_dirpath; 
+  queue->funcs.set_engine = afl_queue_set_engine;
+  queue->funcs.get_next_in_queue = afl_queue_next_base_queue;
+  queue->funcs.get_queue_entry = afl_queue_get_entry;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+void afl_queue_deinit(afl_queue_t *queue) {
+
+  /*TODO: Clear the queue entries too here*/
+
+  afl_entry_t *entry = queue->base;
+
+  while (entry) {
+
+    /* Grab the next entry of queue */
+    afl_entry_t *next_entry = entry->next;
+
+    /* We destroy the queue, since none of the entries have references anywhere
+     * else anyways */
+    afl_entry_delete(entry);
+
+    entry = next_entry;
+
+  }
+
+  afl_free(queue->entries);
+
+  queue->base = NULL;
+  queue->current = 0;
+  queue->entries_count = 0;
+  queue->fuzz_started = false;
+
+}
+
+/* *** Possible error cases here? *** */
+afl_ret_t afl_queue_insert(afl_queue_t *queue, afl_entry_t *entry) 
+{
+	if (!entry->input) 
+	{
+		/* Never add an entry with NULL input, something's wrong! */
+		printf("\nQueue entry with NULL input\n");
+		return AFL_RET_NULL_PTR;
+	}
+  
+	/* Before we add the entry to the queue, we call the custom mutators
+	// get_next_in_queue function, so that it can gain some extra info from the
+	// fuzzed queue(especially helpful in case of grammar mutator, e.g see hogfuzz
+	// mutator AFL++) */
+	
+	afl_fuzz_one_t *fuzz_one = queue->engine->fuzz_one;
+	
+	if (fuzz_one) 
+	{	
+		size_t i;
+		for (i = 0; i < fuzz_one->stages_count; ++i) 
+		{
+			afl_stage_t *stage = fuzz_one->stages[i];
+			size_t j;
+			for (j = 0; j < stage->mutators_count; ++j) 
+			{
+	
+				if (stage->mutators[j]->funcs.custom_queue_new_entry) {
+					stage->mutators[j]->funcs.custom_queue_new_entry(stage->mutators[j], entry);
+				}
+			}
+		}
+	}
+	
+	queue->entries_count++;
+	queue->entries = afl_realloc(queue->entries, queue->entries_count * sizeof(afl_entry_t *));
+	if (!queue->entries) { return AFL_RET_ALLOC; }
+	
+	queue->entries[queue->entries_count - 1] = entry;
+	
+	/* Let's save the entry to disk */
+	
+  printf("adding input %s!\n\n\n",entry->input->bytes);
+
+	entry->input->funcs.save_to_file(entry->input, COV_FILE, "test");
+	
+
+	//entry->input->funcs.save_to_file(entry->input, COV_FILE, "test");
+	
+	return AFL_RET_SUCCESS;
+
+}
+
+size_t afl_queue_get_size(afl_queue_t *queue) {
+
+  return queue->entries_count;
+
+}
+
+char *afl_queue_get_dirpath(afl_queue_t *queue) {
+
+  return queue->dirpath;
+
+}
+
+size_t afl_queue_get_names_id(afl_queue_t *queue) {
+
+  return queue->names_id;
+
+}
+
+bool afl_queue_should_save_to_file(afl_queue_t *queue) {
+
+  return queue->save_to_files;
+
+}
+
+
+void afl_queue_set_dirpath(afl_queue_t *queue, char *new_dirpath) {
+
+  if (new_dirpath) {
+
+    strcpy(queue->dirpath, new_dirpath);
+    /* Let's create the directory if it's not already created */
+    struct stat dir;
+
+    if (!((stat(queue->dirpath, &dir) == 0) && (S_ISDIR(dir.st_mode)))) {
+
+      if (mkdir(queue->dirpath, 0777) != 0) { WARNF("Error creating queue directory"); };
+
+    }
+
+  } else {
+
+    memset(queue->dirpath, 0, PATH_MAX);  // We are unsetting the directory path
+
+  }
+
+  queue->save_to_files = true;
+  // If the dirpath is empty, we make the save_to_files bool as false
+  if (!queue->dirpath[0]) { queue->save_to_files = false; }
+
+}
+
+
+void afl_queue_set_engine(afl_queue_t *queue, afl_engine_t *engine) {
+
+  queue->engine = engine;
+  if (engine) { queue->engine_id = engine->id; }
+
+}
+
+afl_entry_t *afl_queue_get_entry(afl_queue_t *queue, u32 entry) {
+
+  if (queue->entries_count <= entry) { return NULL; }
+  return queue->entries[entry];
+
+}
+
+afl_entry_t *afl_queue_next_base_queue(afl_queue_t *queue, int engine_id) {
+	/* printf("\nafl_queue_next_base_queue\n"); */
+	
+	if (queue->entries_count) 
+	{
+	afl_entry_t *current = queue->entries[queue->current];
+	
+	if (engine_id != queue->engine_id && current->info->skip_entry) 
+	{ 
+		return current; 
+	}
+	
+	/* If some other engine grabs from the queue, don't update the queue's
+	// current entry
+	// If we reach the end of queue, start from beginning */
+	queue->current = (queue->current + 1) % queue->entries_count;
+	
+	return current;
+	
+	} else {
+	  printf("\nafl_queue_next_base_queue TERZO IF\n");
+	
+	  DBG("Empty queue at %p", queue);
+	/* Queue empty :( */
+	  return NULL;
+	
+	}
+}
+
+
+afl_ret_t afl_queue_feedback_init(afl_queue_feedback_t *feedback_queue, afl_feedback_t *feedback, char *name) {
+
+  afl_queue_init(&(feedback_queue->base));
+  feedback_queue->feedback = feedback;
+
+  if (feedback) { feedback->queue = feedback_queue; }
+
+  if (!name) { name = (char *)""; }
+
+  feedback_queue->name = name;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+void afl_queue_feedback_deinit(afl_queue_feedback_t *feedback_queue) {
+
+  feedback_queue->feedback = NULL;
+
+  afl_queue_deinit(&feedback_queue->base);
+  feedback_queue->name = NULL;
+
+}
+
+afl_ret_t afl_queue_global_init(afl_queue_global_t *global_queue) {
+
+  afl_queue_init(&(global_queue->base));
+
+  global_queue->feedback_queues_count = 0;
+  global_queue->feedback_queues = NULL;
+
+  global_queue->base.funcs.set_engine = afl_queue_global_set_engine;
+
+  global_queue->funcs.add_feedback_queue = afl_queue_global_add_feedback_queue;
+  global_queue->funcs.schedule = afl_queue_global_schedule;
+  global_queue->base.funcs.get_next_in_queue = afl_queue_next_global_queue;
+  global_queue->base.funcs.set_engine = afl_queue_global_set_engine;
+  global_queue->base.funcs.set_dirpath = afl_queue_set_dirpath;
+
+  return AFL_RET_SUCCESS;
+
+}
+
+void afl_queue_global_deinit(afl_queue_global_t *global_queue) {
+
+  /* Should we also deinit the feedback queues?? */
+  size_t i;
+
+  afl_queue_deinit(&global_queue->base);
+
+  for (i = 0; i < global_queue->feedback_queues_count; ++i) {
+
+    global_queue->feedback_queues[i] = NULL;
+
+  }
+
+  afl_free(global_queue->feedback_queues);
+  global_queue->feedback_queues = NULL;
+  global_queue->feedback_queues_count = 0;
+
+}
+
+afl_ret_t afl_queue_global_add_feedback_queue(afl_queue_global_t *global_queue, afl_queue_feedback_t *feedback_queue) {
+
+  global_queue->feedback_queues_count++;
+  global_queue->feedback_queues =
+      afl_realloc(global_queue->feedback_queues, global_queue->feedback_queues_count * sizeof(afl_queue_feedback_t *));
+  if (!global_queue->feedback_queues) {
+
+    global_queue->feedback_queues_count = 0;
+    return AFL_RET_ALLOC;
+
+  }
+
+  global_queue->feedback_queues[global_queue->feedback_queues_count - 1] = feedback_queue;
+  afl_engine_t *engine = global_queue->base.engine;
+  feedback_queue->base.funcs.set_engine(&feedback_queue->base, engine);
+  return AFL_RET_SUCCESS;
+
+}
+
+afl_entry_t *afl_queue_next_global_queue(afl_queue_t *queue, int engine_id) {
+
+  /* This is to stop from compiler complaining about the incompatible pointer
+  // type for the function ptrs. We need a better solution for this to pass the
+  // scheduled_mutator rather than the mutator as an argument. */
+	/*
+	printf("\nSONO IN: afl_queue_next_global_queue\n");
+	*/
+	
+	afl_queue_global_t *global_queue = (afl_queue_global_t *)queue;
+
+	int fbck_idx = global_queue->funcs.schedule(global_queue);
+	/* printf("\nPOST SCHEDULE - fbck_idx=%d\n", fbck_idx); */
+
+	/* ATTENZIONE: prima era != -1 */
+	if (fbck_idx != 0) 
+	{
+		afl_queue_feedback_t *feedback_queue = global_queue->feedback_queues[fbck_idx];
+	
+		afl_entry_t *  next_entry = feedback_queue->base.funcs.get_next_in_queue(&(feedback_queue->base), engine_id);
+
+		if (next_entry) 
+		{
+			return next_entry;
+	
+		} else {
+		return afl_queue_next_base_queue(queue, engine_id);
+		}
+	
+	} 
+	else {
+	/* We don't have any more entries feedback queue, so base queue it is. */
+		return afl_queue_next_base_queue(queue, engine_id);
+	}
+
+}
+
+int afl_queue_global_schedule(afl_queue_global_t *queue) {
+  return afl_rand_below(&queue->base.engine->rand, queue->feedback_queues_count);
+}
+
+/* TODO: make this a method for engine instead */
+void afl_queue_global_set_engine(afl_queue_t *global_queue_base, afl_engine_t *engine) {
+
+  size_t              i;
+  afl_queue_global_t *global_queue = (afl_queue_global_t *)global_queue_base;
+
+  /* First add engine to the global queue itself */
+
+  afl_queue_set_engine(&global_queue->base, engine);
+  /* Set engine's queue to the global queue */
+
+  if (engine) { engine->global_queue = global_queue; }
+
+  for (i = 0; i < global_queue->feedback_queues_count; ++i) {
+
+    /* Set this engine to every feedback queue in global queue */
+    global_queue->feedback_queues[i]->base.funcs.set_engine(&(global_queue->feedback_queues[i]->base), engine);
+
+  }
+
+}
+
diff -ruN qemu/libAFL/rand.c qemu_patched/libAFL/rand.c
--- qemu/libAFL/rand.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/rand.c	2023-12-02 02:53:23.515531636 +0100
@@ -0,0 +1,114 @@
+
+#include "libAFL/rand.h"
+
+u64 afl_rand_rotl(const u64 x, int k) {
+  return (x << k) | (x >> (64 - k));
+}
+
+
+
+static void afl_rand_seed(afl_rand_t *rnd, s64 init_seed) {
+
+  rnd->init_seed = init_seed;
+  rnd->fixed_seed = true;
+  rnd->rand_seed[0] = XXH64((u8 *)&rnd->init_seed, sizeof(rnd->init_seed), HASH_CONST);
+  rnd->rand_seed[1] = rnd->rand_seed[0] ^ 0x1234567890abcdefULL;
+  rnd->rand_seed[2] = rnd->rand_seed[0] & 0x0123456789abcdefULL;
+  rnd->rand_seed[3] = rnd->rand_seed[0] | 0x01abcde43f567908ULL;
+
+}
+
+/* get the next random number */
+
+u64 afl_rand_next(afl_rand_t *rnd) {
+
+  const uint64_t result = afl_rand_rotl(rnd->rand_seed[0] + rnd->rand_seed[3], 23) + rnd->rand_seed[0];
+
+  const uint64_t t = rnd->rand_seed[1] << 17;
+
+  rnd->rand_seed[2] ^= rnd->rand_seed[0];
+  rnd->rand_seed[3] ^= rnd->rand_seed[1];
+  rnd->rand_seed[1] ^= rnd->rand_seed[2];
+  rnd->rand_seed[0] ^= rnd->rand_seed[3];
+
+  rnd->rand_seed[2] ^= t;
+
+  rnd->rand_seed[3] = afl_rand_rotl(rnd->rand_seed[3], 45);
+
+  return result;
+
+}
+
+
+/* get a random int below the given int (exclusive) */
+
+u64 afl_rand_below(afl_rand_t *rnd, u64 limit) 
+{
+	if (limit <= 1) { return 0; }
+	
+	/* The boundary not being necessarily a power of 2,
+	 we need to ensure the result uniformity. */
+	if (unlikely(!rnd->rand_cnt--) && likely(!rnd->fixed_seed)) 
+	{
+/*		
+  		int read_len = read(rnd->dev_urandom_fd, &rnd->rand_seed, sizeof(rnd->rand_seed));
+		(void)read_len;
+*/
+		rnd->rand_cnt = (RESEED_RNG / 2) + (rnd->rand_seed[1] % RESEED_RNG);
+	}
+	
+	/* Modulo is biased - we don't want our fuzzing to be biased so let's do it
+	right. See
+	https://stackoverflow.com/questions/10984974/why-do-people-say-there-is-modulo-bias-when-using-a-random-number-generator
+	*/
+	
+	u64 unbiased_rnd;
+	do {
+		unbiased_rnd = afl_rand_next(rnd);
+	} while (unlikely(unbiased_rnd >= (UINT64_MAX - (UINT64_MAX % limit))));
+	
+	return unbiased_rnd % limit;
+}
+
+
+/* A random number between min and max, both inclusive */
+u64 afl_rand_between(afl_rand_t *rand, u64 min, u64 max) {
+  return min + afl_rand_below(rand, max - min + 1);
+}
+
+
+/* initialize with a fixed seed (for reproducability) */
+afl_ret_t afl_rand_init_fixed_seed(afl_rand_t *rnd, s64 init_seed) {
+
+  memset(rnd, 0, sizeof(afl_rand_t));
+  afl_rand_seed(rnd, init_seed);
+  return AFL_RET_SUCCESS;
+}
+
+
+/* initialize feeded by urandom */
+/* TODO: sistemare questa funzione - workaround chiamando la funzione "afl_rand_init_fixed_seed" */
+
+afl_ret_t afl_rand_init(afl_rand_t *rnd) 
+{
+  /*
+    memset(rnd, 0, sizeof(afl_rand_t));
+  */  
+    /*rnd->dev_urandom_fd = open("/dev/urandom", O_RDONLY);*/
+  /*  
+    rnd->dev_urandom_fd = rand(); 
+    
+    if (!rnd->dev_urandom_fd) { return AFL_RET_FILE_OPEN_ERROR; }
+    rnd->fixed_seed = false;
+  */
+    /* do one call to rand_below to seed the rng */
+  
+    //afl_rand_below(rnd, 1);
+
+  	afl_rand_init_fixed_seed(rnd, 3); // was 13
+  	return AFL_RET_SUCCESS; 
+}
+
+void afl_rand_deinit(afl_rand_t *rnd) {
+  if (rnd->dev_urandom_fd) { close(rnd->dev_urandom_fd); }
+}
diff -ruN qemu/libAFL/shmem.c qemu_patched/libAFL/shmem.c
--- qemu/libAFL/shmem.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/shmem.c	2023-10-31 00:22:51.383493668 +0100
@@ -0,0 +1,209 @@
+#include <stdio.h>
+#include <string.h>
+#ifdef USEMMAP
+  #include <unistd.h>
+  #include <sys/types.h>
+  #include <sys/stat.h>
+  #include <fcntl.h>
+  #include <sys/mman.h>
+  #include <sys/types.h>
+#else
+  #include <sys/ipc.h>
+  #include <sys/shm.h>
+#endif
+
+#ifdef __ANDROID__
+  #include "android-ashmem.h"
+#endif
+
+#include "libAFL/types.h"
+#include "libAFL/shmem.h"
+
+void afl_shmem_deinit(afl_shmem_t *shm) {
+
+  if (!shm || !shm->map) {
+
+    // Not set or not initialized;
+    return;
+
+  }
+
+  shm->shm_str[0] = '\0';
+
+#ifdef USEMMAP
+  if (shm->map != NULL) {
+
+    munmap(shm->map, shm->map_size);
+    shm->map = NULL;
+
+  }
+
+  if (shm->g_shm_fd != -1) {
+
+    close(shm->g_shm_fd);
+    shm->g_shm_fd = -1;
+
+  }
+
+#else
+  shmctl(shm->shm_id, IPC_RMID, NULL);
+#endif
+
+  shm->map = NULL;
+
+}
+
+u8 *afl_shmem_init(afl_shmem_t *shm, size_t map_size) {
+
+  shm->map_size = map_size;
+
+  shm->map = NULL;
+
+#ifdef USEMMAP
+
+  shm->g_shm_fd = -1;
+
+  /* ======
+  generate random file name for multi instance
+
+  thanks to f*cking glibc we can not use tmpnam securely, it generates a
+  security warning that cannot be suppressed
+  so we do this worse workaround */
+  snprintf(shm->shm_str, 20, "/afl_%d_%ld", getpid(), random());
+
+  /* create the shared memory segment as if it was a file */
+  shm->g_shm_fd = shm_open(shm->shm_str, O_CREAT | O_RDWR | O_EXCL, 0600);
+  if (shm->g_shm_fd == -1) {
+
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+  /* configure the size of the shared memory segment */
+  if (ftruncate(shm->g_shm_fd, map_size)) {
+
+    close(shm->g_shm_fd);
+    shm_unlink(shm->shm_str);
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+  /* map the shared memory segment to the address space of the process */
+  shm->map = mmap(0, map_size, PROT_READ | PROT_WRITE, MAP_SHARED, shm->g_shm_fd, 0);
+  if (shm->map == MAP_FAILED || shm->map == ((void *)-1) || !shm->map) {
+
+    close(shm->g_shm_fd);
+    shm_unlink(shm->shm_str);
+    shm->g_shm_fd = -1;
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+#else
+
+  shm->shm_id = shmget(IPC_PRIVATE, map_size, IPC_CREAT | IPC_EXCL | 0600);
+
+  if (shm->shm_id < 0) {
+
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+  snprintf(shm->shm_str, sizeof(shm->shm_str), "%d", shm->shm_id);
+  shm->shm_str[sizeof(shm->shm_str) - 1] = '\0';
+
+  shm->map = shmat(shm->shm_id, NULL, 0);
+
+  if (shm->map == (void *)-1 || !shm->map) {
+
+    shmctl(shm->shm_id, IPC_RMID, NULL);
+    shm->shm_id = -1;
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+#endif
+
+  return shm->map;
+
+}
+
+u8 *afl_shmem_by_str(afl_shmem_t *shm, char *shm_str, size_t map_size) {
+
+  if (!shm || !shm_str || !shm_str[0] || !map_size) { return NULL; }
+  shm->map = NULL;
+
+  shm->map_size = map_size;
+  strncpy(shm->shm_str, shm_str, sizeof(shm->shm_str) - 1);
+
+#ifdef USEMMAP
+  const char *   shm_file_path = shm_str;
+  unsigned char *shm_base = NULL;
+
+  /* create the shared memory segment as if it was a file */
+  shm->g_shm_fd = shm_open(shm_file_path, O_RDWR, 0600);
+  if (shm->g_shm_fd == -1) {
+
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+  /* map the shared memory segment to the address space of the process */
+  shm_base = mmap(0, shm->map_size, PROT_READ | PROT_WRITE, MAP_SHARED, shm->g_shm_fd, 0);
+  if (shm_base == MAP_FAILED) {
+
+    close(shm->g_shm_fd);
+    shm->g_shm_fd = -1;
+    shm->map_size = 0;
+    shm->shm_str[0] = '\0';
+
+    return NULL;
+
+  }
+
+  shm->map = shm_base;
+#else
+  shm->shm_id = atoi(shm_str);
+
+  shm->map = shmat(shm->shm_id, NULL, 0);
+
+  if (shm->map == (void *)-1) {
+
+    shm->map = NULL;
+    shm->map_size = 0;
+    shm->shm_str[0] = '\0';
+    return NULL;
+
+  }
+
+#endif
+
+  return shm->map;
+
+}
+
+/* Write sharedmap as env var and the size as name#_SIZE */
+afl_ret_t afl_shmem_to_env_var(afl_shmem_t *shmem, char *env_name) {
+
+  if (!env_name || !shmem || !env_name[0] || !shmem->shm_str[0] || strlen(env_name) > 200) { return AFL_RET_NULL_PTR; }
+
+  char shm_str[256];
+  snprintf(shm_str, sizeof(shm_str), "%d", shmem->shm_id);
+  if (setenv(env_name, (char *)shm_str, 1) < 0) { return AFL_RET_ERRNO; }
+
+  /* Write the size to env, too */
+  char size_env_name[256];
+  snprintf(size_env_name, sizeof(size_env_name), "%s_SIZE", env_name);
+  snprintf(shm_str, sizeof(shm_str), "%d", shmem->shm_id);
+  if (setenv(size_env_name, (char *)shm_str, 1) < 0) { return AFL_RET_ERRNO; }
+
+  return AFL_RET_SUCCESS;
+
+}
+
diff -ruN qemu/libAFL/stage.c qemu_patched/libAFL/stage.c
--- qemu/libAFL/stage.c	1970-01-01 01:00:00.000000000 +0100
+++ qemu_patched/libAFL/stage.c	2023-12-20 14:54:09.559508181 +0100
@@ -0,0 +1,489 @@
+/*
+   american fuzzy lop++ - fuzzer header
+   ------------------------------------
+
+   Originally written by Michal Zalewski
+
+   Now maintained by Marc Heuse <mh@mh-sec.de>,
+					 Heiko EiÃfeldt <heiko.eissfeldt@hexco.de>,
+					 Andrea Fioraldi <andreafioraldi@gmail.com>,
+					 Dominik Maier <mail@dmnk.co>
+
+   Copyright 2016, 2017 Google Inc. All rights reserved.
+   Copyright 2019-2020 AFLplusplus Project. All rights reserved.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at:
+
+	 http://www.apache.org/licenses/LICENSE-2.0
+
+ */
+
+#include "libAFL/stage.h"
+#include "libAFL/engine.h"
+#include "libAFL/fuzzone.h"
+#include "libAFL/mutator.h"
+
+afl_ret_t afl_stage_init(afl_stage_t *stage, afl_engine_t *engine)
+{
+
+	stage->engine = engine;
+
+	// We also add this stage to the engine's fuzzone
+	if (engine)
+	{
+		engine->fuzz_one->funcs.add_stage(engine->fuzz_one, stage);
+	}
+
+	stage->funcs.get_iters = afl_stage_get_iters;
+	stage->funcs.perform = afl_stage_perform;
+	stage->funcs.add_mutator_to_stage = afl_stage_add_mutator;
+
+	return AFL_RET_SUCCESS;
+}
+
+void afl_stage_deinit(afl_stage_t *stage)
+{
+
+	stage->engine = NULL;
+
+	for (size_t i = 0; i < stage->mutators_count; ++i)
+	{
+
+		afl_mutator_deinit(stage->mutators[i]);
+	}
+
+	afl_free(stage->mutators);
+	stage->mutators = NULL;
+}
+
+afl_ret_t afl_stage_add_mutator(afl_stage_t *stage, afl_mutator_t *mutator)
+{
+
+	if (!stage || !mutator)
+	{
+		return AFL_RET_NULL_PTR;
+	}
+
+	stage->mutators_count++;
+	stage->mutators = afl_realloc(stage->mutators, stage->mutators_count * sizeof(afl_mutator_t *));
+	if (!stage->mutators)
+	{
+		return AFL_RET_ALLOC;
+	}
+
+	stage->mutators[stage->mutators_count - 1] = mutator;
+
+	return AFL_RET_SUCCESS;
+}
+
+size_t afl_stage_get_iters(afl_stage_t *stage)
+{
+
+	return (1 + afl_rand_below(&stage->engine->rand, 128));
+}
+
+afl_ret_t afl_stage_run(afl_stage_t *stage, afl_input_t *input, bool overwrite)
+{
+
+	afl_input_t *copy;
+	if (!overwrite)
+		copy = input->funcs.copy(input);
+	else
+		copy = input;
+
+	/* Let's post process the mutated data now. */
+	size_t j;
+	for (j = 0; j < stage->mutators_count; ++j)
+	{
+
+		afl_mutator_t *mutator = stage->mutators[j];
+
+		if (mutator->funcs.post_process)
+		{
+			mutator->funcs.post_process(mutator, copy);
+		}
+	}
+
+	afl_ret_t ret = stage->engine->funcs.execute(stage->engine, copy);
+
+	if (!overwrite)
+		afl_input_delete(copy);
+
+	return ret;
+}
+
+float afl_stage_is_interesting(afl_stage_t *stage)
+{
+
+	float interestingness = 0.0f;
+
+	afl_feedback_t **feedbacks = stage->engine->feedbacks;
+	size_t j;
+	for (j = 0; j < stage->engine->feedbacks_count; ++j)
+	{
+
+		interestingness += feedbacks[j]->funcs.is_interesting(feedbacks[j], stage->engine->executor);
+	}
+
+	return interestingness;
+}
+
+int cont = 0;
+uint64_t cont_tot = 0;
+/* Perform default for fuzzing stage */
+afl_ret_t afl_stage_perform(afl_stage_t *stage, afl_entry_t *queue_entry)
+{
+	/*  This is to stop from compiler complaining about the incompatible pointer
+	// type for the function ptrs. We need a better solution for this to pass the
+	// scheduled_mutator rather than the mutator as an argument. */
+	/* printf("\nafl_stage_perform\n"); */
+
+	afl_input_t *input = queue_entry->input;
+	/* printf("\nafl_stage_perform ---------------  INPUT: %s\n", input->bytes); */
+
+	size_t num = stage->funcs.get_iters(stage);
+	/* printf("\nafl_stage_perform ---------------  NUM. OF INPUT TO GENERATE: %d\n", num); */
+
+	// printf("\nNUMBER OF MUTATORS IN STAGE: %d\n", stage->mutators_count);
+
+	size_t i;
+	for (i = 0; i < num; ++i)
+	{
+		afl_input_t *copy = input->funcs.copy(input);
+
+		if (!copy)
+		{
+			return AFL_RET_ERROR_INPUT_COPY;
+		}
+
+		size_t j;
+		for (j = 0; j < stage->mutators_count; ++j)
+		{
+
+			afl_mutator_t *mutator = stage->mutators[j];
+			/* If the mutator decides not to fuzz this input, don't fuzz it. This is to support the custom mutator API of AFL++ */
+			if (mutator->funcs.custom_queue_get)
+			{
+				mutator->funcs.custom_queue_get(mutator, copy);
+				continue;
+			}
+
+			if (mutator->funcs.trim)
+			{
+				size_t orig_len = copy->len;
+				size_t trim_len = mutator->funcs.trim(mutator, copy);
+
+				if (trim_len > orig_len)
+				{
+					return AFL_RET_TRIM_FAIL;
+				}
+			}
+			// printf("\n cont: %ld",cont_tot++);
+			// printf("\nPRE MUTATE  -  copy: %s  -  len: %d\n", copy->bytes, copy->len);
+			mutator->funcs.mutate(mutator, copy);
+			// printf("\n[LEN=%d]  %s\n", copy->len, copy->bytes);
+		}
+
+		// printf("\ntest\n");
+		/*if (cont_tot > 15)
+		{
+			copy->bytes = "ALWAYSTHESAME";
+			copy->len = 13;
+		}
+		
+		else if (cont_tot == 20)
+		{
+			copy->bytes = "YYXX";
+			copy->len = 4;
+		}
+		else if (cont_tot == 400)
+		{
+			copy->bytes = "ZYXX";
+			copy->len = 4;
+		}
+		else if (cont_tot == 410)
+		{
+			copy->bytes = "ZZXX";
+			copy->len = 4;
+		}
+		else if (cont_tot == 30)
+		{
+			copy->bytes = "GYXX";
+			copy->len = 4;
+		}
+		else
+		{
+			//copy->bytes = "VALORE";
+			//copy->len = 6;
+		}*/
+		cont_tot++;
+		afl_ret_t ret = afl_stage_run(stage, copy, true);
+		/* printf("\nPOST STAGE_RUN\n"); */
+
+		/* Let's collect some feedback on the input now */
+		float interestingness = afl_stage_is_interesting(stage);
+
+		/*interestingness = 0.0;*/
+
+		/*
+		if (interestingness >= 0.5)
+		{
+		*/
+		/* TODO: Use queue abstraction instead */
+		/* TODO: Modificare in modo tale da inviare le informazioni sull'interestingness all'engine senza shared memory */
+		/*
+		llmp_message_t *msg = llmp_client_alloc_next(stage->engine->llmp_client, copy->len + sizeof(afl_entry_info_t));
+	  if (!msg) {
+
+		DBG("Error allocating llmp message");
+		return AFL_RET_ALLOC;
+	  }
+
+	  memcpy(msg->buf, copy->bytes, copy->len);
+		 */
+		/* TODO FIXME - here we fill in the entry info structure on the queue */
+		/* afl_entry_info_t *info_ptr = (afl_entry_info_t*)((u8*)(msg->buf + copy->len));
+		// e.g. fill map hash */
+		/*
+	  msg->tag = LLMP_TAG_NEW_QUEUE_ENTRY_V1;
+	  if (!llmp_client_send(stage->engine->llmp_client, msg)) {
+
+		DBG("An error occurred sending our previously allocated msg");
+		return AFL_RET_UNKNOWN_ERROR;
+
+	  }
+		 */
+		/* we don't add it to the queue but wait for it to come back from the broker for now.
+		TODO: Tidy this up. */
+		/*      interestingness = 0.0f;
+
+		}
+		*/
+
+		/* If the input is interesting and there is a global queue add the input to
+		 * the queue */
+		/* TODO: 0.5 is a random value. How do we want to chose interesting input? */
+		/* This block of code is never reached in the above case where we wait for it to return from the broker*/
+
+		/* TODO: FIXME */
+		/* interestingness = 0.6; */
+		cont++;
+		printf("cont is %d\n", cont);
+
+		if (interestingness >= 0.5 && stage->engine->global_queue && copy->len < 50000 && copy->len > 0) // || cont == 130) // interestingness >0.5?
+		{
+			// printf("input is interesting1!!!!\n");
+			afl_input_t *input_copy = copy->funcs.copy(copy);
+
+			if (!input_copy)
+			{
+				return AFL_RET_ERROR_INPUT_COPY;
+			}
+
+			afl_entry_t *entry = afl_entry_new(input_copy, NULL);
+
+			if (!entry)
+			{
+				return AFL_RET_ALLOC;
+			}
+
+			afl_queue_global_t *queue = stage->engine->global_queue;
+
+			/*if(cont==130){
+				entry->input->bytes="AAAA";
+				entry->input->len=4;
+			queue->base.funcs.insert((afl_queue_t *)queue, entry);
+
+			}else
+			*/
+			queue->base.funcs.insert((afl_queue_t *)queue, entry);
+			
+			// printf("input is interesting2!!!!\n");
+
+			if (VERBOSE_LOG)
+			{
+
+				printf("INTERESTING AND ADDED: \n");
+				for (size_t i = 0; i < entry->input->len; i++)
+				{
+					printf("%02X ", *(entry->input->bytes + i));
+				}
+				// printf("interestingness is %f\n",interestingness);
+				printf("\n");
+
+				// exit(0);
+				printf("copy length is %d\n", copy->len);
+			}
+			// afl_input_delete(copy);  //this makes free():invalidsize crash. commented for the moment
+		}
+		else // non c'era else. afl_input_delete era fuori da solo
+		{
+			afl_input_delete(copy);
+		}
+		afl_queue_global_t *queue = stage->engine->global_queue;
+		if (VERBOSE_LOG > 0)
+			printf("queue length is %d\n", queue->base.funcs.get_size(queue));
+		// printf("input is interesting3!!!!\n");
+
+		switch (ret)
+		{
+		case AFL_RET_SUCCESS:
+			continue;
+		/* We'll add more cases here based on the type of exit_ret value given by
+		// the executor.Those will be handled in the engine itself. */
+		default:
+			return ret;
+		}
+	}
+	// printf("input is interesting4!!!!\n");
+	return AFL_RET_SUCCESS;
+}
+
+/* Functions related to det stage */
+
+afl_ret_t afl_det_stage_perform(afl_stage_t *det_stage, afl_entry_t *entry)
+{
+	// printf("\nafl_det_stage_perform\n");
+
+	if (entry->info->det_done)
+	{
+		return AFL_RET_SUCCESS;
+	} /* Deterministic stage done for this entry */
+	//
+
+	afl_input_t *input = entry->input;
+	// printf("entry input is %d\n", input->len);
+	if (input->len < 0 || input->len > 10000) // MKO
+	{										  // bug workaround
+		printf("\nworkaround111111 len is %d\n", input->len);
+		// probably some instruction doesnt handle writing properly
+		// we could fix bug or take advantage of this to randomize even more. ipotesi abbastanza forte ma dovrebbe andare bene
+
+		/*afl_queue_global_t *global_queue = det_stage->engine->global_queue;
+		afl_entry_t *queue_entry = global_queue->base.funcs.get_queue_entry((afl_queue_t *)global_queue, 0);
+		afl_input_t *seed = queue_entry->input;
+
+		printf("queue input is %s\n", seed->bytes);
+		for(int i =0; i< seed->len; i++){
+			input->bytes[i]=seed->bytes[i];
+		}
+		input->len=seed->len;
+		*/
+		// queue->feedback_queues->// sizeof(input->bytes);
+		afl_queue_global_t *global_queue = det_stage->engine->global_queue;
+		afl_entry_t *seed_entry = global_queue->base.funcs.get_queue_entry((afl_queue_t *)global_queue, 0);
+		afl_input_t *seed = seed_entry->input;
+		for (int i = 0; i < seed->len; i++)
+		{
+			input->bytes[i] = seed->bytes[i];
+		}
+		input->len = seed->len;
+
+		// input->bytes ="tesi@inginf.it"; //prendi un elemento random dalla coda e aggiungilo qui
+		// input->len =strlen(input->bytes);
+		printf("\nworkaround222222 len is %d\n", input->len);
+		// input->len = 1 + afl_rand_below(&det_stage->engine->rand, 128); // randomize on bug. NOT A BUG, NOW A FEATURE!
+		// printf("entry input is %d\n", input->len);
+		/*for(int i =0; i< input->len; i++){
+		   printf("%02x ",input->bytes[i]);
+		   printf("afngaianigna\n\n");
+	   }*/
+		// exit(0);
+		for (int i = 0; i < input->len; i++)
+		{
+			printf("%02x ", input->bytes[i]);
+		}
+	}
+	else
+	{
+		printf("\nno workaround33333 len is %d\n", input->len);
+	}
+	/* Let's make a copy of the input now */
+	size_t i = 0;
+	afl_input_t *copy;
+	// printf("\nafl_det_stage_perform2\n");
+
+	for (i = 0; i < det_stage->mutators_count; ++i)
+	{
+		// printf("\ntest1 %d\n",det_stage->mutators_count);
+		afl_mutator_deterministic_t *mutator_det = (afl_mutator_deterministic_t *)(det_stage->mutators[i]);
+		mutator_det->stage_max = mutator_det->funcs.get_iters(mutator_det, input);
+		// printf("\ntest2 %d\n", mutator_det->stage_max);
+		/*if (mutator_det->stage_max > 5000 || mutator_det->stage_max < 0)
+		 { // bug workaround
+		  // bisognerebbe investigare sulla causa.
+		  // copy->bytes="aaaaaaaaaaaaaaaa";
+		  // copy->len=16;
+		  // printf("fixing..\n");
+		  // afl_ret_t ret = afl_stage_run(det_stage, copy, true);
+		  // return AFL_RET_SUCCESS;
+		  // printf("fixed\n");
+		  // return ret;
+		} */
+
+		for (mutator_det->stage_cur = 0; mutator_det->stage_cur < mutator_det->stage_max; ++mutator_det->stage_cur)
+		{
+			// printf("\ntest3\n");
+			/* Much better to have a post-exec function here to restore the original input? So that we don't always have to copy?  */
+			copy = input->funcs.copy(input);
+			if (copy)
+			{
+				// printf("\ntest4\n");
+				mutator_det->base.funcs.mutate((afl_mutator_t *)mutator_det, copy);
+			}
+			// printf("\ntest5\n");
+		}
+	}
+
+	cont += 1;
+	// printf("\nafl_det_stage_perform3\n");
+	afl_ret_t ret = afl_stage_run(det_stage, copy, true);
+	/* printf("\nPOST STAGE_RUN\n"); */
+	// printf("\nafl_det_stage_perform4\n");
+	/* Let's collect some feedback on the input now */
+	float interestingness = afl_stage_is_interesting(det_stage);
+	// printf("\nafl_det_stage_perform5\n");
+	/* TODO: FIXME */
+	/* interestingness = 0.6; */
+	/* interestingness = 0.0;*/
+
+	// printf("it's interesting %f\n\n",interestingness);
+	// printf("\nafl_det_stage_perform6\n");
+
+	if (interestingness >= 0.5 && det_stage->engine->global_queue && copy->len < 50000 && copy->len > 0)
+	{
+
+		afl_entry_t *entry = afl_entry_new(copy, NULL);
+
+		if (!entry)
+		{
+			return AFL_RET_ALLOC;
+		}
+
+		afl_queue_global_t *queue = det_stage->engine->global_queue;
+
+		queue->base.funcs.insert((afl_queue_t *)queue, entry);
+	}
+
+	afl_input_delete(copy);
+	/* Deterministic stage done for this entry. */
+	DBG("Det stage done for: %s", entry->input->bytes);
+	entry->info->det_done = 1;
+
+	return ret;
+	/* return AFL_RET_SUCCESS; */
+}
+
+afl_ret_t afl_det_stage_init(afl_stage_t *det_stage, afl_engine_t *engine)
+{
+
+	if (afl_stage_init(det_stage, engine) != AFL_RET_SUCCESS)
+	{
+		return AFL_RET_ERROR_INITIALIZE;
+	}
+
+	det_stage->funcs.perform = afl_det_stage_perform;
+	return AFL_RET_SUCCESS;
+}
diff -ruN qemu/linux-user/elfload.c qemu_patched/linux-user/elfload.c
--- qemu/linux-user/elfload.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/linux-user/elfload.c	2023-10-30 19:01:26.105790767 +0100
@@ -32,6 +32,8 @@
 
 #define ELF_OSABI   ELFOSABI_SYSV
 
+extern abi_ulong afl_entry_point, afl_start_code, afl_end_code;
+
 /* from personality.h */
 
 /*
@@ -2882,6 +2884,8 @@
     info->brk = 0;
     info->elf_flags = ehdr->e_flags;
 
+    if (!afl_entry_point) afl_entry_point = info->entry;
+
     prot_exec = PROT_EXEC;
 #ifdef TARGET_AARCH64
     /*
@@ -2960,9 +2964,13 @@
             if (elf_prot & PROT_EXEC) {
                 if (vaddr < info->start_code) {
                     info->start_code = vaddr;
+                    if (!afl_start_code) afl_start_code = vaddr;
+                    printf("[QEMU trace] start_code: %u\n", vaddr);
                 }
                 if (vaddr_ef > info->end_code) {
                     info->end_code = vaddr_ef;
+                    if (!afl_end_code) afl_end_code = vaddr_ef;
+                    printf("[QEMU trace] end code: %u\n", vaddr_ef);
                 }
             }
             if (elf_prot & PROT_WRITE) {
diff -ruN qemu/meson.build qemu_patched/meson.build
--- qemu/meson.build	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/meson.build	2023-10-31 01:20:59.598488351 +0100
@@ -2778,6 +2778,7 @@
     'hw/watchdog',
     'hw/xen',
     'hw/gpio',
+    'libAFL',
     'migration',
     'net',
     'softmmu',
@@ -2909,6 +2910,7 @@
 common_ss.add(pagevary)
 specific_ss.add(files('page-vary.c'))
 
+subdir('libAFL')
 subdir('backends')
 subdir('disas')
 subdir('migration')
diff -ruN qemu/migration/savevm.c qemu_patched/migration/savevm.c
--- qemu/migration/savevm.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/migration/savevm.c	2023-12-30 13:51:34.222955301 +0100
@@ -70,44 +70,46 @@
 const unsigned int postcopy_ram_discard_version;
 
 /* Subcommands for QEMU_VM_COMMAND */
-enum qemu_vm_cmd {
-    MIG_CMD_INVALID = 0,   /* Must be 0 */
-    MIG_CMD_OPEN_RETURN_PATH,  /* Tell the dest to open the Return path */
-    MIG_CMD_PING,              /* Request a PONG on the RP */
-
-    MIG_CMD_POSTCOPY_ADVISE,       /* Prior to any page transfers, just
-                                      warn we might want to do PC */
-    MIG_CMD_POSTCOPY_LISTEN,       /* Start listening for incoming
-                                      pages as it's running. */
-    MIG_CMD_POSTCOPY_RUN,          /* Start execution */
-
-    MIG_CMD_POSTCOPY_RAM_DISCARD,  /* A list of pages to discard that
-                                      were previously sent during
-                                      precopy but are dirty. */
-    MIG_CMD_PACKAGED,          /* Send a wrapped stream within this stream */
-    MIG_CMD_ENABLE_COLO,       /* Enable COLO */
-    MIG_CMD_POSTCOPY_RESUME,   /* resume postcopy on dest */
-    MIG_CMD_RECV_BITMAP,       /* Request for recved bitmap on dst */
+enum qemu_vm_cmd
+{
+    MIG_CMD_INVALID = 0,      /* Must be 0 */
+    MIG_CMD_OPEN_RETURN_PATH, /* Tell the dest to open the Return path */
+    MIG_CMD_PING,             /* Request a PONG on the RP */
+
+    MIG_CMD_POSTCOPY_ADVISE, /* Prior to any page transfers, just
+                                warn we might want to do PC */
+    MIG_CMD_POSTCOPY_LISTEN, /* Start listening for incoming
+                                pages as it's running. */
+    MIG_CMD_POSTCOPY_RUN,    /* Start execution */
+
+    MIG_CMD_POSTCOPY_RAM_DISCARD, /* A list of pages to discard that
+                                     were previously sent during
+                                     precopy but are dirty. */
+    MIG_CMD_PACKAGED,             /* Send a wrapped stream within this stream */
+    MIG_CMD_ENABLE_COLO,          /* Enable COLO */
+    MIG_CMD_POSTCOPY_RESUME,      /* resume postcopy on dest */
+    MIG_CMD_RECV_BITMAP,          /* Request for recved bitmap on dst */
     MIG_CMD_MAX
 };
 
 #define MAX_VM_CMD_PACKAGED_SIZE UINT32_MAX
-static struct mig_cmd_args {
-    ssize_t     len; /* -1 = variable */
+static struct mig_cmd_args
+{
+    ssize_t len; /* -1 = variable */
     const char *name;
 } mig_cmd_args[] = {
-    [MIG_CMD_INVALID]          = { .len = -1, .name = "INVALID" },
-    [MIG_CMD_OPEN_RETURN_PATH] = { .len =  0, .name = "OPEN_RETURN_PATH" },
-    [MIG_CMD_PING]             = { .len = sizeof(uint32_t), .name = "PING" },
-    [MIG_CMD_POSTCOPY_ADVISE]  = { .len = -1, .name = "POSTCOPY_ADVISE" },
-    [MIG_CMD_POSTCOPY_LISTEN]  = { .len =  0, .name = "POSTCOPY_LISTEN" },
-    [MIG_CMD_POSTCOPY_RUN]     = { .len =  0, .name = "POSTCOPY_RUN" },
+    [MIG_CMD_INVALID] = {.len = -1, .name = "INVALID"},
+    [MIG_CMD_OPEN_RETURN_PATH] = {.len = 0, .name = "OPEN_RETURN_PATH"},
+    [MIG_CMD_PING] = {.len = sizeof(uint32_t), .name = "PING"},
+    [MIG_CMD_POSTCOPY_ADVISE] = {.len = -1, .name = "POSTCOPY_ADVISE"},
+    [MIG_CMD_POSTCOPY_LISTEN] = {.len = 0, .name = "POSTCOPY_LISTEN"},
+    [MIG_CMD_POSTCOPY_RUN] = {.len = 0, .name = "POSTCOPY_RUN"},
     [MIG_CMD_POSTCOPY_RAM_DISCARD] = {
-                                   .len = -1, .name = "POSTCOPY_RAM_DISCARD" },
-    [MIG_CMD_POSTCOPY_RESUME]  = { .len =  0, .name = "POSTCOPY_RESUME" },
-    [MIG_CMD_PACKAGED]         = { .len =  4, .name = "PACKAGED" },
-    [MIG_CMD_RECV_BITMAP]      = { .len = -1, .name = "RECV_BITMAP" },
-    [MIG_CMD_MAX]              = { .len = -1, .name = "MAX" },
+        .len = -1, .name = "POSTCOPY_RAM_DISCARD"},
+    [MIG_CMD_POSTCOPY_RESUME] = {.len = 0, .name = "POSTCOPY_RESUME"},
+    [MIG_CMD_PACKAGED] = {.len = 4, .name = "PACKAGED"},
+    [MIG_CMD_RECV_BITMAP] = {.len = -1, .name = "RECV_BITMAP"},
+    [MIG_CMD_MAX] = {.len = -1, .name = "MAX"},
 };
 
 /* Note for MIG_CMD_POSTCOPY_ADVISE:
@@ -138,7 +140,8 @@
 
     qemu_iovec_init_external(&qiov, iov, iovcnt);
     ret = bdrv_writev_vmstate(opaque, &qiov, pos);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         return ret;
     }
 
@@ -158,23 +161,21 @@
 
 static const QEMUFileOps bdrv_read_ops = {
     .get_buffer = block_get_buffer,
-    .close =      bdrv_fclose
-};
+    .close = bdrv_fclose};
 
 static const QEMUFileOps bdrv_write_ops = {
-    .writev_buffer  = block_writev_buffer,
-    .close          = bdrv_fclose
-};
+    .writev_buffer = block_writev_buffer,
+    .close = bdrv_fclose};
 
 static QEMUFile *qemu_fopen_bdrv(BlockDriverState *bs, int is_writable)
 {
-    if (is_writable) {
+    if (is_writable)
+    {
         return qemu_fopen_ops(bs, &bdrv_write_ops, false);
     }
     return qemu_fopen_ops(bs, &bdrv_read_ops, false);
 }
 
-
 /* QEMUFile timer support.
  * Not in qemu-file.c to not add qemu-timer.c as dependency to qemu-file.c
  */
@@ -192,14 +193,16 @@
     uint64_t expire_time;
 
     expire_time = qemu_get_be64(f);
-    if (expire_time != -1) {
+    if (expire_time != -1)
+    {
         timer_mod_ns(ts, expire_time);
-    } else {
+    }
+    else
+    {
         timer_del(ts);
     }
 }
 
-
 /* VMState timer support.
  * Not in vmstate.c to not add qemu-timer.c as dependency to vmstate.c
  */
@@ -223,18 +226,20 @@
 
 const VMStateInfo vmstate_info_timer = {
     .name = "timer",
-    .get  = get_timer,
-    .put  = put_timer,
+    .get = get_timer,
+    .put = put_timer,
 };
 
-
-typedef struct CompatEntry {
+typedef struct CompatEntry
+{
     char idstr[256];
     int instance_id;
 } CompatEntry;
 
-typedef struct SaveStateEntry {
-    QTAILQ_ENTRY(SaveStateEntry) entry;
+typedef struct SaveStateEntry
+{
+    QTAILQ_ENTRY(SaveStateEntry)
+    entry;
     char idstr[256];
     uint32_t instance_id;
     int alias_id;
@@ -251,8 +256,10 @@
     int is_ram;
 } SaveStateEntry;
 
-typedef struct SaveState {
-    QTAILQ_HEAD(, SaveStateEntry) handlers;
+typedef struct SaveState
+{
+    QTAILQ_HEAD(, SaveStateEntry)
+    handlers;
     SaveStateEntry *handler_pri_head[MIG_PRI_MAX + 1];
     int global_section_id;
     uint32_t len;
@@ -265,7 +272,7 @@
 
 static SaveState savevm_state = {
     .handlers = QTAILQ_HEAD_INITIALIZER(savevm_state.handlers),
-    .handler_pri_head = { [MIG_PRI_DEFAULT ... MIG_PRI_MAX] = NULL },
+    .handler_pri_head = {[MIG_PRI_DEFAULT... MIG_PRI_MAX] = NULL},
     .global_section_id = 0,
 };
 
@@ -273,7 +280,8 @@
 {
     assert(capability >= 0 && capability < MIGRATION_CAPABILITY__MAX);
     /* Validate only new capabilities to keep compatibility. */
-    switch (capability) {
+    switch (capability)
+    {
     case MIGRATION_CAPABILITY_X_IGNORE_SHARED:
         return true;
     default:
@@ -286,8 +294,10 @@
     MigrationState *s = migrate_get_current();
     uint32_t result = 0;
     int i;
-    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++) {
-        if (should_validate_capability(i) && s->enabled_capabilities[i]) {
+    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++)
+    {
+        if (should_validate_capability(i) && s->enabled_capabilities[i])
+        {
             result++;
         }
     }
@@ -308,8 +318,10 @@
     state->caps_count = get_validatable_capabilities_count();
     state->capabilities = g_renew(MigrationCapability, state->capabilities,
                                   state->caps_count);
-    for (i = j = 0; i < MIGRATION_CAPABILITY__MAX; i++) {
-        if (should_validate_capability(i) && s->enabled_capabilities[i]) {
+    for (i = j = 0; i < MIGRATION_CAPABILITY__MAX; i++)
+    {
+        if (should_validate_capability(i) && s->enabled_capabilities[i])
+        {
             state->capabilities[j++] = i;
         }
     }
@@ -348,19 +360,23 @@
     int i;
 
     source_caps_bm = bitmap_new(MIGRATION_CAPABILITY__MAX);
-    for (i = 0; i < state->caps_count; i++) {
+    for (i = 0; i < state->caps_count; i++)
+    {
         MigrationCapability capability = state->capabilities[i];
         set_bit(capability, source_caps_bm);
     }
 
-    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++) {
+    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++)
+    {
         bool source_state, target_state;
-        if (!should_validate_capability(i)) {
+        if (!should_validate_capability(i))
+        {
             continue;
         }
         source_state = test_bit(i, source_caps_bm);
         target_state = s->enabled_capabilities[i];
-        if (source_state != target_state) {
+        if (source_state != target_state)
+        {
             error_report("Capability %s is %s, but received capability is %s",
                          MigrationCapability_str(i),
                          target_state ? "on" : "off",
@@ -380,21 +396,24 @@
     const char *current_name = MACHINE_GET_CLASS(current_machine)->name;
     int ret = 0;
 
-    if (strncmp(state->name, current_name, state->len) != 0) {
+    if (strncmp(state->name, current_name, state->len) != 0)
+    {
         error_report("Machine type received is '%.*s' and local is '%s'",
-                     (int) state->len, state->name, current_name);
+                     (int)state->len, state->name, current_name);
         ret = -EINVAL;
         goto out;
     }
 
-    if (state->target_page_bits != qemu_target_page_bits()) {
+    if (state->target_page_bits != qemu_target_page_bits())
+    {
         error_report("Received TARGET_PAGE_BITS is %d but local is %d",
                      state->target_page_bits, qemu_target_page_bits());
         ret = -EINVAL;
         goto out;
     }
 
-    if (!configuration_validate_capabilities(state)) {
+    if (!configuration_validate_capabilities(state))
+    {
         ret = -EINVAL;
         goto out;
     }
@@ -421,8 +440,10 @@
     len = qemu_get_byte(f);
     qemu_get_buffer(f, (uint8_t *)capability_str, len);
     capability_str[len] = '\0';
-    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++) {
-        if (!strcmp(MigrationCapability_str(i), capability_str)) {
+    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++)
+    {
+        if (!strcmp(MigrationCapability_str(i), capability_str))
+        {
             *capability = i;
             return 0;
         }
@@ -446,8 +467,8 @@
 
 static const VMStateInfo vmstate_info_capability = {
     .name = "capability",
-    .get  = get_capability,
-    .put  = put_capability,
+    .get = get_capability,
+    .put = put_capability,
 };
 
 /* The target-page-bits subsection is present only if the
@@ -459,8 +480,7 @@
  */
 static bool vmstate_target_page_bits_needed(void *opaque)
 {
-    return qemu_target_page_bits()
-        > qemu_target_page_bits_min();
+    return qemu_target_page_bits() > qemu_target_page_bits_min();
 }
 
 static const VMStateDescription vmstate_target_page_bits = {
@@ -468,11 +488,9 @@
     .version_id = 1,
     .minimum_version_id = 1,
     .needed = vmstate_target_page_bits_needed,
-    .fields = (VMStateField[]) {
+    .fields = (VMStateField[]){
         VMSTATE_UINT32(target_page_bits, SaveState),
-        VMSTATE_END_OF_LIST()
-    }
-};
+        VMSTATE_END_OF_LIST()}};
 
 static bool vmstate_capabilites_needed(void *opaque)
 {
@@ -484,14 +502,12 @@
     .version_id = 1,
     .minimum_version_id = 1,
     .needed = vmstate_capabilites_needed,
-    .fields = (VMStateField[]) {
+    .fields = (VMStateField[]){
         VMSTATE_UINT32_V(caps_count, SaveState, 1),
         VMSTATE_VARRAY_UINT32_ALLOC(capabilities, SaveState, caps_count, 1,
                                     vmstate_info_capability,
                                     MigrationCapability),
-        VMSTATE_END_OF_LIST()
-    }
-};
+        VMSTATE_END_OF_LIST()}};
 
 static bool vmstate_uuid_needed(void *opaque)
 {
@@ -504,17 +520,19 @@
     char uuid_src[UUID_FMT_LEN + 1];
     char uuid_dst[UUID_FMT_LEN + 1];
 
-    if (!qemu_uuid_set) {
+    if (!qemu_uuid_set)
+    {
         /*
          * It's warning because user might not know UUID in some cases,
          * e.g. load an old snapshot
          */
         qemu_uuid_unparse(&state->uuid, uuid_src);
         warn_report("UUID is received %s, but local uuid isn't set",
-                     uuid_src);
+                    uuid_src);
         return 0;
     }
-    if (!qemu_uuid_is_equal(&state->uuid, &qemu_uuid)) {
+    if (!qemu_uuid_is_equal(&state->uuid, &qemu_uuid))
+    {
         qemu_uuid_unparse(&state->uuid, uuid_src);
         qemu_uuid_unparse(&qemu_uuid, uuid_dst);
         error_report("UUID received is %s and local is %s", uuid_src, uuid_dst);
@@ -529,11 +547,9 @@
     .minimum_version_id = 1,
     .needed = vmstate_uuid_needed,
     .post_load = vmstate_uuid_post_load,
-    .fields = (VMStateField[]) {
+    .fields = (VMStateField[]){
         VMSTATE_UINT8_ARRAY_V(uuid.data, SaveState, sizeof(QemuUUID), 1),
-        VMSTATE_END_OF_LIST()
-    }
-};
+        VMSTATE_END_OF_LIST()}};
 
 static const VMStateDescription vmstate_configuration = {
     .name = "configuration",
@@ -542,18 +558,11 @@
     .post_load = configuration_post_load,
     .pre_save = configuration_pre_save,
     .post_save = configuration_post_save,
-    .fields = (VMStateField[]) {
+    .fields = (VMStateField[]){
         VMSTATE_UINT32(len, SaveState),
         VMSTATE_VBUFFER_ALLOC_UINT32(name, SaveState, 0, NULL, len),
-        VMSTATE_END_OF_LIST()
-    },
-    .subsections = (const VMStateDescription *[]) {
-        &vmstate_target_page_bits,
-        &vmstate_capabilites,
-        &vmstate_uuid,
-        NULL
-    }
-};
+        VMSTATE_END_OF_LIST()},
+    .subsections = (const VMStateDescription *[]){&vmstate_target_page_bits, &vmstate_capabilites, &vmstate_uuid, NULL}};
 
 static void dump_vmstate_vmsd(FILE *out_file,
                               const VMStateDescription *vmsd, int indent,
@@ -570,7 +579,8 @@
     fprintf(out_file, "%*s\"field_exists\": %s,\n", indent, "",
             field->field_exists ? "true" : "false");
     fprintf(out_file, "%*s\"size\": %zu", indent, "", field->size);
-    if (field->vmsd != NULL) {
+    if (field->vmsd != NULL)
+    {
         fprintf(out_file, ",\n");
         dump_vmstate_vmsd(out_file, field->vmsd, indent, false);
     }
@@ -581,7 +591,8 @@
                               const VMStateDescription **subsection,
                               int indent)
 {
-    if (*subsection != NULL) {
+    if (*subsection != NULL)
+    {
         dump_vmstate_vmsd(out_file, *subsection, indent, true);
     }
 }
@@ -590,9 +601,12 @@
                               const VMStateDescription *vmsd, int indent,
                               bool is_subsection)
 {
-    if (is_subsection) {
+    if (is_subsection)
+    {
         fprintf(out_file, "%*s{\n", indent, "");
-    } else {
+    }
+    else
+    {
         fprintf(out_file, "%*s\"%s\": {\n", indent, "", "Description");
     }
     indent += 2;
@@ -601,19 +615,23 @@
             vmsd->version_id);
     fprintf(out_file, "%*s\"minimum_version_id\": %d", indent, "",
             vmsd->minimum_version_id);
-    if (vmsd->fields != NULL) {
+    if (vmsd->fields != NULL)
+    {
         const VMStateField *field = vmsd->fields;
         bool first;
 
         fprintf(out_file, ",\n%*s\"Fields\": [\n", indent, "");
         first = true;
-        while (field->name != NULL) {
-            if (field->flags & VMS_MUST_EXIST) {
+        while (field->name != NULL)
+        {
+            if (field->flags & VMS_MUST_EXIST)
+            {
                 /* Ignore VMSTATE_VALIDATE bits; these don't get migrated */
                 field++;
                 continue;
             }
-            if (!first) {
+            if (!first)
+            {
                 fprintf(out_file, ",\n");
             }
             dump_vmstate_vmsf(out_file, field, indent + 2);
@@ -622,14 +640,17 @@
         }
         fprintf(out_file, "\n%*s]", indent, "");
     }
-    if (vmsd->subsections != NULL) {
+    if (vmsd->subsections != NULL)
+    {
         const VMStateDescription **subsection = vmsd->subsections;
         bool first;
 
         fprintf(out_file, ",\n%*s\"Subsections\": [\n", indent, "");
         first = true;
-        while (*subsection != NULL) {
-            if (!first) {
+        while (*subsection != NULL)
+        {
+            if (!first)
+            {
                 fprintf(out_file, ",\n");
             }
             dump_vmstate_vmss(out_file, subsection, indent + 2);
@@ -662,17 +683,20 @@
 
     first = true;
     list = object_class_get_list(TYPE_DEVICE, true);
-    for (elt = list; elt; elt = elt->next) {
+    for (elt = list; elt; elt = elt->next)
+    {
         DeviceClass *dc = OBJECT_CLASS_CHECK(DeviceClass, elt->data,
                                              TYPE_DEVICE);
         const char *name;
         int indent = 2;
 
-        if (!dc->vmsd) {
+        if (!dc->vmsd)
+        {
             continue;
         }
 
-        if (!first) {
+        if (!first)
+        {
             fprintf(out_file, ",\n");
         }
         name = object_class_get_name(OBJECT_CLASS(dc));
@@ -699,9 +723,10 @@
     SaveStateEntry *se;
     uint32_t instance_id = 0;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (strcmp(idstr, se->idstr) == 0
-            && instance_id <= se->instance_id) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (strcmp(idstr, se->idstr) == 0 && instance_id <= se->instance_id)
+        {
             instance_id = se->instance_id + 1;
         }
     }
@@ -715,13 +740,15 @@
     SaveStateEntry *se;
     int instance_id = 0;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->compat) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->compat)
+        {
             continue;
         }
 
-        if (strcmp(idstr, se->compat->idstr) == 0
-            && instance_id <= se->compat->instance_id) {
+        if (strcmp(idstr, se->compat->idstr) == 0 && instance_id <= se->compat->instance_id)
+        {
             instance_id = se->compat->instance_id + 1;
         }
     }
@@ -730,7 +757,8 @@
 
 static inline MigrationPriority save_state_priority(SaveStateEntry *se)
 {
-    if (se->vmsd) {
+    if (se->vmsd)
+    {
         return se->vmsd->priority;
     }
     return MIG_PRI_DEFAULT;
@@ -744,21 +772,27 @@
 
     assert(priority <= MIG_PRI_MAX);
 
-    for (i = priority - 1; i >= 0; i--) {
+    for (i = priority - 1; i >= 0; i--)
+    {
         se = savevm_state.handler_pri_head[i];
-        if (se != NULL) {
+        if (se != NULL)
+        {
             assert(save_state_priority(se) < priority);
             break;
         }
     }
 
-    if (i >= 0) {
+    if (i >= 0)
+    {
         QTAILQ_INSERT_BEFORE(se, nse, entry);
-    } else {
+    }
+    else
+    {
         QTAILQ_INSERT_TAIL(&savevm_state.handlers, nse, entry);
     }
 
-    if (savevm_state.handler_pri_head[priority] == NULL) {
+    if (savevm_state.handler_pri_head[priority] == NULL)
+    {
         savevm_state.handler_pri_head[priority] = nse;
     }
 }
@@ -768,11 +802,15 @@
     SaveStateEntry *next;
     MigrationPriority priority = save_state_priority(se);
 
-    if (se == savevm_state.handler_pri_head[priority]) {
+    if (se == savevm_state.handler_pri_head[priority])
+    {
         next = QTAILQ_NEXT(se, entry);
-        if (next != NULL && save_state_priority(next) == priority) {
+        if (next != NULL && save_state_priority(next) == priority)
+        {
             savevm_state.handler_pri_head[priority] = next;
-        } else {
+        }
+        else
+        {
             savevm_state.handler_pri_head[priority] = NULL;
         }
     }
@@ -798,15 +836,19 @@
     se->opaque = opaque;
     se->vmsd = NULL;
     /* if this is a live_savem then set is_ram */
-    if (ops->save_setup != NULL) {
+    if (ops->save_setup != NULL)
+    {
         se->is_ram = 1;
     }
 
     pstrcat(se->idstr, sizeof(se->idstr), idstr);
 
-    if (instance_id == VMSTATE_INSTANCE_ID_ANY) {
+    if (instance_id == VMSTATE_INSTANCE_ID_ANY)
+    {
         se->instance_id = calculate_new_instance_id(se->idstr);
-    } else {
+    }
+    else
+    {
         se->instance_id = instance_id;
     }
     assert(!se->compat || se->instance_id == 0);
@@ -819,9 +861,11 @@
     SaveStateEntry *se, *new_se;
     char id[256] = "";
 
-    if (obj) {
+    if (obj)
+    {
         char *oid = vmstate_if_get_id(obj);
-        if (oid) {
+        if (oid)
+        {
             pstrcpy(id, sizeof(id), oid);
             pstrcat(id, sizeof(id), "/");
             g_free(oid);
@@ -829,8 +873,10 @@
     }
     pstrcat(id, sizeof(id), idstr);
 
-    QTAILQ_FOREACH_SAFE(se, &savevm_state.handlers, entry, new_se) {
-        if (strcmp(se->idstr, id) == 0 && se->opaque == opaque) {
+    QTAILQ_FOREACH_SAFE(se, &savevm_state.handlers, entry, new_se)
+    {
+        if (strcmp(se->idstr, id) == 0 && se->opaque == opaque)
+        {
             savevm_state_handler_remove(se);
             g_free(se->compat);
             g_free(se);
@@ -856,11 +902,14 @@
     se->vmsd = vmsd;
     se->alias_id = alias_id;
 
-    if (obj) {
+    if (obj)
+    {
         char *id = vmstate_if_get_id(obj);
-        if (id) {
+        if (id)
+        {
             if (snprintf(se->idstr, sizeof(se->idstr), "%s/", id) >=
-                sizeof(se->idstr)) {
+                sizeof(se->idstr))
+            {
                 error_setg(errp, "Path too long for VMState (%s)", id);
                 g_free(id);
                 g_free(se);
@@ -871,16 +920,18 @@
 
             se->compat = g_new0(CompatEntry, 1);
             pstrcpy(se->compat->idstr, sizeof(se->compat->idstr), vmsd->name);
-            se->compat->instance_id = instance_id == VMSTATE_INSTANCE_ID_ANY ?
-                         calculate_compat_instance_id(vmsd->name) : instance_id;
+            se->compat->instance_id = instance_id == VMSTATE_INSTANCE_ID_ANY ? calculate_compat_instance_id(vmsd->name) : instance_id;
             instance_id = VMSTATE_INSTANCE_ID_ANY;
         }
     }
     pstrcat(se->idstr, sizeof(se->idstr), vmsd->name);
 
-    if (instance_id == VMSTATE_INSTANCE_ID_ANY) {
+    if (instance_id == VMSTATE_INSTANCE_ID_ANY)
+    {
         se->instance_id = calculate_new_instance_id(se->idstr);
-    } else {
+    }
+    else
+    {
         se->instance_id = instance_id;
     }
     assert(!se->compat || se->instance_id == 0);
@@ -893,8 +944,10 @@
 {
     SaveStateEntry *se, *new_se;
 
-    QTAILQ_FOREACH_SAFE(se, &savevm_state.handlers, entry, new_se) {
-        if (se->vmsd == vmsd && se->opaque == opaque) {
+    QTAILQ_FOREACH_SAFE(se, &savevm_state.handlers, entry, new_se)
+    {
+        if (se->vmsd == vmsd && se->opaque == opaque)
+        {
             savevm_state_handler_remove(se);
             g_free(se->compat);
             g_free(se);
@@ -905,7 +958,8 @@
 static int vmstate_load(QEMUFile *f, SaveStateEntry *se)
 {
     trace_vmstate_load(se->idstr, se->vmsd ? se->vmsd->name : "(old)");
-    if (!se->vmsd) {         /* Old style */
+    if (!se->vmsd)
+    { /* Old style */
         return se->ops->load_state(f, se->opaque, se->load_version_id);
     }
     return vmstate_load_state(f, se->vmsd, se->opaque, se->load_version_id);
@@ -920,7 +974,8 @@
     se->ops->save_state(f, se->opaque);
     size = qemu_ftell_fast(f) - old_offset;
 
-    if (vmdesc) {
+    if (vmdesc)
+    {
         json_writer_int64(vmdesc, "size", size);
         json_writer_start_array(vmdesc, "fields");
         json_writer_start_object(vmdesc, NULL);
@@ -936,7 +991,8 @@
                         JSONWriter *vmdesc)
 {
     trace_vmstate_save(se->idstr, se->vmsd ? se->vmsd->name : "(old)");
-    if (!se->vmsd) {
+    if (!se->vmsd)
+    {
         vmstate_save_old_style(f, se, vmdesc);
         return 0;
     }
@@ -953,7 +1009,8 @@
     qemu_put_be32(f, se->section_id);
 
     if (section_type == QEMU_VM_SECTION_FULL ||
-        section_type == QEMU_VM_SECTION_START) {
+        section_type == QEMU_VM_SECTION_START)
+    {
         /* ID string */
         size_t len = strlen(se->idstr);
         qemu_put_byte(f, len);
@@ -970,7 +1027,8 @@
  */
 static void save_section_footer(QEMUFile *f, SaveStateEntry *se)
 {
-    if (migrate_get_current()->send_section_footer) {
+    if (migrate_get_current()->send_section_footer)
+    {
         qemu_put_byte(f, QEMU_VM_SECTION_FOOTER);
         qemu_put_be32(f, se->section_id);
     }
@@ -1032,7 +1090,8 @@
 {
     uint32_t tmp;
 
-    if (len > MAX_VM_CMD_PACKAGED_SIZE) {
+    if (len > MAX_VM_CMD_PACKAGED_SIZE)
+    {
         error_report("%s: Unreasonably large packaged state: %zu",
                      __func__, len);
         return -1;
@@ -1051,7 +1110,8 @@
 /* Send prior to any postcopy transfer */
 void qemu_savevm_send_postcopy_advise(QEMUFile *f)
 {
-    if (migrate_postcopy_ram()) {
+    if (migrate_postcopy_ram())
+    {
         uint64_t tmp[2];
         tmp[0] = cpu_to_be64(ram_pagesize_summary());
         tmp[1] = cpu_to_be64(qemu_target_page_size());
@@ -1059,7 +1119,9 @@
         trace_qemu_savevm_send_postcopy_advise();
         qemu_savevm_command_send(f, MIG_CMD_POSTCOPY_ADVISE,
                                  16, (uint8_t *)tmp);
-    } else {
+    }
+    else
+    {
         qemu_savevm_command_send(f, MIG_CMD_POSTCOPY_ADVISE, 0, NULL);
     }
 }
@@ -1100,7 +1162,8 @@
     tmplen = 2 + name_len;
     buf[tmplen++] = '\0';
 
-    for (t = 0; t < len; t++) {
+    for (t = 0; t < len; t++)
+    {
         stq_be_p(buf + tmplen, start_list[t]);
         tmplen += 8;
         stq_be_p(buf + tmplen, length_list[t]);
@@ -1147,8 +1210,10 @@
 {
     SaveStateEntry *se;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (se->vmsd && se->vmsd->unmigratable) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (se->vmsd && se->vmsd->unmigratable)
+        {
             error_setg(errp, "State blocked by non-migratable device '%s'",
                        se->idstr);
             return true;
@@ -1161,8 +1226,10 @@
 {
     SaveStateEntry *se;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (se->vmsd && se->vmsd->unmigratable) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (se->vmsd && se->vmsd->unmigratable)
+        {
             QAPI_LIST_PREPEND(*reasons,
                               g_strdup_printf("non-migratable device: %s",
                                               se->idstr));
@@ -1176,7 +1243,8 @@
     qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
     qemu_put_be32(f, QEMU_VM_FILE_VERSION);
 
-    if (migrate_get_current()->send_configuration) {
+    if (migrate_get_current()->send_configuration)
+    {
         qemu_put_byte(f, QEMU_VM_CONFIGURATION);
         vmstate_save_state(f, &vmstate_configuration, &savevm_state, 0);
     }
@@ -1186,9 +1254,11 @@
 {
     SaveStateEntry *se;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
         if (se->vmsd && se->vmsd->dev_unplug_pending &&
-            se->vmsd->dev_unplug_pending(se->opaque)) {
+            se->vmsd->dev_unplug_pending(se->opaque))
+        {
             return true;
         }
     }
@@ -1203,12 +1273,16 @@
     int ret;
 
     trace_savevm_state_setup();
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->ops || !se->ops->save_setup) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->ops || !se->ops->save_setup)
+        {
             continue;
         }
-        if (se->ops->is_active) {
-            if (!se->ops->is_active(se->opaque)) {
+        if (se->ops->is_active)
+        {
+            if (!se->ops->is_active(se->opaque))
+            {
                 continue;
             }
         }
@@ -1216,13 +1290,15 @@
 
         ret = se->ops->save_setup(f, se->opaque);
         save_section_footer(f, se);
-        if (ret < 0) {
+        if (ret < 0)
+        {
             qemu_file_set_error(f, ret);
             break;
         }
     }
 
-    if (precopy_notify(PRECOPY_NOTIFY_SETUP, &local_err)) {
+    if (precopy_notify(PRECOPY_NOTIFY_SETUP, &local_err))
+    {
         error_report_err(local_err);
     }
 }
@@ -1234,17 +1310,22 @@
 
     trace_savevm_state_resume_prepare();
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->ops || !se->ops->resume_prepare) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->ops || !se->ops->resume_prepare)
+        {
             continue;
         }
-        if (se->ops->is_active) {
-            if (!se->ops->is_active(se->opaque)) {
+        if (se->ops->is_active)
+        {
+            if (!se->ops->is_active(se->opaque))
+            {
                 continue;
             }
         }
         ret = se->ops->resume_prepare(s, se->opaque);
-        if (ret < 0) {
+        if (ret < 0)
+        {
             return ret;
         }
     }
@@ -1264,16 +1345,20 @@
     int ret = 1;
 
     trace_savevm_state_iterate();
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->ops || !se->ops->save_live_iterate) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->ops || !se->ops->save_live_iterate)
+        {
             continue;
         }
         if (se->ops->is_active &&
-            !se->ops->is_active(se->opaque)) {
+            !se->ops->is_active(se->opaque))
+        {
             continue;
         }
         if (se->ops->is_active_iterate &&
-            !se->ops->is_active_iterate(se->opaque)) {
+            !se->ops->is_active_iterate(se->opaque))
+        {
             continue;
         }
         /*
@@ -1283,10 +1368,12 @@
          * iterate afterwards.
          */
         if (postcopy &&
-            !(se->ops->has_postcopy && se->ops->has_postcopy(se->opaque))) {
+            !(se->ops->has_postcopy && se->ops->has_postcopy(se->opaque)))
+        {
             continue;
         }
-        if (qemu_file_rate_limit(f)) {
+        if (qemu_file_rate_limit(f))
+        {
             return 0;
         }
         trace_savevm_section_start(se->idstr, se->section_id);
@@ -1297,13 +1384,15 @@
         trace_savevm_section_end(se->idstr, se->section_id, ret);
         save_section_footer(f, se);
 
-        if (ret < 0) {
+        if (ret < 0)
+        {
             error_report("failed to save SaveStateEntry with id(name): "
                          "%d(%s): %d",
                          se->section_id, se->idstr, ret);
             qemu_file_set_error(f, ret);
         }
-        if (ret <= 0) {
+        if (ret <= 0)
+        {
             /* Do not proceed to the next vmstate before this one reported
                completion of the current stage. This serializes the migration
                and reduces the probability that a faster changing state is
@@ -1333,12 +1422,16 @@
     SaveStateEntry *se;
     int ret;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->ops || !se->ops->save_live_complete_postcopy) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->ops || !se->ops->save_live_complete_postcopy)
+        {
             continue;
         }
-        if (se->ops->is_active) {
-            if (!se->ops->is_active(se->opaque)) {
+        if (se->ops->is_active)
+        {
+            if (!se->ops->is_active(se->opaque))
+            {
                 continue;
             }
         }
@@ -1350,7 +1443,8 @@
         ret = se->ops->save_live_complete_postcopy(f, se->opaque);
         trace_savevm_section_end(se->idstr, se->section_id, ret);
         save_section_footer(f, se);
-        if (ret < 0) {
+        if (ret < 0)
+        {
             qemu_file_set_error(f, ret);
             return;
         }
@@ -1360,22 +1454,25 @@
     qemu_fflush(f);
 }
 
-static
-int qemu_savevm_state_complete_precopy_iterable(QEMUFile *f, bool in_postcopy)
+static int qemu_savevm_state_complete_precopy_iterable(QEMUFile *f, bool in_postcopy)
 {
     SaveStateEntry *se;
     int ret;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
         if (!se->ops ||
             (in_postcopy && se->ops->has_postcopy &&
              se->ops->has_postcopy(se->opaque)) ||
-            !se->ops->save_live_complete_precopy) {
+            !se->ops->save_live_complete_precopy)
+        {
             continue;
         }
 
-        if (se->ops->is_active) {
-            if (!se->ops->is_active(se->opaque)) {
+        if (se->ops->is_active)
+        {
+            if (!se->ops->is_active(se->opaque))
+            {
                 continue;
             }
         }
@@ -1386,7 +1483,8 @@
         ret = se->ops->save_live_complete_precopy(f, se->opaque);
         trace_savevm_section_end(se->idstr, se->section_id, ret);
         save_section_footer(f, se);
-        if (ret < 0) {
+        if (ret < 0)
+        {
             qemu_file_set_error(f, ret);
             return -1;
         }
@@ -1408,12 +1506,15 @@
     json_writer_start_object(vmdesc, NULL);
     json_writer_int64(vmdesc, "page_size", qemu_target_page_size());
     json_writer_start_array(vmdesc, "devices");
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
 
-        if ((!se->ops || !se->ops->save_state) && !se->vmsd) {
+        if ((!se->ops || !se->ops->save_state) && !se->vmsd)
+        {
             continue;
         }
-        if (se->vmsd && !vmstate_save_needed(se->vmsd, se->opaque)) {
+        if (se->vmsd && !vmstate_save_needed(se->vmsd, se->opaque))
+        {
             trace_savevm_section_skip(se->idstr, se->section_id);
             continue;
         }
@@ -1426,7 +1527,8 @@
 
         save_section_header(f, se, QEMU_VM_SECTION_FULL);
         ret = vmstate_save(f, se, vmdesc);
-        if (ret) {
+        if (ret)
+        {
             qemu_file_set_error(f, ret);
             return ret;
         }
@@ -1436,18 +1538,21 @@
         json_writer_end_object(vmdesc);
     }
 
-    if (inactivate_disks) {
+    if (inactivate_disks)
+    {
         /* Inactivate before sending QEMU_VM_EOF so that the
          * bdrv_activate_all() on the other end won't fail. */
         ret = bdrv_inactivate_all();
-        if (ret) {
+        if (ret)
+        {
             error_report("%s: bdrv_inactivate_all() failed (%d)",
                          __func__, ret);
             qemu_file_set_error(f, ret);
             return ret;
         }
     }
-    if (!in_postcopy) {
+    if (!in_postcopy)
+    {
         /* Postcopy stream will still be going */
         qemu_put_byte(f, QEMU_VM_EOF);
     }
@@ -1456,7 +1561,8 @@
     json_writer_end_object(vmdesc);
     vmdesc_len = strlen(json_writer_get(vmdesc));
 
-    if (should_send_vmdesc()) {
+    if (should_send_vmdesc())
+    {
         qemu_put_byte(f, QEMU_VM_VMDESCRIPTION);
         qemu_put_be32(f, vmdesc_len);
         qemu_put_buffer(f, (uint8_t *)json_writer_get(vmdesc), vmdesc_len);
@@ -1472,7 +1578,8 @@
     Error *local_err = NULL;
     bool in_postcopy = migration_in_postcopy();
 
-    if (precopy_notify(PRECOPY_NOTIFY_COMPLETE, &local_err)) {
+    if (precopy_notify(PRECOPY_NOTIFY_COMPLETE, &local_err))
+    {
         error_report_err(local_err);
     }
 
@@ -1480,20 +1587,24 @@
 
     cpu_synchronize_all_states();
 
-    if (!in_postcopy || iterable_only) {
+    if (!in_postcopy || iterable_only)
+    {
         ret = qemu_savevm_state_complete_precopy_iterable(f, in_postcopy);
-        if (ret) {
+        if (ret)
+        {
             return ret;
         }
     }
 
-    if (iterable_only) {
+    if (iterable_only)
+    {
         goto flush;
     }
 
     ret = qemu_savevm_state_complete_precopy_non_iterable(f, in_postcopy,
                                                           inactivate_disks);
-    if (ret) {
+    if (ret)
+    {
         return ret;
     }
 
@@ -1517,13 +1628,16 @@
     *res_compatible = 0;
     *res_postcopy_only = 0;
 
-
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->ops || !se->ops->save_live_pending) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->ops || !se->ops->save_live_pending)
+        {
             continue;
         }
-        if (se->ops->is_active) {
-            if (!se->ops->is_active(se->opaque)) {
+        if (se->ops->is_active)
+        {
+            if (!se->ops->is_active(se->opaque))
+            {
                 continue;
             }
         }
@@ -1538,13 +1652,16 @@
     SaveStateEntry *se;
     Error *local_err = NULL;
 
-    if (precopy_notify(PRECOPY_NOTIFY_CLEANUP, &local_err)) {
+    if (precopy_notify(PRECOPY_NOTIFY_CLEANUP, &local_err))
+    {
         error_report_err(local_err);
     }
 
     trace_savevm_state_cleanup();
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (se->ops && se->ops->save_cleanup) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (se->ops && se->ops->save_cleanup)
+        {
             se->ops->save_cleanup(se->opaque);
         }
     }
@@ -1556,12 +1673,14 @@
     MigrationState *ms = migrate_get_current();
     MigrationStatus status;
 
-    if (migration_is_running(ms->state)) {
+    if (migration_is_running(ms->state))
+    {
         error_setg(errp, QERR_MIGRATION_ACTIVE);
         return -EINVAL;
     }
 
-    if (migrate_use_block()) {
+    if (migrate_use_block())
+    {
         error_setg(errp, "Block migration and snapshots are incompatible");
         return -EINVAL;
     }
@@ -1576,25 +1695,32 @@
     qemu_savevm_state_setup(f);
     qemu_mutex_lock_iothread();
 
-    while (qemu_file_get_error(f) == 0) {
-        if (qemu_savevm_state_iterate(f, false) > 0) {
+    while (qemu_file_get_error(f) == 0)
+    {
+        if (qemu_savevm_state_iterate(f, false) > 0)
+        {
             break;
         }
     }
 
     ret = qemu_file_get_error(f);
-    if (ret == 0) {
+    if (ret == 0)
+    {
         qemu_savevm_state_complete_precopy(f, false, false);
         ret = qemu_file_get_error(f);
     }
     qemu_savevm_state_cleanup();
-    if (ret != 0) {
+    if (ret != 0)
+    {
         error_setg_errno(errp, -ret, "Error while writing VM state");
     }
 
-    if (ret != 0) {
+    if (ret != 0)
+    {
         status = MIGRATION_STATUS_FAILED;
-    } else {
+    }
+    else
+    {
         status = MIGRATION_STATUS_COMPLETED;
     }
     migrate_set_state(&ms->state, MIGRATION_STATUS_SETUP, status);
@@ -1617,29 +1743,35 @@
 {
     SaveStateEntry *se;
 
-    if (!migration_in_colo_state()) {
+    if (!migration_in_colo_state())
+    {
         qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
         qemu_put_be32(f, QEMU_VM_FILE_VERSION);
     }
     cpu_synchronize_all_states();
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
         int ret;
 
-        if (se->is_ram) {
+        if (se->is_ram)
+        {
             continue;
         }
-        if ((!se->ops || !se->ops->save_state) && !se->vmsd) {
+        if ((!se->ops || !se->ops->save_state) && !se->vmsd)
+        {
             continue;
         }
-        if (se->vmsd && !vmstate_save_needed(se->vmsd, se->opaque)) {
+        if (se->vmsd && !vmstate_save_needed(se->vmsd, se->opaque))
+        {
             continue;
         }
 
         save_section_header(f, se, QEMU_VM_SECTION_FULL);
 
         ret = vmstate_save(f, se, NULL);
-        if (ret) {
+        if (ret)
+        {
             return ret;
         }
 
@@ -1655,13 +1787,15 @@
 {
     SaveStateEntry *se;
 
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
         if (!strcmp(se->idstr, idstr) &&
             (instance_id == se->instance_id ||
              instance_id == se->alias_id))
             return se;
         /* Migrating from an older version? */
-        if (strstr(se->idstr, idstr) && se->compat) {
+        if (strstr(se->idstr, idstr) && se->compat)
+        {
             if (!strcmp(se->compat->idstr, idstr) &&
                 (instance_id == se->compat->instance_id ||
                  instance_id == se->alias_id))
@@ -1671,9 +1805,10 @@
     return NULL;
 }
 
-enum LoadVMExitCodes {
+enum LoadVMExitCodes
+{
     /* Allow a command to quit all layers of nested loadvm loops */
-    LOADVM_QUIT     =  1,
+    LOADVM_QUIT = 1,
 };
 
 /* ------ incoming postcopy messages ------ */
@@ -1690,20 +1825,24 @@
     Error *local_err = NULL;
 
     trace_loadvm_postcopy_handle_advise();
-    if (ps != POSTCOPY_INCOMING_NONE) {
+    if (ps != POSTCOPY_INCOMING_NONE)
+    {
         error_report("CMD_POSTCOPY_ADVISE in wrong postcopy state (%d)", ps);
         return -1;
     }
 
-    switch (len) {
+    switch (len)
+    {
     case 0:
-        if (migrate_postcopy_ram()) {
+        if (migrate_postcopy_ram())
+        {
             error_report("RAM postcopy is enabled but have 0 byte advise");
             return -EINVAL;
         }
         return 0;
     case 8 + 8:
-        if (!migrate_postcopy_ram()) {
+        if (!migrate_postcopy_ram())
+        {
             error_report("RAM postcopy is disabled but have 16 byte advise");
             return -EINVAL;
         }
@@ -1713,7 +1852,8 @@
         return -EINVAL;
     }
 
-    if (!postcopy_ram_supported_by_host(mis)) {
+    if (!postcopy_ram_supported_by_host(mis))
+    {
         postcopy_state_set(POSTCOPY_INCOMING_NONE);
         return -1;
     }
@@ -1721,7 +1861,8 @@
     remote_pagesize_summary = qemu_get_be64(mis->from_src_file);
     local_pagesize_summary = ram_pagesize_summary();
 
-    if (remote_pagesize_summary != local_pagesize_summary)  {
+    if (remote_pagesize_summary != local_pagesize_summary)
+    {
         /*
          * This detects two potential causes of mismatch:
          *   a) A mismatch in host page sizes
@@ -1737,13 +1878,14 @@
          *      do huge pages.
          */
         error_report("Postcopy needs matching RAM page sizes (s=%" PRIx64
-                                                             " d=%" PRIx64 ")",
+                     " d=%" PRIx64 ")",
                      remote_pagesize_summary, local_pagesize_summary);
         return -1;
     }
 
     remote_tps = qemu_get_be64(mis->from_src_file);
-    if (remote_tps != page_size) {
+    if (remote_tps != page_size)
+    {
         /*
          * Again, some differences could be dealt with, but for now keep it
          * simple.
@@ -1753,12 +1895,14 @@
         return -1;
     }
 
-    if (postcopy_notify(POSTCOPY_NOTIFY_INBOUND_ADVISE, &local_err)) {
+    if (postcopy_notify(POSTCOPY_NOTIFY_INBOUND_ADVISE, &local_err))
+    {
         error_report_err(local_err);
         return -1;
     }
 
-    if (ram_postcopy_incoming_init(mis)) {
+    if (ram_postcopy_incoming_init(mis))
+    {
         return -1;
     }
 
@@ -1779,11 +1923,13 @@
 
     trace_loadvm_postcopy_ram_handle_discard();
 
-    switch (ps) {
+    switch (ps)
+    {
     case POSTCOPY_INCOMING_ADVISE:
         /* 1st discard */
         tmp = postcopy_ram_prepare_discard(mis);
-        if (tmp) {
+        if (tmp)
+        {
             return tmp;
         }
         break;
@@ -1801,42 +1947,49 @@
      *    Version (0)
      *    a RAM ID string (length byte, name, 0 term)
      *    then at least 1 16 byte chunk
-    */
-    if (len < (1 + 1 + 1 + 1 + 2 * 8)) {
+     */
+    if (len < (1 + 1 + 1 + 1 + 2 * 8))
+    {
         error_report("CMD_POSTCOPY_RAM_DISCARD invalid length (%d)", len);
         return -1;
     }
 
     tmp = qemu_get_byte(mis->from_src_file);
-    if (tmp != postcopy_ram_discard_version) {
+    if (tmp != postcopy_ram_discard_version)
+    {
         error_report("CMD_POSTCOPY_RAM_DISCARD invalid version (%d)", tmp);
         return -1;
     }
 
-    if (!qemu_get_counted_string(mis->from_src_file, ramid)) {
+    if (!qemu_get_counted_string(mis->from_src_file, ramid))
+    {
         error_report("CMD_POSTCOPY_RAM_DISCARD Failed to read RAMBlock ID");
         return -1;
     }
     tmp = qemu_get_byte(mis->from_src_file);
-    if (tmp != 0) {
+    if (tmp != 0)
+    {
         error_report("CMD_POSTCOPY_RAM_DISCARD missing nil (%d)", tmp);
         return -1;
     }
 
     len -= 3 + strlen(ramid);
-    if (len % 16) {
+    if (len % 16)
+    {
         error_report("CMD_POSTCOPY_RAM_DISCARD invalid length (%d)", len);
         return -1;
     }
     trace_loadvm_postcopy_ram_handle_discard_header(ramid, len);
-    while (len) {
+    while (len)
+    {
         uint64_t start_addr, block_length;
         start_addr = qemu_get_be64(mis->from_src_file);
         block_length = qemu_get_be64(mis->from_src_file);
 
         len -= 16;
         int ret = ram_discard_range(ramid, start_addr, block_length);
-        if (ret) {
+        if (ret)
+        {
             return ret;
         }
     }
@@ -1862,7 +2015,7 @@
     object_ref(OBJECT(migr));
 
     migrate_set_state(&mis->state, MIGRATION_STATUS_ACTIVE,
-                                   MIGRATION_STATUS_POSTCOPY_ACTIVE);
+                      MIGRATION_STATUS_POSTCOPY_ACTIVE);
     qemu_sem_post(&mis->thread_sync_sem);
     trace_postcopy_ram_listen_thread_start();
 
@@ -1885,7 +2038,8 @@
     qemu_file_set_blocking(f, false);
 
     trace_postcopy_ram_listen_thread_exit();
-    if (load_res < 0) {
+    if (load_res < 0)
+    {
         qemu_file_set_error(f, load_res);
         dirty_bitmap_mig_cancel_incoming();
         if (postcopy_state_get() == POSTCOPY_INCOMING_RUNNING &&
@@ -1897,13 +2051,16 @@
                          "bitmaps are correctly migrated and valid.",
                          __func__, load_res);
             load_res = 0; /* prevent further exit() */
-        } else {
+        }
+        else
+        {
             error_report("%s: loadvm failed: %d", __func__, load_res);
             migrate_set_state(&mis->state, MIGRATION_STATUS_POSTCOPY_ACTIVE,
-                                           MIGRATION_STATUS_FAILED);
+                              MIGRATION_STATUS_FAILED);
         }
     }
-    if (load_res >= 0) {
+    if (load_res >= 0)
+    {
         /*
          * This looks good, but it's possible that the device loading in the
          * main thread hasn't finished yet, and so we might not be in 'RUN'
@@ -1913,7 +2070,8 @@
     }
     postcopy_ram_incoming_cleanup(mis);
 
-    if (load_res < 0) {
+    if (load_res < 0)
+    {
         /*
          * If something went wrong then we have a bad state so exit;
          * depending how far we got it might be possible at this point
@@ -1925,7 +2083,7 @@
     }
 
     migrate_set_state(&mis->state, MIGRATION_STATUS_POSTCOPY_ACTIVE,
-                                   MIGRATION_STATUS_COMPLETED);
+                      MIGRATION_STATUS_COMPLETED);
     /*
      * If everything has worked fine, then the main thread has waited
      * for us to start, and we're the last use of the mis.
@@ -1952,16 +2110,19 @@
 
     trace_loadvm_postcopy_handle_listen("enter");
 
-    if (ps != POSTCOPY_INCOMING_ADVISE && ps != POSTCOPY_INCOMING_DISCARD) {
+    if (ps != POSTCOPY_INCOMING_ADVISE && ps != POSTCOPY_INCOMING_DISCARD)
+    {
         error_report("CMD_POSTCOPY_LISTEN in wrong postcopy state (%d)", ps);
         return -1;
     }
-    if (ps == POSTCOPY_INCOMING_ADVISE) {
+    if (ps == POSTCOPY_INCOMING_ADVISE)
+    {
         /*
          * A rare case, we entered listen without having to do any discards,
          * so do the setup that's normally done at the time of the 1st discard.
          */
-        if (migrate_postcopy_ram()) {
+        if (migrate_postcopy_ram())
+        {
             postcopy_ram_prepare_discard(mis);
         }
     }
@@ -1973,8 +2134,10 @@
      * However, at this point the CPU shouldn't be running, and the IO
      * shouldn't be doing anything yet so don't actually expect requests
      */
-    if (migrate_postcopy_ram()) {
-        if (postcopy_ram_incoming_setup(mis)) {
+    if (migrate_postcopy_ram())
+    {
+        if (postcopy_ram_incoming_setup(mis))
+        {
             postcopy_ram_incoming_cleanup(mis);
             return -1;
         }
@@ -1982,7 +2145,8 @@
 
     trace_loadvm_postcopy_handle_listen("after uffd");
 
-    if (postcopy_notify(POSTCOPY_NOTIFY_INBOUND_LISTEN, &local_err)) {
+    if (postcopy_notify(POSTCOPY_NOTIFY_INBOUND_LISTEN, &local_err))
+    {
         error_report_err(local_err);
         return -1;
     }
@@ -2016,7 +2180,8 @@
     /* Make sure all file formats throw away their mutable metadata.
      * If we get an error here, just don't restart the VM yet. */
     bdrv_activate_all(&local_err);
-    if (local_err) {
+    if (local_err)
+    {
         error_report_err(local_err);
         local_err = NULL;
         autostart = false;
@@ -2026,10 +2191,13 @@
 
     dirty_bitmap_mig_before_vm_start();
 
-    if (autostart) {
+    if (autostart)
+    {
         /* Hold onto your hats, starting the CPU */
         vm_start();
-    } else {
+    }
+    else
+    {
         /* leave it paused and let management decide when to start the CPU */
         runstate_set(RUN_STATE_PAUSED);
     }
@@ -2045,7 +2213,8 @@
     PostcopyState ps = postcopy_state_get();
 
     trace_loadvm_postcopy_handle_run();
-    if (ps != POSTCOPY_INCOMING_LISTENING) {
+    if (ps != POSTCOPY_INCOMING_LISTENING)
+    {
         error_report("CMD_POSTCOPY_RUN in wrong postcopy state (%d)", ps);
         return -1;
     }
@@ -2067,13 +2236,14 @@
                                        gpointer data)
 {
     MigrationIncomingState *mis = data;
-    void *host_addr = (void *) key;
+    void *host_addr = (void *)key;
     ram_addr_t rb_offset;
     RAMBlock *rb;
     int ret;
 
     rb = qemu_ram_block_from_host(host_addr, true, &rb_offset);
-    if (!rb) {
+    if (!rb)
+    {
         /*
          * This should _never_ happen.  However be nice for a migrating VM to
          * not crash/assert.  Post an error (note: intended to not use *_once
@@ -2086,7 +2256,8 @@
     }
 
     ret = migrate_send_rp_message_req_pages(mis, rb, rb_offset);
-    if (ret) {
+    if (ret)
+    {
         /* Please refer to above comment. */
         error_report("%s: send rp message failed for addr %p",
                      __func__, host_addr);
@@ -2100,14 +2271,16 @@
 
 static void migrate_send_rp_req_pages_pending(MigrationIncomingState *mis)
 {
-    WITH_QEMU_LOCK_GUARD(&mis->page_request_mutex) {
+    WITH_QEMU_LOCK_GUARD(&mis->page_request_mutex)
+    {
         g_tree_foreach(mis->page_requested, postcopy_sync_page_req, mis);
     }
 }
 
 static int loadvm_postcopy_handle_resume(MigrationIncomingState *mis)
 {
-    if (mis->state != MIGRATION_STATUS_POSTCOPY_RECOVER) {
+    if (mis->state != MIGRATION_STATUS_POSTCOPY_RECOVER)
+    {
         error_report("%s: illegal resume received", __func__);
         /* Don't fail the load, only for this. */
         return 0;
@@ -2174,7 +2347,8 @@
     length = qemu_get_be32(mis->from_src_file);
     trace_loadvm_handle_cmd_packaged(length);
 
-    if (length > MAX_VM_CMD_PACKAGED_SIZE) {
+    if (length > MAX_VM_CMD_PACKAGED_SIZE)
+    {
         error_report("Unreasonably large packaged state: %zu", length);
         return -1;
     }
@@ -2184,7 +2358,8 @@
     ret = qemu_get_buffer(mis->from_src_file,
                           bioc->data,
                           length);
-    if (ret != length) {
+    if (ret != length)
+    {
         object_unref(OBJECT(bioc));
         error_report("CMD_PACKAGED: Buffer receive fail ret=%d length=%zu",
                      ret, length);
@@ -2218,23 +2393,27 @@
     size_t cnt;
 
     cnt = qemu_get_counted_string(file, block_name);
-    if (!cnt) {
+    if (!cnt)
+    {
         error_report("%s: failed to read block name", __func__);
         return -EINVAL;
     }
 
     /* Validate before using the data */
-    if (qemu_file_get_error(file)) {
+    if (qemu_file_get_error(file))
+    {
         return qemu_file_get_error(file);
     }
 
-    if (len != cnt + 1) {
+    if (len != cnt + 1)
+    {
         error_report("%s: invalid payload length (%d)", __func__, len);
         return -EINVAL;
     }
 
     rb = qemu_ram_block_by_name(block_name);
-    if (!rb) {
+    if (!rb)
+    {
         error_report("%s: block '%s' not found", __func__, block_name);
         return -EINVAL;
     }
@@ -2250,9 +2429,11 @@
 {
     int ret = migration_incoming_enable_colo();
 
-    if (!ret) {
+    if (!ret)
+    {
         ret = colo_init_ram_cache();
-        if (ret) {
+        if (ret)
+        {
             migration_incoming_disable_colo();
         }
     }
@@ -2276,33 +2457,39 @@
     len = qemu_get_be16(f);
 
     /* Check validity before continue processing of cmds */
-    if (qemu_file_get_error(f)) {
+    if (qemu_file_get_error(f))
+    {
         return qemu_file_get_error(f);
     }
 
-    if (cmd >= MIG_CMD_MAX || cmd == MIG_CMD_INVALID) {
+    if (cmd >= MIG_CMD_MAX || cmd == MIG_CMD_INVALID)
+    {
         error_report("MIG_CMD 0x%x unknown (len 0x%x)", cmd, len);
         return -EINVAL;
     }
 
     trace_loadvm_process_command(mig_cmd_args[cmd].name, len);
 
-    if (mig_cmd_args[cmd].len != -1 && mig_cmd_args[cmd].len != len) {
+    if (mig_cmd_args[cmd].len != -1 && mig_cmd_args[cmd].len != len)
+    {
         error_report("%s received with bad length - expecting %zu, got %d",
                      mig_cmd_args[cmd].name,
                      (size_t)mig_cmd_args[cmd].len, len);
         return -ERANGE;
     }
 
-    switch (cmd) {
+    switch (cmd)
+    {
     case MIG_CMD_OPEN_RETURN_PATH:
-        if (mis->to_src_file) {
+        if (mis->to_src_file)
+        {
             error_report("CMD_OPEN_RETURN_PATH called when RP already open");
             /* Not really a problem, so don't give up */
             return 0;
         }
         mis->to_src_file = qemu_file_get_return_path(f);
-        if (!mis->to_src_file) {
+        if (!mis->to_src_file)
+        {
             error_report("CMD_OPEN_RETURN_PATH failed");
             return -1;
         }
@@ -2311,7 +2498,8 @@
     case MIG_CMD_PING:
         tmp32 = qemu_get_be32(f);
         trace_loadvm_process_command_ping(tmp32);
-        if (!mis->to_src_file) {
+        if (!mis->to_src_file)
+        {
             error_report("CMD_PING (0x%x) received with no return path",
                          tmp32);
             return -1;
@@ -2359,7 +2547,8 @@
     uint8_t read_mark;
     uint32_t read_section_id;
 
-    if (!migrate_get_current()->send_section_footer) {
+    if (!migrate_get_current()->send_section_footer)
+    {
         /* No footer to check */
         return true;
     }
@@ -2367,19 +2556,22 @@
     read_mark = qemu_get_byte(f);
 
     ret = qemu_file_get_error(f);
-    if (ret) {
+    if (ret)
+    {
         error_report("%s: Read section footer failed: %d",
                      __func__, ret);
         return false;
     }
 
-    if (read_mark != QEMU_VM_SECTION_FOOTER) {
+    if (read_mark != QEMU_VM_SECTION_FOOTER)
+    {
         error_report("Missing section footer for %s", se->idstr);
         return false;
     }
 
     read_section_id = qemu_get_be32(f);
-    if (read_section_id != se->load_section_id) {
+    if (read_section_id != se->load_section_id)
+    {
         error_report("Mismatched section id in footer for %s -"
                      " read 0x%x expected 0x%x",
                      se->idstr, read_section_id, se->load_section_id);
@@ -2400,7 +2592,8 @@
 
     /* Read section start */
     section_id = qemu_get_be32(f);
-    if (!qemu_get_counted_string(f, idstr)) {
+    if (!qemu_get_counted_string(f, idstr))
+    {
         error_report("Unable to read ID string for section %u",
                      section_id);
         return -EINVAL;
@@ -2409,18 +2602,20 @@
     version_id = qemu_get_be32(f);
 
     ret = qemu_file_get_error(f);
-    if (ret) {
+    if (ret)
+    {
         error_report("%s: Failed to read instance/version ID: %d",
                      __func__, ret);
         return ret;
     }
 
     trace_qemu_loadvm_state_section_startfull(section_id, idstr,
-            instance_id, version_id);
+                                              instance_id, version_id);
     /* Find savevm section */
     se = find_se(idstr, instance_id);
-    if (se == NULL) {
-        error_report("Unknown savevm section or instance '%s' %"PRIu32". "
+    if (se == NULL)
+    {
+        error_report("Unknown savevm section or instance '%s' %" PRIu32 ". "
                      "Make sure that your current VM setup matches your "
                      "saved VM setup, including any hotplugged devices",
                      idstr, instance_id);
@@ -2428,7 +2623,8 @@
     }
 
     /* Validate version */
-    if (version_id > se->version_id) {
+    if (version_id > se->version_id)
+    {
         error_report("savevm: unsupported version %d for '%s' v%d",
                      version_id, idstr, se->version_id);
         return -EINVAL;
@@ -2437,18 +2633,22 @@
     se->load_section_id = section_id;
 
     /* Validate if it is a device's state */
-    if (xen_enabled() && se->is_ram) {
+    if (xen_enabled() && se->is_ram)
+    {
         error_report("loadvm: %s RAM loading not allowed on Xen", idstr);
         return -EINVAL;
     }
 
     ret = vmstate_load(f, se);
-    if (ret < 0) {
-        error_report("error while loading state for instance 0x%"PRIx32" of"
-                     " device '%s'", instance_id, idstr);
+    if (ret < 0)
+    {
+        error_report("error while loading state for instance 0x%" PRIx32 " of"
+                     " device '%s'",
+                     instance_id, idstr);
         return ret;
     }
-    if (!check_section_footer(f, se)) {
+    if (!check_section_footer(f, se))
+    {
         return -EINVAL;
     }
 
@@ -2465,30 +2665,36 @@
     section_id = qemu_get_be32(f);
 
     ret = qemu_file_get_error(f);
-    if (ret) {
+    if (ret)
+    {
         error_report("%s: Failed to read section ID: %d",
                      __func__, ret);
         return ret;
     }
 
     trace_qemu_loadvm_state_section_partend(section_id);
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (se->load_section_id == section_id) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (se->load_section_id == section_id)
+        {
             break;
         }
     }
-    if (se == NULL) {
+    if (se == NULL)
+    {
         error_report("Unknown savevm section %d", section_id);
         return -EINVAL;
     }
 
     ret = vmstate_load(f, se);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         error_report("error while loading state section id %d(%s)",
                      section_id, se->idstr);
         return ret;
     }
-    if (!check_section_footer(f, se)) {
+    if (!check_section_footer(f, se))
+    {
         return -EINVAL;
     }
 
@@ -2501,30 +2707,36 @@
     int ret;
 
     v = qemu_get_be32(f);
-    if (v != QEMU_VM_FILE_MAGIC) {
+    if (v != QEMU_VM_FILE_MAGIC)
+    {
         error_report("Not a migration stream");
         return -EINVAL;
     }
 
     v = qemu_get_be32(f);
-    if (v == QEMU_VM_FILE_VERSION_COMPAT) {
+    if (v == QEMU_VM_FILE_VERSION_COMPAT)
+    {
         error_report("SaveVM v2 format is obsolete and don't work anymore");
         return -ENOTSUP;
     }
-    if (v != QEMU_VM_FILE_VERSION) {
+    if (v != QEMU_VM_FILE_VERSION)
+    {
         error_report("Unsupported migration stream version");
         return -ENOTSUP;
     }
 
-    if (migrate_get_current()->send_configuration) {
-        if (qemu_get_byte(f) != QEMU_VM_CONFIGURATION) {
+    if (migrate_get_current()->send_configuration)
+    {
+        if (qemu_get_byte(f) != QEMU_VM_CONFIGURATION)
+        {
             error_report("Configuration section missing");
             qemu_loadvm_state_cleanup();
             return -EINVAL;
         }
         ret = vmstate_load_state(f, &vmstate_configuration, &savevm_state, 0);
 
-        if (ret) {
+        if (ret)
+        {
             qemu_loadvm_state_cleanup();
             return ret;
         }
@@ -2538,18 +2750,23 @@
     int ret;
 
     trace_loadvm_state_setup();
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (!se->ops || !se->ops->load_setup) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (!se->ops || !se->ops->load_setup)
+        {
             continue;
         }
-        if (se->ops->is_active) {
-            if (!se->ops->is_active(se->opaque)) {
+        if (se->ops->is_active)
+        {
+            if (!se->ops->is_active(se->opaque))
+            {
                 continue;
             }
         }
 
         ret = se->ops->load_setup(f, se->opaque);
-        if (ret < 0) {
+        if (ret < 0)
+        {
             qemu_file_set_error(f, ret);
             error_report("Load state of device %s failed", se->idstr);
             return ret;
@@ -2563,8 +2780,10 @@
     SaveStateEntry *se;
 
     trace_loadvm_state_cleanup();
-    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
-        if (se->ops && se->ops->load_cleanup) {
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry)
+    {
+        if (se->ops && se->ops->load_cleanup)
+        {
             se->ops->load_cleanup(se->opaque);
         }
     }
@@ -2581,7 +2800,8 @@
      * proper recovery later (which will sync src dirty bitmap with receivedmap
      * on dest) these cached small pages will be resent again.
      */
-    for (i = 0; i < mis->postcopy_channels; i++) {
+    for (i = 0; i < mis->postcopy_channels; i++)
+    {
         postcopy_temp_page_reset(&mis->postcopy_tmp_pages[i]);
     }
 
@@ -2619,7 +2839,8 @@
     error_report("Detected IO failure for postcopy. "
                  "Migration paused.");
 
-    while (mis->state == MIGRATION_STATUS_POSTCOPY_PAUSED) {
+    while (mis->state == MIGRATION_STATUS_POSTCOPY_PAUSED)
+    {
         qemu_sem_wait(&mis->postcopy_pause_sem_dst);
     }
 
@@ -2634,34 +2855,40 @@
     int ret = 0;
 
 retry:
-    while (true) {
+    while (true)
+    {
         section_type = qemu_get_byte(f);
 
-        if (qemu_file_get_error(f)) {
+        if (qemu_file_get_error(f))
+        {
             ret = qemu_file_get_error(f);
             break;
         }
 
         trace_qemu_loadvm_state_section(section_type);
-        switch (section_type) {
+        switch (section_type)
+        {
         case QEMU_VM_SECTION_START:
         case QEMU_VM_SECTION_FULL:
             ret = qemu_loadvm_section_start_full(f, mis);
-            if (ret < 0) {
+            if (ret < 0)
+            {
                 goto out;
             }
             break;
         case QEMU_VM_SECTION_PART:
         case QEMU_VM_SECTION_END:
             ret = qemu_loadvm_section_part_end(f, mis);
-            if (ret < 0) {
+            if (ret < 0)
+            {
                 goto out;
             }
             break;
         case QEMU_VM_COMMAND:
             ret = loadvm_process_command(f);
             trace_qemu_loadvm_state_section_command(ret);
-            if ((ret < 0) || (ret == LOADVM_QUIT)) {
+            if ((ret < 0) || (ret == LOADVM_QUIT))
+            {
                 goto out;
             }
             break;
@@ -2676,7 +2903,8 @@
     }
 
 out:
-    if (ret < 0) {
+    if (ret < 0)
+    {
         qemu_file_set_error(f, ret);
 
         /* Cancel bitmaps incoming regardless of recovery */
@@ -2694,7 +2922,8 @@
          * recovering.
          */
         if (postcopy_state_get() == POSTCOPY_INCOMING_RUNNING &&
-            migrate_postcopy_ram() && postcopy_pause_incoming(mis)) {
+            migrate_postcopy_ram() && postcopy_pause_incoming(mis))
+        {
             /* Reset f to point to the newly created channel */
             f = mis->from_src_file;
             goto retry;
@@ -2709,17 +2938,20 @@
     Error *local_err = NULL;
     int ret;
 
-    if (qemu_savevm_state_blocked(&local_err)) {
+    if (qemu_savevm_state_blocked(&local_err))
+    {
         error_report_err(local_err);
         return -EINVAL;
     }
 
     ret = qemu_loadvm_state_header(f);
-    if (ret) {
+    if (ret)
+    {
         return ret;
     }
 
-    if (qemu_loadvm_state_setup(f) != 0) {
+    if (qemu_loadvm_state_setup(f) != 0)
+    {
         return -EINVAL;
     }
 
@@ -2730,12 +2962,14 @@
 
     trace_qemu_loadvm_state_post_main(ret);
 
-    if (mis->have_listen_thread) {
+    if (mis->have_listen_thread)
+    {
         /* Listen thread still going, can't clean up yet */
         return ret;
     }
 
-    if (ret == 0) {
+    if (ret == 0)
+    {
         ret = qemu_file_get_error(f);
     }
 
@@ -2749,23 +2983,28 @@
      * We also mustn't read data that isn't there; some transports (RDMA)
      * will stall waiting for that data when the source has already closed.
      */
-    if (ret == 0 && should_send_vmdesc()) {
+    if (ret == 0 && should_send_vmdesc())
+    {
         uint8_t *buf;
         uint32_t size;
-        uint8_t  section_type = qemu_get_byte(f);
+        uint8_t section_type = qemu_get_byte(f);
 
-        if (section_type != QEMU_VM_VMDESCRIPTION) {
+        if (section_type != QEMU_VM_VMDESCRIPTION)
+        {
             error_report("Expected vmdescription section, but got %d",
                          section_type);
             /*
              * It doesn't seem worth failing at this point since
              * we apparently have an otherwise valid VM state
              */
-        } else {
+        }
+        else
+        {
             buf = g_malloc(0x1000);
             size = qemu_get_be32(f);
 
-            while (size > 0) {
+            while (size > 0)
+            {
                 uint32_t read_chunk = MIN(size, 0x1000);
                 qemu_get_buffer(f, buf, read_chunk);
                 size -= read_chunk;
@@ -2787,7 +3026,8 @@
 
     /* Load QEMU_VM_SECTION_FULL section */
     ret = qemu_loadvm_state_main(f, mis);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         error_report("Failed to load device state: %d", ret);
         return ret;
     }
@@ -2797,7 +3037,7 @@
 }
 
 bool save_snapshot(const char *name, bool overwrite, const char *vmstate,
-                  bool has_devices, strList *devices, Error **errp)
+                   bool has_devices, strList *devices, Error **errp)
 {
     BlockDriverState *bs;
     QEMUSnapshotInfo sn1, *sn = &sn1;
@@ -2810,33 +3050,43 @@
 
     GLOBAL_STATE_CODE();
 
-    if (migration_is_blocked(errp)) {
+    if (migration_is_blocked(errp))
+    {
         return false;
     }
 
-    if (!replay_can_snapshot()) {
+    if (!replay_can_snapshot())
+    {
         error_setg(errp, "Record/replay does not allow making snapshot "
-                   "right now. Try once more later.");
+                         "right now. Try once more later.");
         return false;
     }
 
-    if (!bdrv_all_can_snapshot(has_devices, devices, errp)) {
+    if (!bdrv_all_can_snapshot(has_devices, devices, errp))
+    {
         return false;
     }
 
     /* Delete old snapshots of the same name */
-    if (name) {
-        if (overwrite) {
+    if (name)
+    {
+        if (overwrite)
+        {
             if (bdrv_all_delete_snapshot(name, has_devices,
-                                         devices, errp) < 0) {
+                                         devices, errp) < 0)
+            {
                 return false;
             }
-        } else {
+        }
+        else
+        {
             ret2 = bdrv_all_has_snapshot(name, has_devices, devices, errp);
-            if (ret2 < 0) {
+            if (ret2 < 0)
+            {
                 return false;
             }
-            if (ret2 == 1) {
+            if (ret2 == 1)
+            {
                 error_setg(errp,
                            "Snapshot '%s' already exists in one or more devices",
                            name);
@@ -2846,7 +3096,8 @@
     }
 
     bs = bdrv_all_find_vmstate_bs(vmstate, has_devices, devices, errp);
-    if (bs == NULL) {
+    if (bs == NULL)
+    {
         return false;
     }
     aio_context = bdrv_get_aio_context(bs);
@@ -2854,7 +3105,8 @@
     saved_vm_running = runstate_is_running();
 
     ret = global_state_store();
-    if (ret) {
+    if (ret)
+    {
         error_setg(errp, "Error saving global state");
         return false;
     }
@@ -2870,32 +3122,41 @@
     sn->date_sec = g_date_time_to_unix(now);
     sn->date_nsec = g_date_time_get_microsecond(now) * 1000;
     sn->vm_clock_nsec = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL);
-    if (replay_mode != REPLAY_MODE_NONE) {
+    if (replay_mode != REPLAY_MODE_NONE)
+    {
         sn->icount = replay_get_current_icount();
-    } else {
+    }
+    else
+    {
         sn->icount = -1ULL;
     }
 
-    if (name) {
+    if (name)
+    {
         pstrcpy(sn->name, sizeof(sn->name), name);
-    } else {
-        g_autofree char *autoname = g_date_time_format(now,  "vm-%Y%m%d%H%M%S");
+    }
+    else
+    {
+        g_autofree char *autoname = g_date_time_format(now, "vm-%Y%m%d%H%M%S");
         pstrcpy(sn->name, sizeof(sn->name), autoname);
     }
 
     /* save the VM state */
     f = qemu_fopen_bdrv(bs, 1);
-    if (!f) {
+    if (!f)
+    {
         error_setg(errp, "Could not open VM state file");
         goto the_end;
     }
     ret = qemu_savevm_state(f, errp);
     vm_state_size = qemu_ftell(f);
     ret2 = qemu_fclose(f);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         goto the_end;
     }
-    if (ret2 < 0) {
+    if (ret2 < 0)
+    {
         ret = ret2;
         goto the_end;
     }
@@ -2910,21 +3171,24 @@
 
     ret = bdrv_all_create_snapshot(sn, bs, vm_state_size,
                                    has_devices, devices, errp);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         bdrv_all_delete_snapshot(sn->name, has_devices, devices, NULL);
         goto the_end;
     }
 
     ret = 0;
 
- the_end:
-    if (aio_context) {
+the_end:
+    if (aio_context)
+    {
         aio_context_release(aio_context);
     }
 
     bdrv_drain_all_end();
 
-    if (saved_vm_running) {
+    if (saved_vm_running)
+    {
         vm_start();
     }
     return ret == 0;
@@ -2938,7 +3202,8 @@
     int saved_vm_running;
     int ret;
 
-    if (!has_live) {
+    if (!has_live)
+    {
         /* live default to true so old version of Xen tool stack can have a
          * successful live migration */
         live = true;
@@ -2950,33 +3215,40 @@
 
     ioc = qio_channel_file_new_path(filename, O_WRONLY | O_CREAT | O_TRUNC,
                                     0660, errp);
-    if (!ioc) {
+    if (!ioc)
+    {
         goto the_end;
     }
     qio_channel_set_name(QIO_CHANNEL(ioc), "migration-xen-save-state");
     f = qemu_fopen_channel_output(QIO_CHANNEL(ioc));
     object_unref(OBJECT(ioc));
     ret = qemu_save_device_state(f);
-    if (ret < 0 || qemu_fclose(f) < 0) {
+    if (ret < 0 || qemu_fclose(f) < 0)
+    {
         error_setg(errp, QERR_IO_ERROR);
-    } else {
+    }
+    else
+    {
         /* libxl calls the QMP command "stop" before calling
          * "xen-save-devices-state" and in case of migration failure, libxl
          * would call "cont".
          * So call bdrv_inactivate_all (release locks) here to let the other
          * side of the migration take control of the images.
          */
-        if (live && !saved_vm_running) {
+        if (live && !saved_vm_running)
+        {
             ret = bdrv_inactivate_all();
-            if (ret) {
+            if (ret)
+            {
                 error_setg(errp, "%s: bdrv_inactivate_all() failed (%d)",
                            __func__, ret);
             }
         }
     }
 
- the_end:
-    if (saved_vm_running) {
+the_end:
+    if (saved_vm_running)
+    {
         vm_start();
     }
 }
@@ -2990,14 +3262,16 @@
     /* Guest must be paused before loading the device state; the RAM state
      * will already have been loaded by xc
      */
-    if (runstate_is_running()) {
+    if (runstate_is_running())
+    {
         error_setg(errp, "Cannot update device state while vm is running");
         return;
     }
     vm_stop(RUN_STATE_RESTORE_VM);
 
     ioc = qio_channel_file_new_path(filename, O_RDONLY | O_BINARY, 0, errp);
-    if (!ioc) {
+    if (!ioc)
+    {
         return;
     }
     qio_channel_set_name(QIO_CHANNEL(ioc), "migration-xen-load-state");
@@ -3006,7 +3280,8 @@
 
     ret = qemu_loadvm_state(f);
     qemu_fclose(f);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         error_setg(errp, QERR_IO_ERROR);
     }
     migration_incoming_state_destroy();
@@ -3022,21 +3297,25 @@
     AioContext *aio_context;
     MigrationIncomingState *mis = migration_incoming_get_current();
 
-    if (!bdrv_all_can_snapshot(has_devices, devices, errp)) {
+    if (!bdrv_all_can_snapshot(has_devices, devices, errp))
+    {
         return false;
     }
     ret = bdrv_all_has_snapshot(name, has_devices, devices, errp);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         return false;
     }
-    if (ret == 0) {
+    if (ret == 0)
+    {
         error_setg(errp, "Snapshot '%s' does not exist in one or more devices",
                    name);
         return false;
     }
 
     bs_vm_state = bdrv_all_find_vmstate_bs(vmstate, has_devices, devices, errp);
-    if (!bs_vm_state) {
+    if (!bs_vm_state)
+    {
         return false;
     }
     aio_context = bdrv_get_aio_context(bs_vm_state);
@@ -3045,11 +3324,14 @@
     aio_context_acquire(aio_context);
     ret = bdrv_snapshot_find(bs_vm_state, &sn, name);
     aio_context_release(aio_context);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         return false;
-    } else if (sn.vm_state_size == 0) {
+    }
+    else if (sn.vm_state_size == 0)
+    {
         error_setg(errp, "This is a disk-only snapshot. Revert to it "
-                   " offline using qemu-img");
+                         " offline using qemu-img");
         return false;
     }
 
@@ -3063,13 +3345,15 @@
     bdrv_drain_all_begin();
 
     ret = bdrv_all_goto_snapshot(name, has_devices, devices, errp);
-    if (ret < 0) {
+    if (ret < 0)
+    {
         goto err_drain;
     }
 
     /* restore the VM state */
     f = qemu_fopen_bdrv(bs_vm_state, 0);
-    if (!f) {
+    if (!f)
+    {
         error_setg(errp, "Could not open VM state file");
         goto err_drain;
     }
@@ -3077,7 +3361,8 @@
     qemu_system_reset(SHUTDOWN_CAUSE_NONE);
     mis->from_src_file = f;
 
-    if (!yank_register_instance(MIGRATION_YANK_INSTANCE, errp)) {
+    if (!yank_register_instance(MIGRATION_YANK_INSTANCE, errp))
+    {
         ret = -EINVAL;
         goto err_drain;
     }
@@ -3088,7 +3373,8 @@
 
     bdrv_drain_all_end();
 
-    if (ret < 0) {
+    if (ret < 0)
+    {
         error_setg(errp, "Error %d while loading VM state", ret);
         return false;
     }
@@ -3103,11 +3389,13 @@
 bool delete_snapshot(const char *name, bool has_devices,
                      strList *devices, Error **errp)
 {
-    if (!bdrv_all_can_snapshot(has_devices, devices, errp)) {
+    if (!bdrv_all_can_snapshot(has_devices, devices, errp))
+    {
         return false;
     }
 
-    if (bdrv_all_delete_snapshot(name, has_devices, devices, errp) < 0) {
+    if (bdrv_all_delete_snapshot(name, has_devices, devices, errp) < 0)
+    {
         return false;
     }
 
@@ -3135,14 +3423,16 @@
 bool vmstate_check_only_migratable(const VMStateDescription *vmsd)
 {
     /* check needed if --only-migratable is specified */
-    if (!only_migratable) {
+    if (!only_migratable)
+    {
         return true;
     }
 
     return !(vmsd && vmsd->unmigratable);
 }
 
-typedef struct SnapshotJob {
+typedef struct SnapshotJob
+{
     Job common;
     char *tag;
     char *vmstate;
@@ -3159,7 +3449,6 @@
     qapi_free_strList(s->devices);
 }
 
-
 static void snapshot_load_job_bh(void *opaque)
 {
     Job *job = opaque;
@@ -3172,7 +3461,8 @@
     vm_stop(RUN_STATE_RESTORE_VM);
 
     s->ret = load_snapshot(s->tag, s->vmstate, true, s->devices, s->errp);
-    if (s->ret && orig_vm_running) {
+    if (s->ret && orig_vm_running)
+    {
         vm_start();
     }
 
@@ -3242,38 +3532,38 @@
     return s->ret ? 0 : -1;
 }
 
-
 static const JobDriver snapshot_load_job_driver = {
     .instance_size = sizeof(SnapshotJob),
-    .job_type      = JOB_TYPE_SNAPSHOT_LOAD,
-    .run           = snapshot_load_job_run,
+    .job_type = JOB_TYPE_SNAPSHOT_LOAD,
+    .run = snapshot_load_job_run,
 };
 
 static const JobDriver snapshot_save_job_driver = {
     .instance_size = sizeof(SnapshotJob),
-    .job_type      = JOB_TYPE_SNAPSHOT_SAVE,
-    .run           = snapshot_save_job_run,
+    .job_type = JOB_TYPE_SNAPSHOT_SAVE,
+    .run = snapshot_save_job_run,
 };
 
 static const JobDriver snapshot_delete_job_driver = {
     .instance_size = sizeof(SnapshotJob),
-    .job_type      = JOB_TYPE_SNAPSHOT_DELETE,
-    .run           = snapshot_delete_job_run,
+    .job_type = JOB_TYPE_SNAPSHOT_DELETE,
+    .run = snapshot_delete_job_run,
 };
 
-
 void qmp_snapshot_save(const char *job_id,
                        const char *tag,
                        const char *vmstate,
                        strList *devices,
                        Error **errp)
 {
+    printf(" im in qemp snapshot save!!\n");
     SnapshotJob *s;
-
     s = job_create(job_id, &snapshot_save_job_driver, NULL,
                    qemu_get_aio_context(), JOB_MANUAL_DISMISS,
                    NULL, NULL, errp);
-    if (!s) {
+
+    if (!s)
+    {
         return;
     }
 
@@ -3295,7 +3585,8 @@
     s = job_create(job_id, &snapshot_load_job_driver, NULL,
                    qemu_get_aio_context(), JOB_MANUAL_DISMISS,
                    NULL, NULL, errp);
-    if (!s) {
+    if (!s)
+    {
         return;
     }
 
@@ -3316,7 +3607,8 @@
     s = job_create(job_id, &snapshot_delete_job_driver, NULL,
                    qemu_get_aio_context(), JOB_MANUAL_DISMISS,
                    NULL, NULL, errp);
-    if (!s) {
+    if (!s)
+    {
         return;
     }
 
diff -ruN qemu/plugins/api.c qemu_patched/plugins/api.c
--- qemu/plugins/api.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/plugins/api.c	2023-12-20 23:32:18.574980611 +0100
@@ -440,3 +440,66 @@
 #endif
     return entry;
 }
+
+uint32_t qemu_plugin_get_cpu_register(unsigned int cpu_index,
+                                    unsigned int offset1, 
+                                    unsigned int offset2, 
+                                    unsigned int reg) {
+    void* cpu = qemu_get_cpu(cpu_index);
+    return *(uint32_t*)(cpu + offset1 + offset2 + reg * 4);
+}
+
+uint64_t qemu_plugin_get_cpu_register_64(unsigned int cpu_index,
+                                    unsigned int offset1, 
+                                    unsigned int offset2, 
+                                    unsigned int reg) {
+    void* cpu = qemu_get_cpu(cpu_index);
+    return *(uint64_t*)(16*32 + cpu + offset1 + offset2 + reg * 8);
+}
+
+
+void qemu_plugin_set_cpu_register(unsigned int cpu_index,
+                                     unsigned int offset1,
+                                     unsigned int offset2,
+                                     unsigned int reg,
+                                     uint64_t value) {
+    void* cpu = qemu_get_cpu(cpu_index);
+    // Trova l'indirizzo del registro all'interno della memoria della CPU
+    uint32_t* register_address = (uint32_t*)((cpu + offset1 + offset2 + reg * 4));
+    // Scrivi il valore nel registro
+    *register_address = value;
+}
+
+void qemu_plugin_vcpu_read_phys_mem(unsigned int vcpu_index,
+                                    uint64_t addr,
+                                    void *buf,
+                                    uint64_t len) {
+    CPUClass *cc;
+    CPUState *cpu;
+
+    cpu = qemu_get_cpu(vcpu_index);
+    cc = CPU_GET_CLASS(cpu);
+    if (cc->memory_rw_debug) {
+        cc->memory_rw_debug(cpu, addr, buf, len, false);
+    } else {
+        cpu_memory_rw_debug(cpu, addr, buf, len, false);
+    }
+}
+
+void qemu_plugin_vcpu_write_phys_mem(unsigned int vcpu_index,
+                                    uint64_t addr,
+                                    void *buf,
+                                    uint64_t len) {
+    CPUClass *cc;
+    CPUState *cpu;
+
+    cpu = qemu_get_cpu(vcpu_index);
+    cc = CPU_GET_CLASS(cpu);
+    if (cc->memory_rw_debug) {
+        cc->memory_rw_debug(cpu, addr, buf, len, true);
+    } else {
+        cpu_memory_rw_debug(cpu, addr, buf, len, true);
+    }
+}
+
+
diff -ruN qemu/plugins/qemu-plugins.symbols qemu_patched/plugins/qemu-plugins.symbols
--- qemu/plugins/qemu-plugins.symbols	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/plugins/qemu-plugins.symbols	2023-11-03 16:31:36.674401562 +0100
@@ -42,4 +42,9 @@
   qemu_plugin_tb_vaddr;
   qemu_plugin_uninstall;
   qemu_plugin_vcpu_for_each;
+  qemu_plugin_get_cpu_register;
+  qemu_plugin_get_cpu_register_64;
+  qemu_plugin_set_cpu_register;
+  qemu_plugin_vcpu_read_phys_mem;
+  qemu_plugin_vcpu_write_phys_mem;
 };
I file binari qemu/scripts/qapi/__pycache__/commands.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/commands.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/common.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/common.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/error.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/error.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/events.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/events.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/expr.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/expr.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/gen.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/gen.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/__init__.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/__init__.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/introspect.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/introspect.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/main.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/main.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/parser.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/parser.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/schema.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/schema.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/source.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/source.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/types.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/types.cpython-38.pyc sono diversi
I file binari qemu/scripts/qapi/__pycache__/visit.cpython-38.pyc e qemu_patched/scripts/qapi/__pycache__/visit.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/backend/__pycache__/dtrace.cpython-38.pyc e qemu_patched/scripts/tracetool/backend/__pycache__/dtrace.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/backend/__pycache__/__init__.cpython-38.pyc e qemu_patched/scripts/tracetool/backend/__pycache__/__init__.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/backend/__pycache__/log.cpython-38.pyc e qemu_patched/scripts/tracetool/backend/__pycache__/log.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/format/__pycache__/c.cpython-38.pyc e qemu_patched/scripts/tracetool/format/__pycache__/c.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/format/__pycache__/h.cpython-38.pyc e qemu_patched/scripts/tracetool/format/__pycache__/h.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/format/__pycache__/__init__.cpython-38.pyc e qemu_patched/scripts/tracetool/format/__pycache__/__init__.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/__pycache__/__init__.cpython-38.pyc e qemu_patched/scripts/tracetool/__pycache__/__init__.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/__pycache__/transform.cpython-38.pyc e qemu_patched/scripts/tracetool/__pycache__/transform.cpython-38.pyc sono diversi
I file binari qemu/scripts/tracetool/__pycache__/vcpu.cpython-38.pyc e qemu_patched/scripts/tracetool/__pycache__/vcpu.cpython-38.pyc sono diversi
diff -ruN qemu/softmmu/main.c qemu_patched/softmmu/main.c
--- qemu/softmmu/main.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/softmmu/main.c	2023-10-30 19:01:26.105790767 +0100
@@ -27,6 +27,8 @@
 #include "sysemu/sysemu.h"
 
 #include "qemu-traces.h"
+#include "monitor/monitor-internal.h"
+#include "monitor/monitor.h"
 
 #ifdef CONFIG_SDL
 #if defined(__APPLE__) || defined(main)
@@ -49,6 +51,7 @@
 int main(int argc, char **argv, char **envp)
 {
     qemu_init(argc, argv, envp);
+    qemu_fuzzing_loop();
     qemu_main_loop();
     qemu_cleanup();
 
diff -ruN qemu/softmmu/runstate.c qemu_patched/softmmu/runstate.c
--- qemu/softmmu/runstate.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/softmmu/runstate.c	2024-01-13 16:31:37.700543167 +0100
@@ -34,6 +34,7 @@
 #include "hw/boards.h"
 #include "migration/misc.h"
 #include "migration/postcopy-ram.h"
+#include "migration/snapshot.h"
 #include "monitor/monitor.h"
 #include "net/net.h"
 #include "net/vhost_net.h"
@@ -61,6 +62,883 @@
 #include "sysemu/tpm.h"
 #include "trace.h"
 
+#include "libAFL/queue.h"
+
+#include "monitor/monitor-internal.h"
+
+#include <stdio.h>
+
+#define LOOP_DELAY_SBC 50000
+
+/******************************************************************************
+ *
+ * INCLUDES PER IL FUZZER
+ */
+
+#include "libAFL/config.h"
+#include "libAFL/types.h"
+#include "libAFL/debug.h"
+#include "libAFL/xxh3.h"
+#include "libAFL/alloc-inl.h"
+#include "libAFL/aflpp.h"
+#include "libAFL/afl-returns.h"
+
+#define SUPER_INTERESTING (0.5)
+#define VERY_INTERESTING (0.4)
+#define INTERESTING (0.3)
+
+#define AFL_FEEDBACK_TAG_OUTCOME (0xFEEDC10C)
+
+#define SHM_FUZZING_REPORT "/fuzzing_input_shm_report_buffer"
+#define SEM_RD_END_TCG_PLUGIN_CONTINUE "/fuzzer_tcg_plugin_continue"
+#define SEM_GATHERING "/sem_gathering"
+#define SEM_GATHERING2 "/sem_gathering2"
+
+/******************************************************************************
+ *
+ * SIPC
+ */
+
+#include <sys/types.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <string.h>
+/******************************************************************************
+ *
+ * FUZZER
+ */
+
+struct QueueItem
+{
+    u8 *data;
+    size_t length;
+};
+
+struct SingleFuzzReport
+{
+    int status; // 0 = ok, -1 = error
+    char *info; // info on error
+    struct QueueItem q;
+};
+
+static afl_queue_feedback_t *coverage_feedback_queue;
+static afl_feedback_cov_t *coverage_feedback;
+static afl_observer_covmap_t *observer_covmap;
+
+static afl_queue_global_t *global_queue;
+static int debug = 0;
+static ssize_t calibration_idx = -1;
+
+#include <stdio.h>
+
+#define MAX_SIZE 100
+
+typedef unsigned char u8; // Assuming u8 is an unsigned char
+
+int shm_fd;
+int shm_fd2;
+static u8 *afl_area_ptr;
+static u8 *block_afl_area_ptr;
+static u8 *virgin_bits;
+
+sem_t *wait_for_end_test_case;
+sem_t *tcg_plugin_continue;
+sem_t *fuzzed_input_rdy;
+sem_t *sem_gathering;
+sem_t *sem_gathering2;
+
+int shmfd_fuzzing_input;                // file descriptor
+struct QueueItem *shmptr_fuzzing_input; // shm pointer
+
+int shmfd_fuzzing_report;
+struct SingleFuzzReport *shmptr_fuzzing_report;
+
+// Added variables for the fuzzing component
+MonitorHMP *mon_hmp;
+int fuzzer_mode = DEFAULT_MODE;
+void displayQueueItem(struct QueueItem *q)
+{
+    if (VERBOSE_LOG > 0)
+    {
+        printf("\n[INJECTOR THREAD]\n");
+        printf("length is %d\n", q->length);
+
+        if (VERBOSE_LOG == 2)
+            for (size_t i = 0; i < q->length; i++)
+            {
+                printf("%c", q->data[i]); // Display in hexadecimal format
+                // For character representation: printf("%c ", *(ptr + i));
+                // Note: Uncomment the above line for character representation
+            }
+        printf("\n");
+    }
+}
+/* TODO: Make this a method of queue instead */
+
+static void afl_print_queue()
+{
+    // possibile problema di lettura su entry. problema di concorrenza?
+    printf("\nQUEUE. NUMBER OF ENTRIES: %ld\n", ((afl_queue_t *)global_queue)->entries_count);
+    size_t i;
+    for (i = 0; i < (u32)((afl_queue_t *)global_queue)->entries_count; i++)
+    {
+        afl_entry_t *queue_entry = global_queue->base.funcs.get_queue_entry((afl_queue_t *)global_queue, i);
+
+        printf("\nENTRY: [%d]", queue_entry->input->len);
+        // bug bypass weird item
+        if (queue_entry->input->len < 10000)
+            printf("%s \n", queue_entry->input->bytes);
+        // for(size_t j = 0; j < queue_entry->input->len; j++){
+        //     printf("%02X ", *(queue_entry->input->bytes + j));
+        // }
+        printf("\n");
+    }
+}
+
+/* Initializer: run initial seeds */
+static afl_ret_t mils_fuzzer_initialize(afl_executor_t *executor)
+{
+    mils_executor_t *mils_executor = (mils_executor_t *)executor;
+
+    global_queue = mils_executor->global_queue;
+
+    if (calibration_idx > 0)
+    {
+        if (debug)
+            printf("\nCalibrations to check: %ld\n", calibration_idx);
+        while (calibration_idx > 0)
+        {
+            --calibration_idx;
+            if (debug)
+                printf("\nSeed %ld\n", calibration_idx);
+            afl_entry_t *queue_entry = mils_executor->global_queue->base.funcs.get_queue_entry((afl_queue_t *)mils_executor->global_queue, calibration_idx);
+            if (queue_entry && !queue_entry->info->skip_entry)
+            {
+                if (debug)
+                    printf("Seed %ld testing ...\n", calibration_idx);
+                queue_entry->info->skip_entry = 1;
+                if (afl_stage_run(mils_executor->stage, queue_entry->input, false) == AFL_RET_SUCCESS)
+                {
+                    afl_stage_is_interesting(mils_executor->stage);
+                    queue_entry->info->skip_entry = 0;
+                }
+                else
+                {
+                    WARNF("\nQueue entry %ld misbehaved, disabling...", calibration_idx);
+                }
+            }
+        }
+    }
+
+    if (calibration_idx == 0)
+    {
+        if (debug)
+        {
+            printf("\nCalibration checks done.\n");
+            u32 i;
+            printf("%u seeds:\n", (u32)((afl_queue_t *)mils_executor->global_queue)->entries_count);
+            for (i = 0; i < (u32)((afl_queue_t *)mils_executor->global_queue)->entries_count; i++)
+            {
+                afl_entry_t *queue_entry = mils_executor->global_queue->base.funcs.get_queue_entry((afl_queue_t *)mils_executor->global_queue, i);
+                if (queue_entry && queue_entry->info->skip_entry)
+                    printf("Seed #%u is disabled\n", i);
+            }
+        }
+
+        calibration_idx = -1; /* we are done */
+    }
+
+    return AFL_RET_SUCCESS;
+}
+
+/*
+void write_cur_state()
+{
+  state->current_input_len = current_input->len;
+  state->calibration_idx = calibration_idx;
+}
+*/
+
+void writeToLogFile(afl_input_t *input, double timeElapsed, int interactionsCount, afl_input_t *seed)
+{
+
+    char filename[50]; // Adjust the size as needed
+    time_t currentTime = time(NULL);
+    struct tm *localTime = localtime(&currentTime);
+
+    // Format the timestamp as YYYYMMDD_HHMMSS
+    char timestamp[20];
+    strftime(timestamp, sizeof(timestamp), "%Y%m%d_%H%M%S", localTime);
+
+    sprintf(filename, "./crash/crash_%s_%d.txt", timestamp, interactionsCount);
+
+    FILE *file;
+    file = fopen(filename, "w+"); // Open the file in write mode
+
+    if (file == NULL)
+    {
+        perror("Error opening file");
+        exit(EXIT_FAILURE);
+    }
+
+    // Write input string, time elapsed, and interactions count to the file
+    fprintf(file, "Seed String: %.*s\n", (int)seed->len, seed->bytes);
+    fprintf(file, "Input String: %.*s\n", (int)input->len, input->bytes);
+    fprintf(file, "Time Elapsed: %lf seconds\n", timeElapsed);
+    fprintf(file, "Interactions Count: %d\n", interactionsCount);
+
+    // Close the file
+    fclose(file);
+}
+
+static int ixx = 0;
+static time_t start_time = 0;
+
+struct timespec start_timeC, end_timeC;
+
+void handleCrash(afl_executor_t *executor)
+{
+    clock_gettime(CLOCK_MONOTONIC, &end_timeC);
+    long elapsed_ = 0;
+
+    elapsed_ = (end_timeC.tv_sec - start_timeC.tv_sec) * 1000;       // seconds to milliseconds
+    elapsed_ += (end_timeC.tv_nsec - start_timeC.tv_nsec) / 1000000; // nanoseconds to milliseconds
+    printf("seed was %s\n", global_queue->base.entries[0]->input->bytes);
+    double timeElapsed = (double)elapsed_ / 1000;
+    writeToLogFile(executor->current_input, timeElapsed, ixx, global_queue->base.entries[0]->input);
+    // afl_input_dump_to_crashfile(executor->current_input, global_queue->base.funcs.get_dirpath(&global_queue->base));
+
+    printf("CRASH!!!\n\n");
+    printf("crash causato da input:\n");
+
+    displayQueueItem(shmptr_fuzzing_input);
+    // teoricamente non Ã¨ giusto stampare shmptr_fuzzing_input, ma bisognerebbe gestirla con una struttura
+    // in executor
+    // perÃ² l'utilizzo del semaforo tcg_plugin_continue ci assicura che quando leggiamo questo ci sia ancora il vecchio valore
+    // (se il semaforo l'ho gestito bene lol)
+
+    afl_print_queue();
+    FATAL("Crash Detected, execution arrested");
+}
+
+static const char base64_table[65] = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
+
+char *base64_encode(const unsigned char *data, size_t input_length)
+{
+    size_t output_length = 4 * ((input_length + 2) / 3);
+    char *encoded_data = malloc(output_length + 1);
+    if (encoded_data == NULL)
+        return NULL;
+
+    for (size_t i = 0, j = 0; i < input_length;)
+    {
+        uint32_t octet_a = i < input_length ? data[i++] : 0;
+        uint32_t octet_b = i < input_length ? data[i++] : 0;
+        uint32_t octet_c = i < input_length ? data[i++] : 0;
+
+        uint32_t triple = (octet_a << 0x10) + (octet_b << 0x08) + octet_c;
+
+        encoded_data[j++] = base64_table[(triple >> 3 * 6) & 0x3F];
+        encoded_data[j++] = base64_table[(triple >> 2 * 6) & 0x3F];
+        encoded_data[j++] = base64_table[(triple >> 1 * 6) & 0x3F];
+        encoded_data[j++] = base64_table[(triple >> 0 * 6) & 0x3F];
+    }
+
+    for (size_t i = 0; i < (3 - input_length % 3) % 3; i++)
+    {
+        encoded_data[output_length - 1 - i] = '=';
+    }
+
+    encoded_data[output_length] = '\0';
+    return encoded_data;
+}
+
+size_t base64_encoded_length(size_t input_length)
+{
+    return 4 * ((input_length + 2) / 3);
+}
+int hexCharToInt(char c)
+{
+    if (c >= '0' && c <= '9')
+    {
+        return c - '0';
+    }
+    else if (c >= 'A' && c <= 'F')
+    {
+        return 10 + (c - 'A');
+    }
+    else if (c >= 'a' && c <= 'f')
+    {
+        return 10 + (c - 'a');
+    }
+    return -1; // Invalid character
+}
+char *hexToAscii(const char *hexString)
+{
+    if (hexString == NULL || strlen(hexString) % 2 != 0)
+    {
+        return NULL; // Invalid input or odd-length string
+    }
+
+    size_t hexLen = strlen(hexString);
+    size_t asciiLen = hexLen / 2;
+    char *asciiString = (char *)malloc(asciiLen + 1); // +1 for null terminator
+
+    if (asciiString == NULL)
+    {
+        return NULL; // Memory allocation failed
+    }
+
+    for (size_t i = 0, j = 0; i < hexLen; i += 2, j++)
+    {
+        int highNibble = hexCharToInt(hexString[i]);
+        int lowNibble = hexCharToInt(hexString[i + 1]);
+
+        if (highNibble == -1 || lowNibble == -1)
+        {
+            free(asciiString);
+            return NULL; // Invalid hexadecimal characters
+        }
+
+        asciiString[j] = (char)((highNibble << 4) | lowNibble);
+    }
+
+    asciiString[asciiLen] = '\0'; // Null-terminate the ASCII string
+    return asciiString;
+}
+static int shmfd_fuzzer_mode;                   // file descriptor
+static int *shmptr_fuzzer_mode;                 // shm pointer
+sem_t *sem_rd_fuzzer_mode, *sem_wr_fuzzer_mode; // semaphores to read and write fuzzer mode
+off_t length = sizeof(struct QueueItem);
+
+sem_t *sem_rd_fuzzing_input, *sem_wr_fuzzing_input; // semaphores to read and write fuzzing input
+off_t length_fuzz_input = sizeof(struct QueueItem);
+
+struct timespec start_timeT, end_timeT;
+long elapsed_time;
+
+static int gogo = 0;
+// PRODUTTORE
+static int retries = 0;
+static int max_retries = 0;
+static int ixx2 = 0; // for snapshot workaround
+
+afl_exit_t debug_harness_func(afl_executor_t *executor, u8 *input, size_t len)
+{
+    int status = -2;
+    while (status == -2)
+    {
+        if (++retries > 0)
+        {
+            printf("tentativo numero %d\n", retries);
+            if (retries > max_retries)
+            {
+                max_retries = retries;
+                printf("NEW MAX RETRIES!!! is %d\n", max_retries);
+            }
+        }
+        if (VERBOSE_LOG > 0)
+            printf("\n------\n");
+        (void)executor;
+        /*if (ixx > 0) // else
+        {
+            printf("Loading snapshot!\n");
+            fuzzer_mode = RECOVER_MODE;
+            //  sleep(5);
+            sleep(1);
+            //  sem_wait(sem_rd_fuzzer_mode);
+            printf("Loaded snapshot\n");
+        }*/
+        if (ixx < INIT_TIMES && !SNAPSHOT_ENABLED)
+        {
+            printf("Waiting %d seconds...\n", INIT_WAIT_TIME);
+            sleep(INIT_WAIT_TIME); // this way shm ok is garanteed
+        }
+        // sleep(1);
+        if (ixx > 0)
+            sem_post(tcg_plugin_continue); // utile perchÃ¨ durante il calcolo della bitmap non devbono inficiiare gli input dopo!
+                                           // printf("\ntest\n");
+                                           //  shmptr_fuzzing_input->data = (u8 *)malloc((len+1) * sizeof(u8)); // len +1 ?
+
+        /*size_t input_length = sizeof(jpeg_data) / sizeof(jpeg_data[0]);
+         char *encoded_string = base64_encode(input, input_length);
+         if (encoded_string != NULL)
+         {
+             printf("Encoded string: %s\n", encoded_string);
+
+             size_t encoded_length = base64_encoded_length(input_length);
+             printf("Length of the encoded string: %zu\n", encoded_length);
+             shmptr_fuzzing_input->data = (u8 *)encoded_string;
+             shmptr_fuzzing_input->length = encoded_length;
+             //free(encoded_string);
+         }
+         else
+         {
+             printf("Failed to encode the string.\n");
+         }*/
+        // MINIMIZE to obtain a simpler seed
+
+        // sem_wait(sem_rd_fuzzer_mode);
+
+        sem_wait(sem_wr_fuzzing_input);
+
+        if (ixx > -1)
+        {
+            if (PC_GATHERING_MODE)
+            {
+                shmptr_fuzzing_input->data = "TEST";
+                shmptr_fuzzing_input->length = 4;
+            }
+            else
+            {
+                if (BASE64_MODE) // sendMessageSIPC and receivemsg may truncate on \0. A future approach could be base64 encoding the intercepted sys call and decode in MILS (target application)
+                {
+                    char *input64 = base64_encode(input, len);
+                    size_t len64 = strlen(input64);
+
+                    shmptr_fuzzing_input->data = (u8 *)realloc(shmptr_fuzzing_input->data, (len64 + 1) * sizeof(u8));
+                    memcpy(shmptr_fuzzing_input->data, input64, len64 + 1);
+                    shmptr_fuzzing_input->length = len64;
+                    printf("input 64 is %s\n", input64);
+                    printf("len 64 is %d\n", len64);
+
+                    // shmptr_fuzzing_input->data[len64] = '\0';
+                }
+                else
+                {
+                    shmptr_fuzzing_input->data = (u8 *)realloc(shmptr_fuzzing_input->data, (len + 1) * sizeof(u8));
+                    // memcpy(shmptr_fuzzing_input->data, input, len + 1);
+
+                    for (size_t i = 0; i < len; i++)
+                    {
+                        shmptr_fuzzing_input->data[i] = input[i];
+                    }
+                    shmptr_fuzzing_input->data[len] = '\0';
+                    shmptr_fuzzing_input->length = len;
+                }
+            }
+
+            // questo peszzo di codice qua sotto serve per verificare che tutte le sys call send del mils
+            // vengano effettivamente ricevute (quindi si passa un intero crescente e si vede se arrivano nel log
+            // della seriale del mils)
+
+            /*int num = ixx; // You can change this value to any integer
+            char str[20];  // Define a character array to hold the resulting string
+
+            // Convert integer to string
+            snprintf(str, sizeof(str), "%d", num);
+
+            size_t len = strlen(str);
+            shmptr_fuzzing_input->length = len;
+
+            // Allocate memory for shmptr_fuzzing_input->data
+            shmptr_fuzzing_input->data = (u8 *)realloc(shmptr_fuzzing_input->data, (len + 1) * sizeof(u8));
+
+            // Copy the string content to shmptr_fuzzing_input->data
+            memcpy(shmptr_fuzzing_input->data, str, len + 1); // +1 for null-terminator
+
+            // Update the length if necessary
+            shmptr_fuzzing_input->length = len;*/
+        }
+        else
+        {
+            char *tar = "XX";
+            len = ixx + 1;
+            shmptr_fuzzing_input->data = (u8 *)realloc(shmptr_fuzzing_input->data, (len + 1) * sizeof(u8));
+            for (size_t i = 0; i < len; i++)
+            {
+                shmptr_fuzzing_input->data[i] = tar[i];
+            }
+            shmptr_fuzzing_input->length = len;
+
+            /*char *inputJson = "{ \
+                \"nome\": \"Mario\", \
+                \"cognome\": \"Rossi\", \
+                \"eta\": 30, \
+                \"email\": \"mario.rossi@email.com\", \
+                \"indirizzo\": { \
+                    \"via\": \"Via Roma\", \
+                    \"citta\": \"Roma\", \
+                    \"CAP\": \"00100\" \
+                }, \
+                \"interessi\": [ \
+                    \"musica\", \
+                    \"viaggi\", \
+                    \"tecnologia\" \
+                ] \
+            }";
+            inputJson="1+2+3";
+            shmptr_fuzzing_input->data = inputJson;
+
+            //memcpy(shmptr_fuzzing_input->data, inputJson, sizeof(inputJson));
+
+            shmptr_fuzzing_input->length = strlen(inputJson) / 2;*/
+
+            // shmptr_fuzzing_input->data = inputJson;
+            //  tinyExpr
+            //  char *malevolo = "706f7728312c3231353872e8b02b34352b322b332b3433d4342b746f7728313432313538322b332b34352b322b332b34332b342bcaca4bb7442b37";
+
+            /*
+            shmptr_fuzzing_input->data = hexToAscii(malevolo);
+            shmptr_fuzzing_input->length = strlen(malevolo) / 2;
+            */
+        }
+
+        /*if (ixx == 100000)
+        {
+            afl_print_queue();
+            exit(0);
+            return;
+        }*/
+
+        ixx++;
+
+        // printf("\nfirst value:\n");
+        displayQueueItem(shmptr_fuzzing_input);
+
+        // SEMAFORO PER DIRE CHE INPUT Ã PRONTO AL INJECTOR THREAD E A SUA VOLTA GIRA A TCG PLUGIN
+        // sem_post(fuzzed_input_rdy);
+        printf("input %d Ã¨ pronto\n", shmptr_fuzzing_input->length);
+        // SI ASPETTA CHE FINISCE L'ESECUZIONE DELLA FUNZIONE (ASPETTIAMO SEGNALE DA TCG PLUGIN)
+
+        sem_post(sem_rd_fuzzing_input);
+
+        sem_wait(wait_for_end_test_case);
+        printf("input %d ha finito esecuzione, si preleva status\n", shmptr_fuzzing_input->length);
+
+        // sleep(1);
+        //  SI PRELEVA IL REPORT DAL TCG PLUGIN
+        status = shmptr_fuzzing_report->status;
+        executor->info = shmptr_fuzzing_report->info;
+
+        // dopo che finisce il report, calcoliamo il throughput
+
+        if (THROUGHPUT_METRIC)
+        {
+            if (status != -2)
+            {
+                ixx2++;
+            }
+            if (ixx2 == THROUGHPUT_LBOUND)
+            {                                                 /* THROUGHPUT GATHERING*/
+                clock_gettime(CLOCK_MONOTONIC, &start_timeT); // Get the current time
+            }
+            if (ixx2 == THROUGHPUT_HBOUND)
+            {
+                clock_gettime(CLOCK_MONOTONIC, &end_timeT); // Get the current time after the task
+
+                // Calculate elapsed time in milliseconds
+                elapsed_time = (end_timeT.tv_sec - start_timeT.tv_sec) * 1000;       // seconds to milliseconds
+                elapsed_time += (end_timeT.tv_nsec - start_timeT.tv_nsec) / 1000000; // nanoseconds to milliseconds
+
+                double throughput = (double)ixx2 / elapsed_time * 1000;
+                printf("iterations is %d\n", THROUGHPUT_HBOUND - THROUGHPUT_LBOUND);
+                printf("elapsed time is %ld\n", elapsed_time);
+                printf("throughput is %f\n", throughput);
+                exit(0);
+            }
+        }
+
+        if (PC_GATHERING_MODE)
+        {
+            return AFL_EXIT_OK;
+        }
+
+        if (status == 0)
+        {
+            // ok
+            retries = 0;
+            if (VERBOSE_LOG > 0)
+                afl_print_queue();
+            return AFL_EXIT_OK;
+        }
+        else if (status == -1)
+        {
+            // crashed
+            handleCrash(executor);
+            return AFL_EXIT_ERROR_DETECTED;
+        }
+        else if (status == -2)
+        {
+            // status=-4;
+            //  snapshot error. should retry writing shared mem on next syscall intercettata
+            printf("Trying to fuzz the input %s but failed. trying again..\n", shmptr_fuzzing_report->q.data);
+        }
+    }
+    // should never reach here but just in case
+    return AFL_EXIT_OK;
+}
+
+afl_engine_t *initialize_engine_istance()
+{
+    clock_gettime(CLOCK_MONOTONIC, &start_timeC); // initialize start_time
+
+    mils_executor_t *mils_executor = calloc(1, sizeof(mils_executor_t));
+    if (!mils_executor)
+    {
+        PFATAL("Unable to allocate mem.");
+    }
+    mils_executor_init(mils_executor, debug_harness_func);
+
+    /* Observation channel, map based, we initialize this ourselves since we don't
+     * actually create a shared map */
+    observer_covmap = afl_observer_covmap_new(MAP_SIZE);
+    if (!observer_covmap)
+    {
+        PFATAL("Trace bits channel error");
+    }
+    // afl_observer_covmap_init(observer_covmap,MAP_SIZE); test?
+    afl_shmem_deinit(&observer_covmap->shared_map);
+
+    if (BLOCKCOV_MODE)
+    {
+        observer_covmap->shared_map.map = block_afl_area_ptr; // Coverage "Map" we have
+        observer_covmap->shared_map.map_size = MAP_SIZE;
+        observer_covmap->shared_map.shm_id = -1;
+        observer_covmap->shared_map_block_coverage.map = afl_area_ptr;
+        observer_covmap->shared_map_block_coverage.map_size = MAP_SIZE;
+        observer_covmap->shared_map_block_coverage.shm_id = -2;
+    }
+    else
+    {
+        observer_covmap->shared_map.map = afl_area_ptr; // Coverage "Map" we have
+        observer_covmap->shared_map.map_size = MAP_SIZE;
+        observer_covmap->shared_map.shm_id = -1;
+        observer_covmap->shared_map_block_coverage.map = block_afl_area_ptr;
+        observer_covmap->shared_map_block_coverage.map_size = MAP_SIZE;
+        observer_covmap->shared_map_block_coverage.shm_id = -2;
+    }
+
+    mils_executor->base.funcs.observer_add(&mils_executor->base, &observer_covmap->base);
+
+    coverage_feedback_queue = afl_queue_feedback_new(NULL, (char *)"Coverage feedback queue");
+    if (!coverage_feedback_queue)
+    {
+        FATAL("Error initializing feedback queue");
+    }
+
+    afl_queue_global_t *new_global_queue = afl_queue_global_new();
+    if (!new_global_queue)
+    {
+        FATAL("Error initializing global queue");
+    }
+
+    afl_queue_global_init(new_global_queue);
+    new_global_queue->base.funcs.set_dirpath(&new_global_queue->base, "./crash");
+    new_global_queue->funcs.add_feedback_queue(new_global_queue, coverage_feedback_queue);
+
+    coverage_feedback = afl_feedback_cov_new(coverage_feedback_queue, observer_covmap);
+    coverage_feedback_queue->feedback = &coverage_feedback->base;
+    afl_feedback_cov_init(coverage_feedback, coverage_feedback_queue, observer_covmap);
+
+    // printf("dir path is %s\n",global_queue->base.funcs.get_dirpath(&global_queue->base));
+    afl_queue_feedback_init(coverage_feedback_queue, coverage_feedback, "queue_name1");
+    coverage_feedback_queue->base.funcs.set_dirpath(&coverage_feedback_queue->base, "./coverage");
+    // al momento si aggiunge alla global queue
+
+    if (!coverage_feedback)
+    {
+        FATAL("Error initializing feedback");
+    }
+
+    afl_engine_t *engine = afl_engine_new(&mils_executor->base, NULL, new_global_queue);
+    if (!engine)
+    {
+        FATAL("Error initializing Engine");
+    }
+    engine->verbose = 1;
+    engine->funcs.add_feedback(engine, &coverage_feedback->base);
+    engine->in_dir="./seeds";
+    engine->funcs.set_global_queue(engine, new_global_queue);
+
+    afl_fuzz_one_t *fuzz_one = afl_fuzz_one_new(engine);
+    if (!fuzz_one)
+    {
+        FATAL("Error initializing fuzz_one");
+    }
+
+    /* We also add the fuzzone to the engine here. */
+    engine->funcs.set_fuzz_one(engine, fuzz_one);
+
+    /* Deterministic stage */
+
+    afl_stage_t *det_stage = calloc(1, sizeof(afl_stage_t));
+    if (!det_stage)
+    {
+        FATAL("Error allocating memory for fuzzing stage");
+    }
+
+    if (afl_det_stage_init(det_stage, engine) != AFL_RET_SUCCESS)
+    {
+        FATAL("Error initializing fuzzing stage");
+        free(det_stage); // Free allocated memory before exiting
+    }
+
+    AFL_TRY(det_stage->funcs.add_mutator_to_stage(det_stage, (afl_mutator_t *)afl_mutator_deterministic_new(afl_mutate_bitflip_det, afl_get_iters_bitflip_det)),
+            { FATAL("Error adding mutator: %s", afl_ret_stringify(err)); });
+    AFL_TRY(det_stage->funcs.add_mutator_to_stage(det_stage, (afl_mutator_t *)afl_mutator_deterministic_new(afl_mutate_det_flip_two, afl_get_iters_flip_two_det)),
+            { FATAL("Error adding mutator: %s", afl_ret_stringify(err)); });
+    AFL_TRY(det_stage->funcs.add_mutator_to_stage(det_stage, (afl_mutator_t *)afl_mutator_deterministic_new(afl_mutate_det_flip_four, afl_get_iters_flip_four_det)),
+            { FATAL("Error adding mutator: %s", afl_ret_stringify(err)); });
+    AFL_TRY(det_stage->funcs.add_mutator_to_stage(det_stage, (afl_mutator_t *)afl_mutator_deterministic_new(afl_mutate_det_flip_byte, afl_get_iters_flip_byte_det)),
+            { FATAL("Error adding mutator: %s", afl_ret_stringify(err)); });
+    AFL_TRY(det_stage->funcs.add_mutator_to_stage(det_stage, (afl_mutator_t *)afl_mutator_deterministic_new(afl_mutate_det_flip_two_byte, afl_get_iters_flip_two_byte_det)),
+            { FATAL("Error adding mutator: %s", afl_ret_stringify(err)); });
+
+    /* Havoc stage */
+    afl_mutator_scheduled_t *mutators_havoc = afl_mutator_scheduled_new(engine, 8);
+    if (!mutators_havoc)
+    {
+        FATAL("Error initializing Mutators");
+    }
+
+    AFL_TRY(afl_mutator_scheduled_add_havoc_funcs(mutators_havoc),
+            { FATAL("Error adding mutators: %s", afl_ret_stringify(err)); });
+
+    afl_stage_t *stage = afl_stage_new(engine);
+    if (!stage)
+    {
+        FATAL("Error creating fuzzing stage");
+    }
+    AFL_TRY(stage->funcs.add_mutator_to_stage(stage, &mutators_havoc->base),
+            { FATAL("Error adding mutator: %s", afl_ret_stringify(err)); });
+    mils_executor->stage = stage;
+
+    mils_executor->global_queue = new_global_queue;
+
+    /* Check for engine to be configured properly */
+    if (afl_engine_check_configuration(engine) != AFL_RET_SUCCESS)
+    {
+        printf("Engine configured incompletely");
+    };
+
+    /*AFL_TRY(engine->funcs.load_testcases_from_dir(engine),
+                { WARNF("Error loading testcase dir: %s", afl_ret_stringify(err)); });
+    */
+   
+    AFL_TRY(engine->funcs.load_testcases_from_dir(engine,engine->in_dir),
+                { WARNF("Error loading testcase dir: %s", afl_ret_stringify(err)); });
+  
+
+    /* no seeds? add a dummy one  */
+    if (((afl_queue_t *)engine->global_queue)->entries_count == 0)
+    {
+        afl_input_t *input = afl_input_new();
+        if (!input)
+        {
+            FATAL("Could not create input");
+        }
+        u32 cnt;
+        u32 input_len = 15;
+        input->len = input_len;
+        input->bytes = calloc(input_len + 1, 1);
+        if (!input->bytes)
+        {
+            PFATAL("Could not allocate input bytes");
+        }
+
+        for (cnt = 0; cnt < input_len; cnt++)
+        {
+            input->bytes[cnt] = ' ' + cnt; /*  values: 0x20 ... 0x60 */
+
+            input->bytes[input_len] = 0;
+
+            afl_entry_t *new_entry = afl_entry_new(input, NULL);
+            if (!new_entry)
+            {
+                FATAL("Could not create new entry");
+            }
+            engine->global_queue->base.funcs.insert(&engine->global_queue->base, new_entry);
+        }
+    }
+
+    calibration_idx = (ssize_t)((afl_queue_t *)engine->global_queue)->entries_count;
+    OKF("\nStarting seed count: %lu", calibration_idx);
+
+    return engine;
+}
+
+void fuzzer_process_main(void *data)
+{
+    afl_engine_t *engine = (afl_engine_t *)data;
+
+    afl_observer_covmap_t *observer_covmap = NULL;
+    size_t i;
+    for (i = 0; i < engine->executor->observors_count; i++)
+    {
+
+        if (engine->executor->observors[i]->tag == AFL_OBSERVER_TAG_COVMAP)
+        {
+
+            observer_covmap = (afl_observer_covmap_t *)engine->executor->observors[0];
+        }
+    }
+
+    if (!observer_covmap)
+    {
+        FATAL("Got no covmap observer");
+    }
+
+    // set the global virgin_bits for error handlers, so we can restore them after a crash
+    virgin_bits = observer_covmap->shared_map.map;
+
+    afl_feedback_cov_t *coverage_feedback = NULL;
+    for (i = 0; i < engine->feedbacks_count; i++)
+    {
+
+        if (engine->feedbacks[i]->tag == AFL_FEEDBACK_TAG_COV)
+        {
+
+            coverage_feedback = (afl_feedback_cov_t *)(engine->feedbacks[i]);
+            break;
+        }
+    }
+
+    if (!coverage_feedback)
+    {
+        FATAL("No coverage feedback added to engine");
+    }
+
+    afl_stage_t *stage = engine->fuzz_one->stages[0];
+    afl_mutator_scheduled_t *mutators_havoc = (afl_mutator_scheduled_t *)stage->mutators[0];
+
+    mils_fuzzer_initialize(engine->executor);
+
+    /* The actual fuzzing */
+    AFL_TRY(engine->funcs.loop(engine), { PFATAL("Error fuzzing the target: %s", afl_ret_stringify(err)); });
+
+    SAYF("Fuzzing ends with all the queue entries fuzzed. No of executions %llu\n", engine->executions);
+    printf("segm?\n\n");
+
+    /* Let's free everything now. Note that if you've extended any structure,
+     * which now contains pointers to any dynamically allocated region, you have
+     * to free them yourselves, but the extended structure itself can be de
+     * initialized using the deleted functions provided */
+
+    afl_executor_delete(engine->executor);
+    afl_mutator_scheduled_delete(mutators_havoc);
+    afl_stage_delete(stage);
+    afl_fuzz_one_delete(engine->fuzz_one);
+
+    for (i = 0; i < engine->feedbacks_count; ++i)
+    {
+
+        afl_feedback_delete((afl_feedback_t *)engine->feedbacks[i]);
+    }
+
+    for (i = 0; i < engine->global_queue->feedback_queues_count; ++i)
+    {
+
+        afl_queue_feedback_delete(engine->global_queue->feedback_queues[i]);
+    }
+
+    afl_queue_global_delete(engine->global_queue);
+    afl_engine_delete(engine);
+
+    // taskSuspend (0);
+}
+
 static NotifierList exit_notifiers =
     NOTIFIER_LIST_INITIALIZER(exit_notifiers);
 
@@ -70,101 +948,102 @@
 static RunState vmstop_requested = RUN_STATE__MAX;
 static QemuMutex vmstop_lock;
 
-typedef struct {
+typedef struct
+{
     RunState from;
     RunState to;
 } RunStateTransition;
 
 static const RunStateTransition runstate_transitions_def[] = {
-    { RUN_STATE_PRELAUNCH, RUN_STATE_INMIGRATE },
+    {RUN_STATE_PRELAUNCH, RUN_STATE_INMIGRATE},
 
-    { RUN_STATE_DEBUG, RUN_STATE_RUNNING },
-    { RUN_STATE_DEBUG, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_DEBUG, RUN_STATE_PRELAUNCH },
-
-    { RUN_STATE_INMIGRATE, RUN_STATE_INTERNAL_ERROR },
-    { RUN_STATE_INMIGRATE, RUN_STATE_IO_ERROR },
-    { RUN_STATE_INMIGRATE, RUN_STATE_PAUSED },
-    { RUN_STATE_INMIGRATE, RUN_STATE_RUNNING },
-    { RUN_STATE_INMIGRATE, RUN_STATE_SHUTDOWN },
-    { RUN_STATE_INMIGRATE, RUN_STATE_SUSPENDED },
-    { RUN_STATE_INMIGRATE, RUN_STATE_WATCHDOG },
-    { RUN_STATE_INMIGRATE, RUN_STATE_GUEST_PANICKED },
-    { RUN_STATE_INMIGRATE, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_INMIGRATE, RUN_STATE_PRELAUNCH },
-    { RUN_STATE_INMIGRATE, RUN_STATE_POSTMIGRATE },
-    { RUN_STATE_INMIGRATE, RUN_STATE_COLO },
-
-    { RUN_STATE_INTERNAL_ERROR, RUN_STATE_PAUSED },
-    { RUN_STATE_INTERNAL_ERROR, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_INTERNAL_ERROR, RUN_STATE_PRELAUNCH },
-
-    { RUN_STATE_IO_ERROR, RUN_STATE_RUNNING },
-    { RUN_STATE_IO_ERROR, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_IO_ERROR, RUN_STATE_PRELAUNCH },
-
-    { RUN_STATE_PAUSED, RUN_STATE_RUNNING },
-    { RUN_STATE_PAUSED, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_PAUSED, RUN_STATE_POSTMIGRATE },
-    { RUN_STATE_PAUSED, RUN_STATE_PRELAUNCH },
-    { RUN_STATE_PAUSED, RUN_STATE_COLO},
-
-    { RUN_STATE_POSTMIGRATE, RUN_STATE_RUNNING },
-    { RUN_STATE_POSTMIGRATE, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_POSTMIGRATE, RUN_STATE_PRELAUNCH },
-
-    { RUN_STATE_PRELAUNCH, RUN_STATE_RUNNING },
-    { RUN_STATE_PRELAUNCH, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_PRELAUNCH, RUN_STATE_INMIGRATE },
-
-    { RUN_STATE_FINISH_MIGRATE, RUN_STATE_RUNNING },
-    { RUN_STATE_FINISH_MIGRATE, RUN_STATE_PAUSED },
-    { RUN_STATE_FINISH_MIGRATE, RUN_STATE_POSTMIGRATE },
-    { RUN_STATE_FINISH_MIGRATE, RUN_STATE_PRELAUNCH },
-    { RUN_STATE_FINISH_MIGRATE, RUN_STATE_COLO},
-
-    { RUN_STATE_RESTORE_VM, RUN_STATE_RUNNING },
-    { RUN_STATE_RESTORE_VM, RUN_STATE_PRELAUNCH },
-
-    { RUN_STATE_COLO, RUN_STATE_RUNNING },
-    { RUN_STATE_COLO, RUN_STATE_SHUTDOWN},
-
-    { RUN_STATE_RUNNING, RUN_STATE_DEBUG },
-    { RUN_STATE_RUNNING, RUN_STATE_INTERNAL_ERROR },
-    { RUN_STATE_RUNNING, RUN_STATE_IO_ERROR },
-    { RUN_STATE_RUNNING, RUN_STATE_PAUSED },
-    { RUN_STATE_RUNNING, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_RUNNING, RUN_STATE_RESTORE_VM },
-    { RUN_STATE_RUNNING, RUN_STATE_SAVE_VM },
-    { RUN_STATE_RUNNING, RUN_STATE_SHUTDOWN },
-    { RUN_STATE_RUNNING, RUN_STATE_WATCHDOG },
-    { RUN_STATE_RUNNING, RUN_STATE_GUEST_PANICKED },
-    { RUN_STATE_RUNNING, RUN_STATE_COLO},
-
-    { RUN_STATE_SAVE_VM, RUN_STATE_RUNNING },
-
-    { RUN_STATE_SHUTDOWN, RUN_STATE_PAUSED },
-    { RUN_STATE_SHUTDOWN, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_SHUTDOWN, RUN_STATE_PRELAUNCH },
-    { RUN_STATE_SHUTDOWN, RUN_STATE_COLO },
-
-    { RUN_STATE_DEBUG, RUN_STATE_SUSPENDED },
-    { RUN_STATE_RUNNING, RUN_STATE_SUSPENDED },
-    { RUN_STATE_SUSPENDED, RUN_STATE_RUNNING },
-    { RUN_STATE_SUSPENDED, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_SUSPENDED, RUN_STATE_PRELAUNCH },
-    { RUN_STATE_SUSPENDED, RUN_STATE_COLO},
-
-    { RUN_STATE_WATCHDOG, RUN_STATE_RUNNING },
-    { RUN_STATE_WATCHDOG, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_WATCHDOG, RUN_STATE_PRELAUNCH },
-    { RUN_STATE_WATCHDOG, RUN_STATE_COLO},
-
-    { RUN_STATE_GUEST_PANICKED, RUN_STATE_RUNNING },
-    { RUN_STATE_GUEST_PANICKED, RUN_STATE_FINISH_MIGRATE },
-    { RUN_STATE_GUEST_PANICKED, RUN_STATE_PRELAUNCH },
+    {RUN_STATE_DEBUG, RUN_STATE_RUNNING},
+    {RUN_STATE_DEBUG, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_DEBUG, RUN_STATE_PRELAUNCH},
+
+    {RUN_STATE_INMIGRATE, RUN_STATE_INTERNAL_ERROR},
+    {RUN_STATE_INMIGRATE, RUN_STATE_IO_ERROR},
+    {RUN_STATE_INMIGRATE, RUN_STATE_PAUSED},
+    {RUN_STATE_INMIGRATE, RUN_STATE_RUNNING},
+    {RUN_STATE_INMIGRATE, RUN_STATE_SHUTDOWN},
+    {RUN_STATE_INMIGRATE, RUN_STATE_SUSPENDED},
+    {RUN_STATE_INMIGRATE, RUN_STATE_WATCHDOG},
+    {RUN_STATE_INMIGRATE, RUN_STATE_GUEST_PANICKED},
+    {RUN_STATE_INMIGRATE, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_INMIGRATE, RUN_STATE_PRELAUNCH},
+    {RUN_STATE_INMIGRATE, RUN_STATE_POSTMIGRATE},
+    {RUN_STATE_INMIGRATE, RUN_STATE_COLO},
+
+    {RUN_STATE_INTERNAL_ERROR, RUN_STATE_PAUSED},
+    {RUN_STATE_INTERNAL_ERROR, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_INTERNAL_ERROR, RUN_STATE_PRELAUNCH},
+
+    {RUN_STATE_IO_ERROR, RUN_STATE_RUNNING},
+    {RUN_STATE_IO_ERROR, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_IO_ERROR, RUN_STATE_PRELAUNCH},
+
+    {RUN_STATE_PAUSED, RUN_STATE_RUNNING},
+    {RUN_STATE_PAUSED, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_PAUSED, RUN_STATE_POSTMIGRATE},
+    {RUN_STATE_PAUSED, RUN_STATE_PRELAUNCH},
+    {RUN_STATE_PAUSED, RUN_STATE_COLO},
+
+    {RUN_STATE_POSTMIGRATE, RUN_STATE_RUNNING},
+    {RUN_STATE_POSTMIGRATE, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_POSTMIGRATE, RUN_STATE_PRELAUNCH},
+
+    {RUN_STATE_PRELAUNCH, RUN_STATE_RUNNING},
+    {RUN_STATE_PRELAUNCH, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_PRELAUNCH, RUN_STATE_INMIGRATE},
+
+    {RUN_STATE_FINISH_MIGRATE, RUN_STATE_RUNNING},
+    {RUN_STATE_FINISH_MIGRATE, RUN_STATE_PAUSED},
+    {RUN_STATE_FINISH_MIGRATE, RUN_STATE_POSTMIGRATE},
+    {RUN_STATE_FINISH_MIGRATE, RUN_STATE_PRELAUNCH},
+    {RUN_STATE_FINISH_MIGRATE, RUN_STATE_COLO},
+
+    {RUN_STATE_RESTORE_VM, RUN_STATE_RUNNING},
+    {RUN_STATE_RESTORE_VM, RUN_STATE_PRELAUNCH},
+
+    {RUN_STATE_COLO, RUN_STATE_RUNNING},
+    {RUN_STATE_COLO, RUN_STATE_SHUTDOWN},
+
+    {RUN_STATE_RUNNING, RUN_STATE_DEBUG},
+    {RUN_STATE_RUNNING, RUN_STATE_INTERNAL_ERROR},
+    {RUN_STATE_RUNNING, RUN_STATE_IO_ERROR},
+    {RUN_STATE_RUNNING, RUN_STATE_PAUSED},
+    {RUN_STATE_RUNNING, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_RUNNING, RUN_STATE_RESTORE_VM},
+    {RUN_STATE_RUNNING, RUN_STATE_SAVE_VM},
+    {RUN_STATE_RUNNING, RUN_STATE_SHUTDOWN},
+    {RUN_STATE_RUNNING, RUN_STATE_WATCHDOG},
+    {RUN_STATE_RUNNING, RUN_STATE_GUEST_PANICKED},
+    {RUN_STATE_RUNNING, RUN_STATE_COLO},
+
+    {RUN_STATE_SAVE_VM, RUN_STATE_RUNNING},
+
+    {RUN_STATE_SHUTDOWN, RUN_STATE_PAUSED},
+    {RUN_STATE_SHUTDOWN, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_SHUTDOWN, RUN_STATE_PRELAUNCH},
+    {RUN_STATE_SHUTDOWN, RUN_STATE_COLO},
+
+    {RUN_STATE_DEBUG, RUN_STATE_SUSPENDED},
+    {RUN_STATE_RUNNING, RUN_STATE_SUSPENDED},
+    {RUN_STATE_SUSPENDED, RUN_STATE_RUNNING},
+    {RUN_STATE_SUSPENDED, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_SUSPENDED, RUN_STATE_PRELAUNCH},
+    {RUN_STATE_SUSPENDED, RUN_STATE_COLO},
+
+    {RUN_STATE_WATCHDOG, RUN_STATE_RUNNING},
+    {RUN_STATE_WATCHDOG, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_WATCHDOG, RUN_STATE_PRELAUNCH},
+    {RUN_STATE_WATCHDOG, RUN_STATE_COLO},
+
+    {RUN_STATE_GUEST_PANICKED, RUN_STATE_RUNNING},
+    {RUN_STATE_GUEST_PANICKED, RUN_STATE_FINISH_MIGRATE},
+    {RUN_STATE_GUEST_PANICKED, RUN_STATE_PRELAUNCH},
 
-    { RUN_STATE__MAX, RUN_STATE__MAX },
+    {RUN_STATE__MAX, RUN_STATE__MAX},
 };
 
 static bool runstate_valid_transitions[RUN_STATE__MAX][RUN_STATE__MAX];
@@ -179,7 +1058,8 @@
     const char *state = RunState_str(current_run_state);
     size_t len = strlen(state) + 1;
 
-    if (len > size) {
+    if (len > size)
+    {
         return false;
     }
     memcpy(str, state, len);
@@ -191,7 +1071,8 @@
     const RunStateTransition *p;
 
     memset(&runstate_valid_transitions, 0, sizeof(runstate_valid_transitions));
-    for (p = &runstate_transitions_def[0]; p->from != RUN_STATE__MAX; p++) {
+    for (p = &runstate_transitions_def[0]; p->from != RUN_STATE__MAX; p++)
+    {
         runstate_valid_transitions[p->from][p->to] = true;
     }
 
@@ -206,11 +1087,13 @@
     trace_runstate_set(current_run_state, RunState_str(current_run_state),
                        new_state, RunState_str(new_state));
 
-    if (current_run_state == new_state) {
+    if (current_run_state == new_state)
+    {
         return;
     }
 
-    if (!runstate_valid_transitions[current_run_state][new_state]) {
+    if (!runstate_valid_transitions[current_run_state][new_state])
+    {
         error_report("invalid runstate transition: '%s' -> '%s'",
                      RunState_str(current_run_state),
                      RunState_str(new_state));
@@ -228,7 +1111,7 @@
 bool runstate_needs_reset(void)
 {
     return runstate_check(RUN_STATE_INTERNAL_ERROR) ||
-        runstate_check(RUN_STATE_SHUTDOWN);
+           runstate_check(RUN_STATE_SHUTDOWN);
 }
 
 StatusInfo *qmp_query_status(Error **errp)
@@ -262,10 +1145,12 @@
     qemu_mutex_unlock(&vmstop_lock);
     qemu_notify_event();
 }
-struct VMChangeStateEntry {
+struct VMChangeStateEntry
+{
     VMChangeStateHandler *cb;
     void *opaque;
-    QTAILQ_ENTRY(VMChangeStateEntry) entries;
+    QTAILQ_ENTRY(VMChangeStateEntry)
+    entries;
     int priority;
 };
 
@@ -285,7 +1170,7 @@
  * Returns: an entry to be freed using qemu_del_vm_change_state_handler()
  */
 VMChangeStateEntry *qemu_add_vm_change_state_handler_prio(
-        VMChangeStateHandler *cb, void *opaque, int priority)
+    VMChangeStateHandler *cb, void *opaque, int priority)
 {
     VMChangeStateEntry *e;
     VMChangeStateEntry *other;
@@ -296,8 +1181,10 @@
     e->priority = priority;
 
     /* Keep list sorted in ascending priority order */
-    QTAILQ_FOREACH(other, &vm_change_state_head, entries) {
-        if (priority < other->priority) {
+    QTAILQ_FOREACH(other, &vm_change_state_head, entries)
+    {
+        if (priority < other->priority)
+        {
             QTAILQ_INSERT_BEFORE(other, e, entries);
             return e;
         }
@@ -325,12 +1212,17 @@
 
     trace_vm_state_notify(running, state, RunState_str(state));
 
-    if (running) {
-        QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next) {
+    if (running)
+    {
+        QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next)
+        {
             e->cb(e->opaque, running, state);
         }
-    } else {
-        QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next) {
+    }
+    else
+    {
+        QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next)
+        {
             e->cb(e->opaque, running, state);
         }
     }
@@ -371,13 +1263,17 @@
 
 static void qemu_kill_report(void)
 {
-    if (!qtest_driver() && shutdown_signal) {
-        if (shutdown_pid == 0) {
+    if (!qtest_driver() && shutdown_signal)
+    {
+        if (shutdown_pid == 0)
+        {
             /* This happens for eg ^C at the terminal, so it's worth
              * avoiding printing an odd message in that case.
              */
             error_report("terminating on signal %d", shutdown_signal);
-        } else {
+        }
+        else
+        {
             char *shutdown_cmd = qemu_get_pid_name(shutdown_pid);
 
             error_report("terminating on signal %d from pid " FMT_pid " (%s)",
@@ -393,7 +1289,8 @@
 {
     ShutdownCause r = reset_requested;
 
-    if (r && replay_checkpoint(CHECKPOINT_RESET_REQUESTED)) {
+    if (r && replay_checkpoint(CHECKPOINT_RESET_REQUESTED))
+    {
         reset_requested = SHUTDOWN_CAUSE_NONE;
         return r;
     }
@@ -403,7 +1300,8 @@
 static int qemu_suspend_requested(void)
 {
     int r = suspend_requested;
-    if (r && replay_checkpoint(CHECKPOINT_SUSPEND_REQUESTED)) {
+    if (r && replay_checkpoint(CHECKPOINT_SUSPEND_REQUESTED))
+    {
         suspend_requested = 0;
         return r;
     }
@@ -440,12 +1338,16 @@
 
     cpu_synchronize_all_states();
 
-    if (mc && mc->reset) {
+    if (mc && mc->reset)
+    {
         mc->reset(current_machine);
-    } else {
+    }
+    else
+    {
         qemu_devices_reset();
     }
-    if (reason && reason != SHUTDOWN_CAUSE_SUBSYSTEM_RESET) {
+    if (reason && reason != SHUTDOWN_CAUSE_SUBSYSTEM_RESET)
+    {
         qapi_event_send_reset(shutdown_caused_by_guest(reason), reason);
     }
     cpu_synchronize_all_post_reset();
@@ -460,7 +1362,8 @@
 
     mc = current_machine ? MACHINE_GET_CLASS(current_machine) : NULL;
 
-    if (mc && mc->wakeup) {
+    if (mc && mc->wakeup)
+    {
         mc->wakeup(current_machine);
     }
 }
@@ -469,7 +1372,8 @@
 {
     qemu_log_mask(LOG_GUEST_ERROR, "Guest crashed");
 
-    if (current_cpu) {
+    if (current_cpu)
+    {
         current_cpu->crash_occurred = true;
     }
     /*
@@ -477,33 +1381,40 @@
      * shutdown, but in principle debug and reset could be supported as well.
      * Investigate any potential use cases for the unimplemented actions.
      */
-    if (panic_action == PANIC_ACTION_PAUSE
-        || (panic_action == PANIC_ACTION_SHUTDOWN && shutdown_action == SHUTDOWN_ACTION_PAUSE)) {
+    if (panic_action == PANIC_ACTION_PAUSE || (panic_action == PANIC_ACTION_SHUTDOWN && shutdown_action == SHUTDOWN_ACTION_PAUSE))
+    {
         qapi_event_send_guest_panicked(GUEST_PANIC_ACTION_PAUSE,
-                                        !!info, info);
+                                       !!info, info);
         vm_stop(RUN_STATE_GUEST_PANICKED);
-    } else if (panic_action == PANIC_ACTION_SHUTDOWN) {
+    }
+    else if (panic_action == PANIC_ACTION_SHUTDOWN)
+    {
         qapi_event_send_guest_panicked(GUEST_PANIC_ACTION_POWEROFF,
                                        !!info, info);
         vm_stop(RUN_STATE_GUEST_PANICKED);
         qemu_system_shutdown_request(SHUTDOWN_CAUSE_GUEST_PANIC);
-    } else {
+    }
+    else
+    {
         qapi_event_send_guest_panicked(GUEST_PANIC_ACTION_RUN,
-                                        !!info, info);
+                                       !!info, info);
     }
 
-    if (info) {
-        if (info->type == GUEST_PANIC_INFORMATION_TYPE_HYPER_V) {
-            qemu_log_mask(LOG_GUEST_ERROR, "\nHV crash parameters: (%#"PRIx64
-                          " %#"PRIx64" %#"PRIx64" %#"PRIx64" %#"PRIx64")\n",
+    if (info)
+    {
+        if (info->type == GUEST_PANIC_INFORMATION_TYPE_HYPER_V)
+        {
+            qemu_log_mask(LOG_GUEST_ERROR, "\nHV crash parameters: (%#" PRIx64 " %#" PRIx64 " %#" PRIx64 " %#" PRIx64 " %#" PRIx64 ")\n",
                           info->u.hyper_v.arg1,
                           info->u.hyper_v.arg2,
                           info->u.hyper_v.arg3,
                           info->u.hyper_v.arg4,
                           info->u.hyper_v.arg5);
-        } else if (info->type == GUEST_PANIC_INFORMATION_TYPE_S390) {
+        }
+        else if (info->type == GUEST_PANIC_INFORMATION_TYPE_S390)
+        {
             qemu_log_mask(LOG_GUEST_ERROR, " on cpu %d: %s\n"
-                          "PSW: 0x%016" PRIx64 " 0x%016" PRIx64"\n",
+                                           "PSW: 0x%016" PRIx64 " 0x%016" PRIx64 "\n",
                           info->u.s390.core,
                           S390CrashReason_str(info->u.s390.reason),
                           info->u.s390.psw_mask,
@@ -518,9 +1429,10 @@
     qemu_log_mask(LOG_GUEST_ERROR, "Guest crash loaded");
 
     qapi_event_send_guest_crashloaded(GUEST_PANIC_ACTION_RUN,
-                                   !!info, info);
+                                      !!info, info);
 
-    if (info) {
+    if (info)
+    {
         qapi_free_GuestPanicInformation(info);
     }
 }
@@ -528,12 +1440,17 @@
 void qemu_system_reset_request(ShutdownCause reason)
 {
     if (reboot_action == REBOOT_ACTION_SHUTDOWN &&
-        reason != SHUTDOWN_CAUSE_SUBSYSTEM_RESET) {
+        reason != SHUTDOWN_CAUSE_SUBSYSTEM_RESET)
+    {
         shutdown_requested = reason;
-    } else if (!cpus_are_resettable()) {
+    }
+    else if (!cpus_are_resettable())
+    {
         error_report("cpus are not resettable, terminating");
         shutdown_requested = reason;
-    } else {
+    }
+    else
+    {
         reset_requested = reason;
     }
     cpu_stop_current();
@@ -550,7 +1467,8 @@
 
 void qemu_system_suspend_request(void)
 {
-    if (runstate_check(RUN_STATE_SUSPENDED)) {
+    if (runstate_check(RUN_STATE_SUSPENDED))
+    {
         return;
     }
     suspend_requested = 1;
@@ -567,12 +1485,14 @@
 {
     trace_system_wakeup_request(reason);
 
-    if (!runstate_check(RUN_STATE_SUSPENDED)) {
+    if (!runstate_check(RUN_STATE_SUSPENDED))
+    {
         error_setg(errp,
                    "Unable to wake up: guest is not in suspended state");
         return;
     }
-    if (!(wakeup_reason_mask & (1 << reason))) {
+    if (!(wakeup_reason_mask & (1 << reason)))
+    {
         return;
     }
     runstate_set(RUN_STATE_RUNNING);
@@ -582,9 +1502,12 @@
 
 void qemu_system_wakeup_enable(WakeupReason reason, bool enabled)
 {
-    if (enabled) {
+    if (enabled)
+    {
         wakeup_reason_mask |= (1 << reason);
-    } else {
+    }
+    else
+    {
         wakeup_reason_mask &= ~(1 << reason);
     }
 }
@@ -667,24 +1590,31 @@
     RunState r;
     ShutdownCause request;
 
-    if (qemu_debug_requested()) {
+    if (qemu_debug_requested())
+    {
         vm_stop(RUN_STATE_DEBUG);
     }
-    if (qemu_suspend_requested()) {
+    if (qemu_suspend_requested())
+    {
         qemu_system_suspend();
     }
     request = qemu_shutdown_requested();
-    if (request) {
+    if (request)
+    {
         qemu_kill_report();
         qemu_system_shutdown(request);
-        if (shutdown_action == SHUTDOWN_ACTION_PAUSE) {
+        if (shutdown_action == SHUTDOWN_ACTION_PAUSE)
+        {
             vm_stop(RUN_STATE_SHUTDOWN);
-        } else {
+        }
+        else
+        {
             return true;
         }
     }
     request = qemu_reset_requested();
-    if (request) {
+    if (request)
+    {
         pause_all_vcpus();
         qemu_system_reset(request);
         resume_all_vcpus();
@@ -693,12 +1623,14 @@
          * as iothread mutex is unlocked
          */
         if (!runstate_check(RUN_STATE_RUNNING) &&
-                !runstate_check(RUN_STATE_INMIGRATE) &&
-                !runstate_check(RUN_STATE_FINISH_MIGRATE)) {
+            !runstate_check(RUN_STATE_INMIGRATE) &&
+            !runstate_check(RUN_STATE_FINISH_MIGRATE))
+        {
             runstate_set(RUN_STATE_PRELAUNCH);
         }
     }
-    if (qemu_wakeup_requested()) {
+    if (qemu_wakeup_requested())
+    {
         pause_all_vcpus();
         qemu_system_wakeup();
         notifier_list_notify(&wakeup_notifiers, &wakeup_reason);
@@ -706,24 +1638,141 @@
         resume_all_vcpus();
         qapi_event_send_wakeup();
     }
-    if (qemu_powerdown_requested()) {
+    if (qemu_powerdown_requested())
+    {
         qemu_system_powerdown();
     }
-    if (qemu_vmstop_requested(&r)) {
+    if (qemu_vmstop_requested(&r))
+    {
         vm_stop(r);
     }
     return false;
 }
 
+static int once = 0;
 void qemu_main_loop(void)
 {
+    int interaction = 0;
+
+    static char *job_id = "qemu_job_123-test";
+    static char *job_id2 = "qemu_job_123-test2";
+    static char *tag = "thetagishere";
+
 #ifdef CONFIG_PROFILER
     int64_t ti;
 #endif
-    while (!main_loop_should_exit()) {
+    while (!main_loop_should_exit())
+    {
 #ifdef CONFIG_PROFILER
         ti = profile_getclock();
 #endif
+        // printf("value of shmptr fuzzer mode is %d\n", *shmptr_fuzzer_mode);
+        if (SNAPSHOT_ENABLED)
+        {
+            if (*shmptr_fuzzer_mode == SNAPSHOT_MODE) // && once == 0) // salva stato
+            {
+                /*                mon_hmp = g_new0(MonitorHMP, 1);
+                                monitor_data_init(&mon_hmp->common, false, false, false);
+                                monitor_list_append(&mon_hmp->common);
+                */
+                once = 1;
+                mon_hmp = g_new0(MonitorHMP, 1);
+                monitor_data_init(&mon_hmp->common, false, false, false);
+                monitor_list_append(&mon_hmp->common);
+
+                fprintf(stderr, "[QEMU debug] run snapshot command\n");
+                // monitor_suspend(&mon_hmp->common);
+                char command[100];
+                sprintf(command, "savevm %s", SNAPSHOT_LABEL);
+                // handle_hmp_command(mon_hmp, command);
+                printf("test\n");
+                Error *err = NULL;
+                // hmp_
+                // qmp_snapshot_save(job_id, tag, NULL, &err);
+
+                // printf("error is %s\n",erro);
+                bool res = save_snapshot(SNAPSHOT_LABEL, true, NULL, false, NULL, &err);
+                printf("save res is %d\n", res);
+                if (res == 1)
+                {
+                    printf("\n[MAIN LOOP] SNAPSHOT DID SUCCESfully!!\n");
+                }
+                // monitor_resume(&mon_hmp->common);
+                // printf("b4 sem_rd_fuzzer_mode\n");
+                // sem_post(sem_rd_fuzzer_mode);
+                // printf("after sem_rd_fuzzer_mode\n");
+                // printf("test1\n");
+                *shmptr_fuzzer_mode = FUZZING_MODE; // DEFAULT_MODE;
+                // printf("test2\n");
+                // fuzzer_mode = RECOVER_MODE;
+            }
+            else if (*shmptr_fuzzer_mode == RECOVER_MODE)
+            {
+                struct timeval start_time, end_time;
+                double execution_time;
+
+                gettimeofday(&start_time, NULL); // Record the starting time
+
+                printf("\nRecover #%d start\n", interaction);
+
+                clock_t t;
+                t = clock();
+                printf("\nrecovering sys!\n");
+                char command[100];
+                sprintf(command, "loadvm %s", SNAPSHOT_LABEL);
+                monitor_suspend(&mon_hmp->common);
+                handle_hmp_command(mon_hmp, "stop");
+                handle_hmp_command(mon_hmp, command);
+                handle_hmp_command(mon_hmp, "cont");
+                monitor_resume(&mon_hmp->common);
+
+                /*int saved_vm_running = runstate_is_running();
+                Error *err = NULL;
+                printf("before snapshot!\n");
+                // qmp_snapshot_load(job_id2, tag, NULL, &err);
+                // const char* msg = error_get_pretty(err);
+                // printf("error msg is %s\n",msg);
+                vm_stop(RUN_STATE_RESTORE_VM);
+                if (load_snapshot(SNAPSHOT_LABEL, NULL, false, NULL, &err) && saved_vm_running)
+                {
+                    vm_start();
+                }*/
+
+                // sleep(5);
+                printf("after snapshot!\n");
+                t = clock() - t;
+                // sleep(5);
+                double time_taken = ((double)t) / CLOCKS_PER_SEC;
+                printf("time taken is %f\n", time_taken);
+                // hmp_handle_error(mon, err);
+                //  Error *err = NULL;
+                // bool res = load_snapshot(SNAPSHOT_LABEL,NULL,false,NULL,&err);
+                // vm_start();
+                //  printf("load res is %d\n",res);
+                //  printf("error msg is %s\n",&err.);
+
+                gettimeofday(&end_time, NULL); // Record the ending time
+
+                // Calculate execution time in seconds
+                execution_time = (double)(end_time.tv_sec - start_time.tv_sec) +
+                                 (double)(end_time.tv_usec - start_time.tv_usec) / 1e6;
+
+                printf("\n\nExecution time: %f seconds\n", execution_time);
+
+                printf("\nrecovered. \n");
+                // sleep(1);
+                // sem_post(sem_rd_fuzzer_mode);
+
+                // sem_post(sem_gathering);
+                // printf("sem posted!\n");
+                //  fuzzer_mode = FUZZING_MODE;
+                *shmptr_fuzzer_mode = FUZZING_MODE;
+                //  gogo = 1;
+                // sem_post(sem_rd_fuzzer_mode);
+                printf("\nRecover #%d end\n", interaction++);
+            }
+        }
+
         main_loop_wait(false);
 #ifdef CONFIG_PROFILER
         dev_time += profile_getclock() - ti;
@@ -768,7 +1817,8 @@
     postcopy_infrastructure_init();
     monitor_init_globals();
 
-    if (qcrypto_init(&err) < 0) {
+    if (qcrypto_init(&err) < 0)
+    {
         error_reportf_err(err, "cannot initialize crypto: ");
         exit(1);
     }
@@ -779,7 +1829,6 @@
     socket_init();
 }
 
-
 void qemu_cleanup(void)
 {
     gdb_exit(0);
@@ -825,3 +1874,226 @@
     user_creatable_cleanup();
     /* TODO: unref root container, check all devices are ok */
 }
+
+void *qemu_afl_thread(void *arg)
+{
+    shmfd_fuzzing_report = shm_open(SHM_FUZZING_REPORT, O_CREAT | O_RDONLY, S_IRWXU | S_IRWXG);
+    if (shmfd_fuzzing_report == -1)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread] Error in shm_open() of shmfd_fuzzing_report\n");
+        exit(1);
+    }
+
+    shmptr_fuzzing_report = (struct SingleFuzzReport *)mmap(0, sizeof(struct SingleFuzzReport), PROT_READ, MAP_SHARED, shmfd_fuzzing_report, 0);
+    if (shmptr_fuzzing_report == MAP_FAILED)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread] Error in mmap() of shmptr_fuzzing_report\n");
+        exit(1);
+    }
+
+    shm_fd = shm_open(SHM_FILE, O_CREAT | O_RDWR, S_IRWXU | S_IRWXG | S_IRWXO); // 0666);
+    if (shm_fd == -1)
+    {
+        printf("Error in shm_open()\n");
+        exit(1);
+    }
+
+    if (ftruncate(shm_fd, sizeof(u8) * MAP_SIZE) == -1)
+    {
+        printf("Error in ftruncate()\n");
+        exit(1);
+    }
+
+    afl_area_ptr = mmap(NULL, sizeof(u8) * MAP_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0);
+    if (afl_area_ptr == MAP_FAILED)
+    {
+        printf("Error in mmap()\n");
+        exit(1);
+    }
+
+    // block coverage bitmap (per grafici)
+    shm_fd2 = shm_open(SHM_FILE_BLOCK_COV, O_CREAT | O_RDWR, S_IRWXU | S_IRWXG | S_IRWXO); // 0666);
+    if (shm_fd2 == -1)
+    {
+        printf("Error in shm_open()\n");
+        exit(1);
+    }
+
+    if (ftruncate(shm_fd2, sizeof(u8) * MAP_SIZE) == -1)
+    {
+        printf("Error in ftruncate()\n");
+        exit(1);
+    }
+
+    block_afl_area_ptr = mmap(NULL, sizeof(u8) * MAP_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd2, 0);
+    if (afl_area_ptr == MAP_FAILED)
+    {
+        printf("Error in mmap()\n");
+        exit(1);
+    }
+
+    // utile questo sleep anche per far passare il transitortio per non avere rumore sulla coverage
+    afl_engine_t *engine = initialize_engine_istance();
+    printf("\n------------------- NUMBER OF STAGES: %ld -------------------------\n", engine->fuzz_one->stages_count);
+
+    fuzzer_process_main(engine);
+}
+
+void qemu_fuzzing_loop(void)
+{
+    // start merge
+
+    struct sigaction sa;
+    sa.sa_flags = 0;
+    sigemptyset(&sa.sa_mask);
+    (void)sigaction(SIGINT, &sa, NULL);
+    (void)sigaction(SIGBUS, &sa, NULL);
+    (void)sigaction(SIGSEGV, &sa, NULL);
+
+    // Shared memory and semaphoreS initialization for fuzzer mode signal (CONSUMER)
+    sem_rd_fuzzer_mode = sem_open(SEM_RD_FUZZER_MODE, O_CREAT, S_IRWXU, 0);
+    if (sem_rd_fuzzer_mode == NULL)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread] Error in sem_open() of sem_rd_fuzzer_mode\n");
+        exit(1);
+    }
+    sem_wr_fuzzer_mode = sem_open(SEM_WR_FUZZER_MODE, O_CREAT, S_IRWXU, 1);
+    if (sem_wr_fuzzer_mode == NULL)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread] Error in sem_open() of sem_rw_fuzzer_mode\n");
+        exit(1);
+    }
+    shmfd_fuzzer_mode = shm_open(SHM_FUZZER_MODE, O_CREAT | O_TRUNC | O_RDWR, S_IRWXU | S_IRWXG);
+    if (shmfd_fuzzer_mode < 0)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread] Error in shm_open()\n");
+        exit(1);
+    }
+    ftruncate(shmfd_fuzzer_mode, length);
+    shmptr_fuzzer_mode = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_SHARED, shmfd_fuzzer_mode, 0);
+    if (shmptr_fuzzer_mode == MAP_FAILED)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread] Error in mmap()\n");
+        exit(1);
+    }
+
+    // Shared memory and semaphores inizialization for fuzzing input (PRODUCER)
+    fprintf(stderr, "[QEMU Fuzzer thread]  Semaphores and SHM initialization for fuzzing input\n");
+    sem_rd_fuzzing_input = sem_open(SEM_RD_FUZZING_INPUT, O_CREAT, S_IRWXU, 0);
+    if (sem_rd_fuzzing_input == NULL)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread]  Error in sem_open()\n");
+        exit(1);
+    }
+    sem_wr_fuzzing_input = sem_open(SEM_WR_FUZZING_INPUT, O_CREAT, S_IRWXU, 1);
+    if (sem_wr_fuzzing_input == NULL)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread]  Error in sem_open()\n");
+        exit(1);
+    }
+    shmfd_fuzzing_input = shm_open(SHM_FUZZING_INPUT, O_CREAT | O_TRUNC | O_RDWR, S_IRWXU | S_IRWXG);
+    if (shmfd_fuzzing_input < 0)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread]  Error in shm_open()\n");
+        exit(1);
+    }
+    ftruncate(shmfd_fuzzing_input, length_fuzz_input);
+    shmptr_fuzzing_input = (struct QueueItem *)mmap(NULL, length_fuzz_input, PROT_READ | PROT_WRITE, MAP_SHARED, shmfd_fuzzing_input, 0);
+    if (shmptr_fuzzing_input == MAP_FAILED)
+    {
+        fprintf(stderr, "[QEMU Fuzzer thread]  Error in mmap()\n");
+        exit(1);
+    }
+
+    // end merge
+    if (SAVE_METRICS)
+    {
+        printf("\n[METRICS] enabled\n");
+    }
+    else
+    {
+        printf("\n[METRICS] disabled\n");
+    }
+    wait_for_end_test_case = sem_open(SEM_RD_END_TEST_CASE_MODE, O_CREAT, S_IRWXU, 0);
+    if (wait_for_end_test_case == NULL)
+    {
+        fprintf(stderr, "[fuzzer] Error in sem_open()\n");
+        exit(1);
+    }
+
+    sem_gathering = sem_open(SEM_GATHERING, O_CREAT, S_IRWXU, 0);
+    if (sem_gathering == NULL)
+    {
+        fprintf(stderr, "[fuzzer] Error in sem_open()\n");
+        exit(1);
+    }
+
+    sem_gathering2 = sem_open(SEM_GATHERING2, O_CREAT, S_IRWXU, 0);
+    if (sem_gathering2 == NULL)
+    {
+        fprintf(stderr, "[fuzzer] Error in sem_open()\n");
+        exit(1);
+    }
+    tcg_plugin_continue = sem_open(SEM_RD_END_TCG_PLUGIN_CONTINUE, O_CREAT, S_IRWXU, 0);
+    if (tcg_plugin_continue == NULL)
+    {
+        fprintf(stderr, "[fuzzer] Error in sem_open()\n");
+        exit(1);
+    }
+    // sem_init(wait_for_end_test_case, 0, 0);
+    fprintf(stderr, "Semafori: inizializzati\n");
+
+    fprintf(stderr, "Strutture monitor inizializzate\n");
+
+    pthread_t tid;
+    pthread_t tid2;
+    int ret;
+    int ret2;
+
+    ret2 = pthread_create(&tid2, NULL, qemu_afl_thread, NULL);
+    if (ret2 != 0)
+    {
+        fprintf(stderr, "[QEMU debug] AFL thread creation error.\n");
+        exit(EXIT_FAILURE);
+    }
+
+    /*            clock_t start, end;
+                double cpu_time_used;
+    start = clock();
+    printf("\nrecovering sys!\n");
+    char command[100];
+    // sprintf(command, "loadvm %s", SNAPSHOT_LABEL);
+    //  monitor_suspend(&mon_hmp->common);
+    //  handle_hmp_command(mon_hmp, command);
+    int saved_vm_running = runstate_is_running();
+    Error *err = NULL;
+
+    vm_stop(RUN_STATE_RESTORE_VM);
+
+    if (load_snapshot(SNAPSHOT_LABEL, NULL, false, NULL, &err) && saved_vm_running)
+    {
+        vm_start();
+    }
+    printf("waiting..\n");
+    sleep(10);
+    printf("finished waiting..\n");
+    // hmp_handle_error(mon, err);
+    //  Error *err = NULL;
+    //  bool res = load_snapshot(SNAPSHOT_LABEL,NULL,false,NULL,&err);
+    //  vm_start();
+    //  printf("load res is %d\n",res);
+    //  printf("error msg is %s\n",&err.);
+    //  monitor_resume(&mon_hmp->common);
+    end = clock();
+
+    cpu_time_used = ((double)(end - start)) / CLOCKS_PER_SEC; // Calculate the CPU time used
+
+    printf("Execution time: %f seconds\n", cpu_time_used);*/
+
+    /*ret = pthread_create(&tid, NULL, qemu_injector_thread, NULL);
+    if (ret != 0)
+    {
+        fprintf(stderr, "[QEMU debug] INjector thread creation error.\n");
+        exit(EXIT_FAILURE);
+    }*/
+}
diff -ruN qemu/softmmu/vl.c qemu_patched/softmmu/vl.c
--- qemu/softmmu/vl.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/softmmu/vl.c	2023-10-30 19:01:26.105790767 +0100
@@ -136,6 +136,7 @@
 #define MAX_VIRTIO_CONSOLES 1
 
 #include "hw/adacore/rlimit.h"
+
 void qemu_exit_with_debug(const char *fmt, ...);
 
 typedef struct BlockdevOptionsQueueEntry {
diff -ruN qemu/tcg/tcg.c qemu_patched/tcg/tcg.c
--- qemu/tcg/tcg.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/tcg/tcg.c	2023-11-08 19:55:34.050010217 +0100
@@ -729,6 +729,7 @@
 
 #ifndef CONFIG_TCG_INTERPRETER
     tcg_qemu_tb_exec = (tcg_prologue_fn *)tcg_splitwx_to_rx(s->code_ptr);
+    
 #endif
 
 #ifdef TCG_TARGET_NEED_POOL_LABELS
diff -ruN qemu/tcg/tcg-op.c qemu_patched/tcg/tcg-op.c
--- qemu/tcg/tcg-op.c	2023-04-05 13:52:48.000000000 +0200
+++ qemu_patched/tcg/tcg-op.c	2023-11-11 01:03:19.677184423 +0100
@@ -2730,6 +2730,7 @@
 #endif
             /* When not chaining, exit without indicating a link.  */
             if (qemu_loglevel_mask(CPU_LOG_TB_NOCHAIN)) {
+
                 val = 0;
             }
         } else {
